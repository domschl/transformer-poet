{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/transformer-poet/blob/main/transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Transformer-Poet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabS0VZ-1Zp0"
      },
      "source": [
        "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtpy59Yq-Qfz",
        "outputId": "05361930-c01b-4f0d-933d-f78e78389778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ml-indie-tools\n",
            "  Downloading ml_indie_tools-0.3.17-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: ml-indie-tools\n",
            "Successfully installed ml-indie-tools-0.3.17\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5T4m6earb1e",
        "outputId": "08c72f63-1bd1-4a4e-da8d-9924f610dd5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TF-Keras version: 2.9.0\n"
          ]
        }
      ],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
        "\n",
        "from ml_indie_tools.keras_custom_layers import MultiHeadSelfAttention, PositionalEncoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A tensorflow deep multi-head attention model for text generation\n",
        "\n",
        "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
        "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "llPw84PkEAP2",
        "outputId": "f79216ee-8e33-4861-d1fe-3e5ca0ad29e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OS: Linux, Python: 3.8.16, Colab Jupyter Notebook Tensorflow: 2.9.2, TPU: TPU, 8 nodes v2 (8GB)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
        "\n",
        "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oZ6t9b6ZwSxi"
      },
      "outputs": [],
      "source": [
        "use_eager=tf.executing_eagerly()\n",
        "if ml_env.is_tpu is True:\n",
        "    tpu_strategy = ml_env.tpu_strategy\n",
        "    tpu_is_init=True\n",
        "    if use_eager is True:\n",
        "        tf.config.run_functions_eagerly(False)\n",
        "    use_eager=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-TP3Pnsrb1f",
        "outputId": "6fa49040-4d9d-4895-f91f-f8137780803e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Root path (all projects) : /content/drive/My Drive (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
            "Project path             : /content/drive/My Drive/Colab Notebooks/women_writers (Changes to the file system happen only below this project path\n",
            "Model path (snapshots)   : /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf (Model weights and snapshots are stored here)\n",
            "Data path (training data): /content/drive/My Drive/Colab Notebooks/women_writers/data (Training data will be downloaded here)\n",
            "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
          ]
        }
      ],
      "source": [
        "project_name='women_writers'\n",
        "model_name='mhsa_v1_tf'\n",
        "\n",
        "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
        "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
        "#\n",
        "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
        "\n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
        "\n",
        "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
        "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
        "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
        "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
        "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
        "encoding, batch generation, and formatted source display. It read some \n",
        "books from Project Gutenberg and supports creation of training batches. \n",
        "The output functions support highlighting to allow to compare generated \n",
        "texts with the actual sources to help to identify identical (memorized) \n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C66X7ynnrb1h",
        "outputId": "3d9a018b-65dd-4cdb-e8e4-fb90a3489ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
          ]
        }
      ],
      "source": [
        "# sample searches\n",
        "search_spec= {\"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
        "\n",
        "book_list=gd.search(search_spec)\n",
        "book_cnt = len(book_list)\n",
        "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "if book_cnt<40:\n",
        "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
        "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
        "else:\n",
        "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH6_7IU3upOd",
        "outputId": "c6aadb04-88cc-47ba-f886-71b8b0a84cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: The Common Reader - Virginia Woolf, 64457\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
            "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
            "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
            "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
            "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
            "6: Pride and Prejudice - Jane Austen, 42671\n",
            "7: The Letters of Jane Austen - Jane Austen, 42078\n",
            "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
            "9: Jacob's Room - Virginia Woolf, 5670\n",
            "10: Pride and Prejudice - Jane Austen, 1342\n",
            "11: Night and Day - Virginia Woolf, 1245\n",
            "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
            "13: Lady Susan - Jane Austen, 946\n",
            "14: Wuthering Heights - Emily Brontë, 768\n",
            "15: Sense and Sensibility - Jane Austen, 161\n",
            "16: Emma - Jane Austen, 158\n",
            "17: The Voyage Out - Virginia Woolf, 144\n",
            "18: Mansfield Park - Jane Austen, 141\n",
            "19: Northanger Abbey - Jane Austen, 121\n",
            "20: Persuasion - Jane Austen, 105\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(book_list)):\n",
        "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jBH3Z15rb1h",
        "outputId": "6597c559-0dea-4a32-ea5d-c84ba3bcd84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using:\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
            "2: Jacob's Room - Virginia Woolf\n",
            "3: Pride and Prejudice - Jane Austen\n",
            "4: Night and Day - Virginia Woolf\n",
            "5: Lady Susan - Jane Austen\n",
            "6: Wuthering Heights - Emily Brontë\n",
            "7: Sense and Sensibility - Jane Austen\n",
            "8: Emma - Jane Austen\n",
            "9: The Voyage Out - Virginia Woolf\n",
            "10: Mansfield Park - Jane Austen\n",
            "11: Northanger Abbey - Jane Austen\n",
            "12: Persuasion - Jane Austen\n"
          ]
        }
      ],
      "source": [
        "MAX_TOKENS = 20000  # This becomes vocab_size\n",
        "MAX_NGRAM_LEN = 8   # Max length of a token\n",
        "\n",
        "select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
        "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
        "\n",
        "print(\"Using:\")\n",
        "for i in range(len(sub_book_list)):\n",
        "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
        "\n",
        "textlib_dataset = None  # Forces re-caching\n",
        "td = Text_Dataset(sub_book_list)\n",
        "td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7_tc2Lirb1i",
        "outputId": "faf15399-3a0d-4552-ff72-9223143826cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1529756 records\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LEN = 80\n",
        "SUB_PROBABILITY = 0.15  # like BERT\n",
        "\n",
        "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN, content_stepping=1)\n",
        "\n",
        "num_records = len(td)\n",
        "\n",
        "print(f\"{num_records} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zZbbsNm0cOeW"
      },
      "outputs": [],
      "source": [
        "def get_sample_batch(td, batch_size, length, SUB_probability=0.15):\n",
        "    for i in range(batch_size):\n",
        "        Xi = td.get_random_item()\n",
        "        yi = Xi.copy()\n",
        "        l=int(len(Xi)*SUB_probability)\n",
        "        for li in range(l):\n",
        "            pos=random.randint(0,len(Xi)-1)\n",
        "            if td.tokenizer_type=='char':\n",
        "                Xi[pos]=td.c2i['␚']\n",
        "            elif td.tokenizer_type=='word':\n",
        "                Xi[pos]=td.w2i['<subst>']\n",
        "            elif td.tokenizer_type=='ngram':\n",
        "                Xi[pos]=td.t2i['<subst>']\n",
        "            else:\n",
        "                print(f\"Unexpected tokenizer_type {td.tokenizer_type}\")\n",
        "        if i==0:\n",
        "            # smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpX=np.array(Xi, dtype=np.int32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI3Fx6bNuR9A",
        "outputId": "0d25536b-daa8-408f-ddbf-28bb71a265b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0](l=80): X=>--\"\n",
            "\n",
            "<subst><subst>ped, regretting with a deep blush that she had implied so much;\n",
            "but less would ha<subst>have been sufficient.  Mrs Smith would hardly\n",
            "have believed so soon in Mr Elliot's failure, <subst>from the perception\n",
            "of there being a some<subst>else.  As it <subst>she i<subst> submitted,\n",
            "and with all<subst>mb<subst> of se<subst>nothing beyond; and Anne, eage<subst>\n",
            "e<, y=>--\"\n",
            "\n",
            "She stopped, regretting with a deep blush that she had implied so much;\n",
            "but less would hardly have been sufficient.  Mrs Smith would hardly\n",
            "have believed so soon in Mr Elliot's failure, but from the perception\n",
            "of there being a somebody else.  As it was, she instantly submitted,\n",
            "and with all the semblance of seeing nothing beyond; and Anne, eager to\n",
            "e<\n",
            "[1](l=80): X=>, the cousin who taught the young ladies of\n",
            "Bungay to play upon the violin, was the <subst>one in whom she could\n",
            "confide, and as she walked up and down benea<subst><subst>ps <subst>pergola, she <subst><subst> a little speech to him, which ran<subst>ng like\n",
            "thi<subst>:\n",
            "\n",
            "“To begin with, I<subst><subst>ond of William. You can’t <subst>y that. I know\n",
            "him better than any one, a<subst>. But wh<, y=>, the cousin who taught the young ladies of\n",
            "Bungay to play upon the violin, was the only one in whom she could\n",
            "confide, and as she walked up and down beneath the hoops of the\n",
            "pergola, she did begin a little speech to him, which ran something like\n",
            "this:\n",
            "\n",
            "“To begin with, I’m very fond of William. You can’t deny that. I know\n",
            "him better than any one, almost. But wh<\n"
          ]
        }
      ],
      "source": [
        "test_x, test_y = get_sample_batch(td, 2, 40, SUB_probability=SUB_PROBABILITY)\n",
        "for i in range(len(test_x)):\n",
        "    xi=[int(x) for x in test_x[i]]\n",
        "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<, y=>{td.decode(test_y[i])}<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnMxRkkmcOeX",
        "outputId": "815e8959-5fea-4aa9-e523-7a8913b5972f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 80), (2, 80))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "test_x.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jn_LcJ6g9Mzy"
      },
      "outputs": [],
      "source": [
        "def expand_name_template(template, params):\n",
        "    exp=copy.copy(template)\n",
        "    for key in params:\n",
        "        src=\"{\"+key+\"}\"\n",
        "        dst=f\"{params[key]}\"\n",
        "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
        "    return exp\n",
        "\n",
        "def save_model_metadata(epoch, suffix='std'):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    params['current_epoch'] = epoch\n",
        "    try:\n",
        "        with open(meta_file, 'w') as f:\n",
        "            f.write(json.dumps(params))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def read_model_metadata(suffix=\"std\"):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    try:\n",
        "        with open(meta_file, 'r') as f:\n",
        "            meta = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
        "        return None\n",
        "    return meta\n",
        "\n",
        "def is_metadata_compatible(params, meta):\n",
        "    is_valid=True\n",
        "    keys=set(list(params.keys())+list(meta.keys()))\n",
        "    for key in keys:\n",
        "        if key in updatable_keys:\n",
        "            continue\n",
        "        if key not in meta:\n",
        "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif key not in params:\n",
        "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif meta[key]!=params[key]:\n",
        "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "    if is_valid is False:\n",
        "        print(\"Aborting import.\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znpIUA3ig3gO",
        "outputId": "dc97e890-18c2-44b4-bb2c-469c19997fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuing last session from epoch 82\n",
            "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 8, 'heads': [4, 4, 4, 4, 4, 4, 4, 4], 'units': [512, 512, 512, 512, 512, 512, 512, 512], 'norm': 'softmax', 'mh_normalize': True, 'l2_regularizer': 1e-09, 'dropout': 0.0, 'join_heads_by_add': True, 'recurrent': True, 'vocab_size': 20000, 'sequence_len': 80, 'embedding_size': 128, 'batch_size': 256, 'learning_rate': 0.0002, 'clipvalue': None, 'sample_every_n_epochs': 100, 'current_epoch': 82}\n"
          ]
        }
      ],
      "source": [
        "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
        "\n",
        "lyrs = 8;\n",
        "\n",
        "params = { # Multi-head self-attention\n",
        "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
        "\n",
        "    'mhsa_layers': lyrs, \n",
        "    'heads': [4]*lyrs,\n",
        "    'units': [512]*lyrs,  # 0 inserts an LSTM for memory-states :-)\n",
        "    'norm': 'softmax', # this is for within each head\n",
        "    'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
        "    'l2_regularizer': 1e-9,\n",
        "    'dropout': 0.0,       # no dropout: 0.0\n",
        "    'join_heads_by_add': True,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads\n",
        "    'recurrent': True,\n",
        "    'vocab_size': vocabulary_size,\n",
        "    'sequence_len': SEQUENCE_LEN,\n",
        "    'embedding_size': 128,\n",
        "\n",
        "    'batch_size': 256,\n",
        "    'learning_rate': 0.0002,\n",
        "    'clipvalue': None,\n",
        "    'sample_every_n_epochs': 100,\n",
        "}\n",
        "\n",
        "if len(params['heads'])!=params['mhsa_layers'] or len(params['units'])!=params['mhsa_layers']:\n",
        "    print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
        "    \n",
        "if ml_env.is_tpu is True:\n",
        "    lr = params['learning_rate']*1.0\n",
        "else:\n",
        "    lr = params['learning_rate']\n",
        "\n",
        "model_suffix = expand_name_template(params['name'], params)\n",
        "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
        "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
        "if os.path.exists(checkpoint_dir) is False:\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# When comparing if training-data is compatible with new params set, \n",
        "# the following keys are updatable, they can be changed while continuing\n",
        "# to use existing checkpoints and continue training with those values\n",
        "# changed:\n",
        "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
        "             'sample_every_n_epochs']\n",
        "\n",
        "# These values are taking from saved checkpoint:\n",
        "keep_keys=['current_epoch']\n",
        "\n",
        "continue_last = True\n",
        "if continue_last is False:\n",
        "    print(\"NOT continuing based on existing training! New start.\")\n",
        "\n",
        "meta = read_model_metadata(suffix=model_suffix)\n",
        "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
        "    for key in keep_keys:\n",
        "        if key in meta:\n",
        "            params[key]=meta[key]\n",
        "    if params is not None:\n",
        "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
        "    else:\n",
        "        print(f\"No previous data, starting new model\")\n",
        "else:\n",
        "    print(\"Starting new model\")\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY3hUuhQYzdT",
        "outputId": "a0a26c47-1022-438e-98ab-c71b80584fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches = 5975\n"
          ]
        }
      ],
      "source": [
        "num_batches = num_records // params['batch_size']\n",
        "print(f\"num_batches = {num_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EeB7jugCV4lI"
      },
      "outputs": [],
      "source": [
        "# @tf.function   (only slows things down [considerably!])\n",
        "def make_tf_dataset(num, random_index=False, SUB_probability=0.0):\n",
        "    dx=[]\n",
        "    dy=[]\n",
        "    num_batches_active = num\n",
        "    for i in range(num_batches_active):\n",
        "        x,y=get_sample_batch(td, params['batch_size'], params['sequence_len'], SUB_probability=SUB_probability)\n",
        "        if i<1:\n",
        "            print(f\"[{num} x]: {x.shape} -> {y.shape}\")\n",
        "        dx.append(x)\n",
        "        dy.append(y)\n",
        "    dx=np.array(dx)\n",
        "    dy=np.array(dy)\n",
        "    print(f\"dx.shape={dx.shape}, dy.shape={dy.shape}\")\n",
        "    data_xy = (dx, dy)\n",
        "    tf_dataset=tf.data.Dataset.from_tensor_slices(data_xy)\n",
        "    return tf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCy7WmQyS9T-",
        "outputId": "a560da5e-a743-4217-fd9d-c59965378957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5975 batches\n",
            "Creating dataset, this is slow. Be patient...\n",
            "[5975 x]: (256, 80) -> (256, 80)\n",
            "dx.shape=(5975, 256, 80), dy.shape=(5975, 256, 80)\n",
            "Dataset done and cached.\n"
          ]
        }
      ],
      "source": [
        "MAX_NUM_BATCHES = 50000\n",
        "\n",
        "if num_batches>MAX_NUM_BATCHES:\n",
        "    restricted_batches=MAX_NUM_BATCHES\n",
        "    print(f\"Restrictinig {num_batches} to max of {restricted_batches}\")\n",
        "else:\n",
        "    restricted_batches=num_batches\n",
        "    print(f\"{restricted_batches} batches\")\n",
        "if cached_batch_data == restricted_batches and textlib_dataset is not None:\n",
        "    print(\"Reusing cached training-data\")\n",
        "else:\n",
        "    print(\"Creating dataset, this is slow. Be patient...\")\n",
        "    textlib_dataset = make_tf_dataset(restricted_batches, SUB_probability=SUB_PROBABILITY)\n",
        "    cached_batch_data = restricted_batches\n",
        "    print(\"Dataset done and cached.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boow8wR7sLwi",
        "outputId": "0da44dce-dd38-4d4f-defc-6fba22d35225"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256, 80), dtype=tf.int32, name=None), TensorSpec(shape=(256, 80), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "shuffle_buffer=10000\n",
        "if ml_env.is_tpu is True:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer).repeat()  # Otherwise TPU may run dry\n",
        "else:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
        "dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B-G5HLMqqbeT"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    validation_dataset = make_tf_dataset(10, random_index=True, SUB_probability=SUB_PROBABILITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZAzFlCVBiL0Q"
      },
      "outputs": [],
      "source": [
        "def model_mhsa(inputs, params):\n",
        "    dense = layers.Dense(params['vocab_size'], kernel_regularizer=regularizers.l2(params['l2_regularizer']))  # using softmax here prevents temperature adjust, affects 'from_logits' param in sparse_categorical loss \n",
        "    fl = layers.Flatten()\n",
        "    dr = layers.Dropout(params['dropout'])\n",
        "    pe = PositionalEncoding(amplitude=0.3)\n",
        "    rs_up = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    if 0 in params['units']:\n",
        "        lstm1 = layers.LSTM(units=vocabulary_size, return_sequences=True)\n",
        "    if vocabulary_size>=300:\n",
        "        emb=layers.Embedding(vocabulary_size,params['embedding_size'],input_length=params['sequence_len'])\n",
        "    rs_down = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    mhsa=[]\n",
        "    residuals=[]\n",
        "\n",
        "    for i in range(params['mhsa_layers']):\n",
        "        if params['units'][i]==0:\n",
        "            mhsa.append(None)\n",
        "            residuals.append(i)\n",
        "        else:\n",
        "            mhsa.append(MultiHeadSelfAttention(params['heads'][i], units=params['units'][i], norm=params['norm'], mh_normalize=params['mh_normalize'], join_heads_by_add=params['join_heads_by_add'], recurrent=params['recurrent']))\n",
        "    xint = tf.cast(inputs,dtype=tf.int32)\n",
        "    if vocabulary_size<300:\n",
        "        x = tf.one_hot(xint, params['vocab_size'], axis=-1)\n",
        "    else:\n",
        "        x = emb(xint)\n",
        "    x = pe(x)\n",
        "    for i in range(len(mhsa)):\n",
        "        if i in residuals:\n",
        "            x = rs_down(lstm1(rs_up(x)))+x\n",
        "            print(f\"Residual at layer {i} added.\")\n",
        "        else:\n",
        "            x = mhsa[i](x)\n",
        "        # x = mhsa[i](x,x)\n",
        "    if params['dropout']>0.0:\n",
        "        x = dr(x)\n",
        "    # x = dense(fl(x))\n",
        "    x = dense(x)\n",
        "    return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4J13Gp_hjqqn"
      },
      "outputs": [],
      "source": [
        "def mhsa_generate(model, text, gen_len=64, temperature=0.9, argmax=False, verbose=False):\n",
        "    if verbose is True:\n",
        "        full=text[:-1]\n",
        "    gen_text=\"\"\n",
        "    lf=0\n",
        "    input = np.array(td.encode(text))\n",
        "    while len(input) < params['sequence_len']:\n",
        "        input = np.concatenate([td.encode('<pad>'),input])\n",
        "    for i in range(gen_len):\n",
        "        input = np.concatenate([input[1:],td.encode('<subst>')])\n",
        "        if len(input)!=params['sequence_len']:\n",
        "            print('assertion failure')\n",
        "            return None\n",
        "        pred = model(input)\n",
        "        pred /= temperature\n",
        "        pred = tf.keras.layers.Softmax()(pred)\n",
        "        if tf.executing_eagerly() is True and ml_env.is_tpu is False:\n",
        "            pred=pred.numpy()\n",
        "        else:\n",
        "            pred=tf.keras.backend.eval(pred)  # this is a cheat, it internaly used Numpy() too.\n",
        "        if argmax is True:\n",
        "            pred=np.argmax(pred[0],axis=1)\n",
        "        else:\n",
        "            pred = [np.random.choice(list(range(len(pred[0][-1]))), p=pred[0][-1])]\n",
        "        input = np.concatenate([input[1:],[pred[-1]]])\n",
        "        c = td.decode([pred[-1]])\n",
        "        if verbose is True:\n",
        "            print(c, end='')\n",
        "            if c=='\\n':\n",
        "                lf=0\n",
        "            else:\n",
        "                lf += 1\n",
        "                if (lf>80 and c==' ') or lf>120:\n",
        "                    print()\n",
        "                    lf=0\n",
        "            full+=c\n",
        "        gen_text+=c\n",
        "    if verbose is True:\n",
        "        print()\n",
        "    return gen_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nf-NHZ326NqJ",
        "outputId": "f060d237-ee10-42de-8ca8-beaf784e2fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TPU-scope model\n",
            "Creating Default-scope model\n"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        print(\"Creating TPU-scope model\")\n",
        "        inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "        outputs = model_mhsa(inputs, params)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    print(\"Creating Default-scope model\")\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model_cpu = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "else:\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    model_cpu = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SXx-nBe5-jyJ"
      },
      "outputs": [],
      "source": [
        "def get_newest_checkpoint(checkpoint_dir):\n",
        "    files = os.listdir(checkpoint_dir)\n",
        "    paths = [os.path.join(checkpoint_dir, basename) for basename in files]\n",
        "    return max(paths, key=os.path.getctime)\n",
        "\n",
        "def import_previous_compatible_checkpoint(model, force_import=False):\n",
        "    meta = read_model_metadata(suffix=model_suffix)\n",
        "    if meta is None:\n",
        "        print(\"No previous checkpoint found\")\n",
        "        return False\n",
        "    if is_metadata_compatible(params, meta) is not True and force_import is False:\n",
        "        print(\"No useable import found.\")\n",
        "        return False\n",
        "    try:\n",
        "        last_checkpoint = get_newest_checkpoint(checkpoint_dir) # Doesn't do anything: tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot determine last checkpoint in {checkpoint_dir}, cannot import due to: {e}\")\n",
        "        return False\n",
        "    print(f\"Last checkpoint: {last_checkpoint}\")\n",
        "    try:\n",
        "        model.load_weights(last_checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to import model {last_checkpoint}: {e}\")\n",
        "        return False\n",
        "    if 'current_epoch' in meta:\n",
        "        params['current_epoch'] = meta['current_epoch']\n",
        "    print(f\"Successful import of epoch {params['current_epoch']} from {last_checkpoint}, continuing from there...\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB-Q8YXvndE"
      },
      "source": [
        "### Loss function, optimizer, tensorboard output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0t5JWEdYZNGz"
      },
      "outputs": [],
      "source": [
        "kscc = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def loss(labels, logits):\n",
        "  vl=kscc(labels, logits)\n",
        "  return vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jc2kbGoAZXHi"
      },
      "outputs": [],
      "source": [
        "if params['clipvalue'] is not None:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "else:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        model.compile(optimizer=opti, loss=loss, metrics=[], run_eagerly=False, jit_compile=True)\n",
        "else:\n",
        "    model.compile(optimizer=opti, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAoMxogcX_Nq",
        "outputId": "84a77c09-c192-4478-867c-aca9a541e34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot determine last checkpoint in /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000, cannot import due to: max() arg is an empty sequence\n"
          ]
        }
      ],
      "source": [
        "import_checkpoint = True\n",
        "force_import = False   # True: ignore metadata and try import anyway. This will of course crash, if the new model doesn't fit the checkpoint-data...\n",
        "\n",
        "if import_checkpoint is True:\n",
        "    import_previous_compatible_checkpoint(model, force_import=force_import)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vxZF0wOEAQr",
        "outputId": "158dfed8-4781-4e4a-a026-1559d492a1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"mhsa_v1_tf\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 80)]              0         \n",
            "                                                                 \n",
            " tf.cast (TFOpLambda)        (None, 80)                0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 80, 128)           2560000   \n",
            "                                                                 \n",
            " positional_encoding (Positi  (None, 80, 128)          0         \n",
            " onalEncoding)                                                   \n",
            "                                                                 \n",
            " multi_head_self_attention (  (None, 80, 128)          1344000   \n",
            " MultiHeadSelfAttention)                                         \n",
            "                                                                 \n",
            " multi_head_self_attention_1  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_2  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_3  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_4  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_5  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_6  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_7  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 80, 20000)         2580000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,892,000\n",
            "Trainable params: 13,794,848\n",
            "Non-trainable params: 2,097,152\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "OHZurM5ei95K"
      },
      "outputs": [],
      "source": [
        "TPU_GENERATE_ON_CPU = False  # The thing is: both options are slow on TPU :-/\n",
        "\n",
        "class ServiceCallback(keras.callbacks.Callback):\n",
        "#    def on_test_end(self, logs=None):\n",
        "    # @tf.function\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_model_metadata(epoch, suffix=model_suffix)\n",
        "        if (epoch+1) % params['sample_every_n_epochs'] == 0:\n",
        "            idx=random.randint(0,len(td)-1)\n",
        "            text=td.decode(td[idx])\n",
        "            print()\n",
        "            if ml_env.is_tpu is True:\n",
        "                temp_list=[0.7] # [0.6,0.7,0.8]\n",
        "                gen_len=50\n",
        "                with tpu_strategy.scope():\n",
        "                    weights=model.get_weights()\n",
        "                model_cpu.set_weights(weights)\n",
        "                # HDF5 is required for saving weights that originate from TPU\n",
        "                # otherwise this just silently fails...\n",
        "                checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.h5\")\n",
        "                chkpt_dest=checkpoint_path.format(epoch=epoch)\n",
        "                print(f\"Checkpoint: {chkpt_dest}\")\n",
        "                model_cpu.save_weights(chkpt_dest)\n",
        "            else:\n",
        "                temp_list=[0.6, 0.7, 0.8]\n",
        "                gen_len=192\n",
        "            print(f\"prompt: {text}\")\n",
        "            for temp in temp_list:\n",
        "                print(f\"---------------- T={temp} ---------------\")\n",
        "                if ml_env.is_tpu is True and TPU_GENERATE_ON_CPU is True:\n",
        "                    with tf.device('/cpu:0'):\n",
        "                        if temp==0.0:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                        else:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                else:\n",
        "                    if temp==0.0:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                    else:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                td.source_highlight(reply, min_quote_size=10, dark_mode=use_dark_mode, display_ref_anchor=False)\n",
        "            print(\"--------------------------------------\")\n",
        "\n",
        "service_callback=ServiceCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "5SKvObcsEAQ5"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "if ml_env.is_tpu:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='epoch', write_graph=False)\n",
        "else:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "o0Ew6pgWzeFj"
      },
      "outputs": [],
      "source": [
        "# Dont try:\n",
        "#    # use the python variable log_path:\n",
        "#   get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
        "#except:\n",
        "#   pass\n",
        "\n",
        "# The following throws errors on non-colab, but the guarding above is too bug-ridden.\n",
        "# if ml_env.is_tpu is False:\n",
        "#    %tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFbZcN0vxOB"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh2yUKBoEAQ8",
        "outputId": "cbe26e43-c77a-4e61-c9fc-699944558d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING override of sample_every_n_epochs sample-generation to: 200\n"
          ]
        }
      ],
      "source": [
        "EPOCHS=500000\n",
        "if 'current_epoch' in params:\n",
        "    initial_epoch=params['current_epoch']\n",
        "else:\n",
        "    initial_epoch=0\n",
        "\n",
        "override=200\n",
        "print(f\"WARNING override of sample_every_n_epochs sample-generation to: {override}\")\n",
        "params['sample_every_n_epochs']=override"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RLbsTmtnEAQ-",
        "outputId": "ca14e8ff-1b61-41bb-8645-9c8960be1a7b",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83/500000\n",
            " 6/23 [======>.......................] - ETA: 0s - loss: 8.9402"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0061s vs `on_train_batch_end` time: 3.9656s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 45s 48ms/step - loss: 8.9430\n",
            "Epoch 84/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 8.0939\n",
            "Epoch 85/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 7.2898\n",
            "Epoch 86/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 6.8513\n",
            "Epoch 87/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 6.5409\n",
            "Epoch 88/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 6.2590\n",
            "Epoch 89/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 5.9985\n",
            "Epoch 90/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 5.7442\n",
            "Epoch 91/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 5.4941\n",
            "Epoch 92/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 5.2565\n",
            "Epoch 93/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 5.0279\n",
            "Epoch 94/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 4.7909\n",
            "Epoch 95/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 4.5612\n",
            "Epoch 96/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 4.3592\n",
            "Epoch 97/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 4.1346\n",
            "Epoch 98/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 3.9360\n",
            "Epoch 99/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 3.7506\n",
            "Epoch 100/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 3.5964\n",
            "Epoch 101/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 3.4032\n",
            "Epoch 102/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 3.2489\n",
            "Epoch 103/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 3.0866\n",
            "Epoch 104/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.9525\n",
            "Epoch 105/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.8223\n",
            "Epoch 106/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.7068\n",
            "Epoch 107/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.5969\n",
            "Epoch 108/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.5036\n",
            "Epoch 109/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.3985\n",
            "Epoch 110/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.3187\n",
            "Epoch 111/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 2.2355\n",
            "Epoch 112/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.1675\n",
            "Epoch 113/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 2.1020\n",
            "Epoch 114/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 2.0371\n",
            "Epoch 115/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.9809\n",
            "Epoch 116/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.9336\n",
            "Epoch 117/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.8847\n",
            "Epoch 118/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.8453\n",
            "Epoch 119/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.8085\n",
            "Epoch 120/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.7658\n",
            "Epoch 121/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.7324\n",
            "Epoch 122/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.7003\n",
            "Epoch 123/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.6735\n",
            "Epoch 124/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.6466\n",
            "Epoch 125/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.6216\n",
            "Epoch 126/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.6068\n",
            "Epoch 127/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.5824\n",
            "Epoch 128/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.5661\n",
            "Epoch 129/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.5496\n",
            "Epoch 130/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.5298\n",
            "Epoch 131/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.5160\n",
            "Epoch 132/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.5035\n",
            "Epoch 133/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.4900\n",
            "Epoch 134/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.4742\n",
            "Epoch 135/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.4664\n",
            "Epoch 136/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 1.4550\n",
            "Epoch 137/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.4479\n",
            "Epoch 138/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.4359\n",
            "Epoch 139/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.4307\n",
            "Epoch 140/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.4177\n",
            "Epoch 141/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.4132\n",
            "Epoch 142/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.4057\n",
            "Epoch 143/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.4028\n",
            "Epoch 144/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3944\n",
            "Epoch 145/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3861\n",
            "Epoch 146/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3824\n",
            "Epoch 147/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3759\n",
            "Epoch 148/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3703\n",
            "Epoch 149/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3685\n",
            "Epoch 150/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3626\n",
            "Epoch 151/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3605\n",
            "Epoch 152/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3561\n",
            "Epoch 153/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3536\n",
            "Epoch 154/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3490\n",
            "Epoch 155/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3495\n",
            "Epoch 156/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3421\n",
            "Epoch 157/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3384\n",
            "Epoch 158/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3360\n",
            "Epoch 159/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3352\n",
            "Epoch 160/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3326\n",
            "Epoch 161/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3314\n",
            "Epoch 162/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3280\n",
            "Epoch 163/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3249\n",
            "Epoch 164/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3196\n",
            "Epoch 165/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3208\n",
            "Epoch 166/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 1.3214\n",
            "Epoch 167/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3180\n",
            "Epoch 168/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3156\n",
            "Epoch 169/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3171\n",
            "Epoch 170/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3118\n",
            "Epoch 171/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3119\n",
            "Epoch 172/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3076\n",
            "Epoch 173/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3054\n",
            "Epoch 174/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3059\n",
            "Epoch 175/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3067\n",
            "Epoch 176/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3041\n",
            "Epoch 177/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.3027\n",
            "Epoch 178/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2987\n",
            "Epoch 179/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.3047\n",
            "Epoch 180/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2972\n",
            "Epoch 181/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2991\n",
            "Epoch 182/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2953\n",
            "Epoch 183/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2961\n",
            "Epoch 184/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2967\n",
            "Epoch 185/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2943\n",
            "Epoch 186/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2922\n",
            "Epoch 187/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2915\n",
            "Epoch 188/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2928\n",
            "Epoch 189/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2919\n",
            "Epoch 190/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2910\n",
            "Epoch 191/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2894\n",
            "Epoch 192/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2901\n",
            "Epoch 193/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2892\n",
            "Epoch 194/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2882\n",
            "Epoch 195/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2897\n",
            "Epoch 196/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2871\n",
            "Epoch 197/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2849\n",
            "Epoch 198/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2857\n",
            "Epoch 199/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2878\n",
            "Epoch 200/500000\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.2835\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-0199.h5\n",
            "prompt:  at all in my style of beauty. I hate a florid\n",
            "complexion and dark eyes in a man. However, he is very well. Amazingly\n",
            "conceited, I am sure. I took him down several times, you know, in my\n",
            "way.”\n",
            "\n",
            "When the young ladies next met, they had a far more interesting subject\n",
            "to discuss. James Morland’s second letter was then received, and the\n",
            "kind intentions of his father \n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " of her ee of<br>-, at it Sh<span style=\"background-color:#ebdef0;\">e her moth</span>ster ft lu’s ful  with he time , e hasd.<br><br>however,. Nothingsy m.”<br><br>“Thwhich i could g<span style=\"background-color:#d4e6f1;\">e of most </span>fesha-, or —me<br><span style=\"background-color:#d8daef;\"> would only </span>! Maryould be ill; vi. rather cti<span style=\"background-color:#d4efdf;\">ontent her</span> , a us“very s"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 27s 1s/step - loss: 1.2835\n",
            "Epoch 201/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2846\n",
            "Epoch 202/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2809\n",
            "Epoch 203/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2841\n",
            "Epoch 204/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2825\n",
            "Epoch 205/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2836\n",
            "Epoch 206/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2839\n",
            "Epoch 207/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2811\n",
            "Epoch 208/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2829\n",
            "Epoch 209/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2797\n",
            "Epoch 210/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2798\n",
            "Epoch 211/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2798\n",
            "Epoch 212/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2784\n",
            "Epoch 213/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2792\n",
            "Epoch 214/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2783\n",
            "Epoch 215/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2777\n",
            "Epoch 216/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2779\n",
            "Epoch 217/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2748\n",
            "Epoch 218/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2776\n",
            "Epoch 219/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2779\n",
            "Epoch 220/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2734\n",
            "Epoch 221/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2771\n",
            "Epoch 222/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2767\n",
            "Epoch 223/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2750\n",
            "Epoch 224/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2772\n",
            "Epoch 225/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2771\n",
            "Epoch 226/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2723\n",
            "Epoch 227/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2749\n",
            "Epoch 228/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2729\n",
            "Epoch 229/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2729\n",
            "Epoch 230/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2722\n",
            "Epoch 231/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2740\n",
            "Epoch 232/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2750\n",
            "Epoch 233/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2706\n",
            "Epoch 234/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2710\n",
            "Epoch 235/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2705\n",
            "Epoch 236/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2723\n",
            "Epoch 237/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2708\n",
            "Epoch 238/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2717\n",
            "Epoch 239/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2705\n",
            "Epoch 240/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2679\n",
            "Epoch 241/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2684\n",
            "Epoch 242/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2686\n",
            "Epoch 243/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2706\n",
            "Epoch 244/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2701\n",
            "Epoch 245/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2696\n",
            "Epoch 246/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2677\n",
            "Epoch 247/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2657\n",
            "Epoch 248/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2689\n",
            "Epoch 249/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2668\n",
            "Epoch 250/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2669\n",
            "Epoch 251/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2650\n",
            "Epoch 252/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 1.2650\n",
            "Epoch 253/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2627\n",
            "Epoch 254/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2635\n",
            "Epoch 255/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2649\n",
            "Epoch 256/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2634\n",
            "Epoch 257/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 1.2639\n",
            "Epoch 258/500000\n"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    steps_per_epoch=restricted_batches//params['batch_size']\n",
        "    if steps_per_epoch < 1:\n",
        "        steps_per_epoch = 1\n",
        "    history = model.fit(dataset, epochs=EPOCHS, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, callbacks=[service_callback]) # for TPU we need to role our own checkpointer since we need to transfer the weights\n",
        "else:\n",
        "    history = model.fit(dataset, validation_data=validation_dataset, epochs=EPOCHS, initial_epoch=initial_epoch, callbacks=[checkpoint_callback, tensorboard_callback, service_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a81LdPyY2dyo"
      },
      "outputs": [],
      "source": [
        "model_cpu.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDNYZiEQtgF"
      },
      "outputs": [],
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "def doDialog(model):\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "    print(\"Please enter some dialog.\")\n",
        "    print(\"The net will answer according to your input.\")\n",
        "    print(\"'bye' for end,\")\n",
        "    print(\"'reset' to reset the conversation context,\")\n",
        "    print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "    print(\"    to change character of the dialog.\")\n",
        "    print(\"    Current temperature={}.\".format(temperature))\n",
        "    print()\n",
        "    xso = None\n",
        "    bye = False\n",
        "    doini = True\n",
        "    bye = False\n",
        "    while not bye:\n",
        "        print(\"> \", end=\"\")\n",
        "        prompt = input()\n",
        "        if prompt == 'bye':\n",
        "            bye = True\n",
        "            print(\"Good bye!\")\n",
        "            continue\n",
        "        if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "            t = float(prompt[len(\"temperature=\"):])\n",
        "            if t > 0.05 and t < 1.4:\n",
        "                temperature = t\n",
        "                print(\"(generator temperature now {})\".format(t))\n",
        "                print()\n",
        "                continue\n",
        "            print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "            continue\n",
        "        reply=mhsa_generate(model, prompt, gen_len=256, temperature=temperature, verbose=True)\n",
        "        td.source_highlight(reply, min_quote_size=13, dark_mode=use_dark_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JEPK2WIQtgI"
      },
      "outputs": [],
      "source": [
        "# Talk to the net!\n",
        "doDialog(model_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMCWf5AZn1-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "VmWbteSFQtfq",
        "yWE_ZZMKEARV"
      ],
      "machine_shape": "hm",
      "name": "transformer_poet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}