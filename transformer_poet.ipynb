{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/transformer-poet/blob/main/transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtpy59Yq-Qfz",
    "outputId": "05361930-c01b-4f0d-933d-f78e78389778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-indie-tools in /home/dsc/.local/lib/python3.10/site-packages (0.3.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:27:02.402130: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5T4m6earb1e",
    "outputId": "08c72f63-1bd1-4a4e-da8d-9924f610dd5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TF-Keras version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.keras_custom_layers import MultiHeadSelfAttention, PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A tensorflow deep multi-head attention model for text generation\n",
    "\n",
    "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
    "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "llPw84PkEAP2",
    "outputId": "f79216ee-8e33-4861-d1fe-3e5ca0ad29e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:27:09.183890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:27:09.371781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:27:09.371990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:27:09.372493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OS: Linux, Python: 3.10.8, Jupyter Notebook Tensorflow: 2.11.0, GPU: NVIDIA GeForce GTX 1080 Ti (8MiB / 11264MiB)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oZ6t9b6ZwSxi"
   },
   "outputs": [],
   "source": [
    "use_eager=tf.executing_eagerly()\n",
    "if ml_env.is_tpu is True:\n",
    "    tpu_strategy = ml_env.tpu_strategy\n",
    "    tpu_is_init=True\n",
    "    if use_eager is True:\n",
    "        tf.config.run_functions_eagerly(False)\n",
    "    use_eager=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-TP3Pnsrb1f",
    "outputId": "6fa49040-4d9d-4895-f91f-f8137780803e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/mhsa_v1_tf (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "project_name='women_writers'\n",
    "model_name='mhsa_v1_tf'\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
    "encoding, batch generation, and formatted source display. It read some \n",
    "books from Project Gutenberg and supports creation of training batches. \n",
    "The output functions support highlighting to allow to compare generated \n",
    "texts with the actual sources to help to identify identical (memorized) \n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C66X7ynnrb1h",
    "outputId": "3d9a018b-65dd-4cdb-e8e4-fb90a3489ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
     ]
    }
   ],
   "source": [
    "# sample searches\n",
    "search_spec= {\"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
    "\n",
    "book_list=gd.search(search_spec)\n",
    "book_cnt = len(book_list)\n",
    "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "if book_cnt<40:\n",
    "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
    "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
    "else:\n",
    "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MH6_7IU3upOd",
    "outputId": "c6aadb04-88cc-47ba-f886-71b8b0a84cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The Common Reader - Virginia Woolf, 64457\n",
      "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
      "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
      "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
      "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
      "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
      "6: Pride and Prejudice - Jane Austen, 42671\n",
      "7: The Letters of Jane Austen - Jane Austen, 42078\n",
      "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
      "9: Jacob's Room - Virginia Woolf, 5670\n",
      "10: Pride and Prejudice - Jane Austen, 1342\n",
      "11: Night and Day - Virginia Woolf, 1245\n",
      "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
      "13: Lady Susan - Jane Austen, 946\n",
      "14: Wuthering Heights - Emily Brontë, 768\n",
      "15: Sense and Sensibility - Jane Austen, 161\n",
      "16: Emma - Jane Austen, 158\n",
      "17: The Voyage Out - Virginia Woolf, 144\n",
      "18: Mansfield Park - Jane Austen, 141\n",
      "19: Northanger Abbey - Jane Austen, 121\n",
      "20: Persuasion - Jane Austen, 105\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(book_list)):\n",
    "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jBH3Z15rb1h",
    "outputId": "6597c559-0dea-4a32-ea5d-c84ba3bcd84b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loaded 12 texts\n",
      "INFO:Datasets:Extracting ngrams of length 1..8 from text_list, selecting 20000 most used ngrams.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
      "2: Jacob's Room - Virginia Woolf\n",
      "3: Pride and Prejudice - Jane Austen\n",
      "4: Night and Day - Virginia Woolf\n",
      "5: Lady Susan - Jane Austen\n",
      "6: Wuthering Heights - Emily Brontë\n",
      "7: Sense and Sensibility - Jane Austen\n",
      "8: Emma - Jane Austen\n",
      "9: The Voyage Out - Virginia Woolf\n",
      "10: Mansfield Park - Jane Austen\n",
      "11: Northanger Abbey - Jane Austen\n",
      "12: Persuasion - Jane Austen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Encoding text corpora as ngrams.\n",
      "INFO:Datasets:Encoding text Mr. Bennett and Mrs. Brown...\n",
      "INFO:Datasets:Encoding text Jacob's Room...\n",
      "INFO:Datasets:Encoding text Pride and Prejudice...\n",
      "INFO:Datasets:Encoding text Night and Day...\n",
      "INFO:Datasets:Encoding text Lady Susan...\n",
      "INFO:Datasets:Encoding text Wuthering Heights...\n",
      "INFO:Datasets:Encoding text Sense and Sensibility...\n",
      "INFO:Datasets:Encoding text Emma...\n",
      "INFO:Datasets:Encoding text The Voyage Out...\n",
      "INFO:Datasets:Encoding text Mansfield Park...\n",
      "INFO:Datasets:Encoding text Northanger Abbey...\n",
      "INFO:Datasets:Encoding text Persuasion...\n",
      "INFO:Datasets:Encoding text corpora as ngrams done.\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 20000  # This becomes vocab_size\n",
    "MAX_NGRAM_LEN = 8   # Max length of a token\n",
    "\n",
    "select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "\n",
    "print(\"Using:\")\n",
    "for i in range(len(sub_book_list)):\n",
    "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "textlib_dataset = None  # Forces re-caching\n",
    "td = Text_Dataset(sub_book_list)\n",
    "td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7_tc2Lirb1i",
    "outputId": "faf15399-3a0d-4552-ff72-9223143826cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1526923 records\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LEN = 80\n",
    "SUB_PROBABILITY = 0.15  # like BERT\n",
    "\n",
    "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN, content_stepping=1)\n",
    "\n",
    "num_records = len(td)\n",
    "\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_batch(td, batch_size, length, SUB_probability=0.15):\n",
    "    for i in range(batch_size):\n",
    "        Xi = td.get_random_item()\n",
    "        yi = Xi.copy()\n",
    "        l=int(len(Xi)*SUB_probability)\n",
    "        for li in range(l):\n",
    "            pos=random.randint(0,len(Xi)-1)\n",
    "            if td.tokenizer_type=='char':\n",
    "                Xi[pos]=td.c2i['␚']\n",
    "            elif td.tokenizer_type=='word':\n",
    "                Xi[pos]=td.w2i['<subst>']\n",
    "            elif td.tokenizer_type=='ngram':\n",
    "                Xi[pos]=td.t2i['<subst>']\n",
    "            else:\n",
    "                print(f\"Unexpected tokenizer_type {td.tokenizer_type}\")\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TI3Fx6bNuR9A",
    "outputId": "0d25536b-daa8-408f-ddbf-28bb71a265b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0](l=80): X=>alue of such a\n",
      "situation in point of privile<subst>and independence beyond all\n",
      "calculation. <subst>You_ think <subst>, I <subst>” (<subst>with a softened\n",
      "voice to <subst>). “Have you ever seen the place?”\n",
      "\n",
      "Fanny gave a quick negative, and tried <subst>de her interest in the\n",
      "<subst>by an eager attention to her <subst>, who was driv<subst> hard a\n",
      "bargain, and imposing on her as much as he <, y=>alue of such a\n",
      "situation in point of privilege and independence beyond all\n",
      "calculation. _You_ think with me, I hope” (turning with a softened\n",
      "voice to Fanny). “Have you ever seen the place?”\n",
      "\n",
      "Fanny gave a quick negative, and tried to hide her interest in the\n",
      "subject by an eager attention to her brother, who was driving as hard a\n",
      "bargain, and imposing on her as much as he <\n",
      "[1](l=80): X=>egre<subst> own regard, nor of <subst>\n",
      "reasonable<subst> She has known him <subst>a fortnight. She danced four<subst><subst>with him<subst>Meryton; she saw him one morning at his own house,\n",
      "and has <subst>dined in company with him f<subst>imes. This is not quite\n",
      "enough to make <subst>understand his character.”\n",
      "\n",
      "“Not as you represent it. Had she merely _<subst>ed_ <subst>, she might\n",
      "only have discover<, y=>egree of her own regard, nor of its\n",
      "reasonableness. She has known him only a fortnight. She danced four\n",
      "dances with him at Meryton; she saw him one morning at his own house,\n",
      "and has since dined in company with him four times. This is not quite\n",
      "enough to make her understand his character.”\n",
      "\n",
      "“Not as you represent it. Had she merely _dined_ with him, she might\n",
      "only have discover<\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = get_sample_batch(td, 2, 40, SUB_probability=SUB_PROBABILITY)\n",
    "for i in range(len(test_x)):\n",
    "    xi=[int(x) for x in test_x[i]]\n",
    "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<, y=>{td.decode(test_y[i])}<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnMxRkkmcOeX",
    "outputId": "815e8959-5fea-4aa9-e523-7a8913b5972f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 80), (2, 80))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. Use tf.data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jn_LcJ6g9Mzy"
   },
   "outputs": [],
   "source": [
    "def expand_name_template(template, params):\n",
    "    exp=copy.copy(template)\n",
    "    for key in params:\n",
    "        src=\"{\"+key+\"}\"\n",
    "        dst=f\"{params[key]}\"\n",
    "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
    "    return exp\n",
    "\n",
    "def save_model_metadata(epoch, suffix='std'):\n",
    "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
    "    params['current_epoch'] = epoch\n",
    "    try:\n",
    "        with open(meta_file, 'w') as f:\n",
    "            f.write(json.dumps(params))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def read_model_metadata(suffix=\"std\"):\n",
    "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
    "    try:\n",
    "        with open(meta_file, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
    "        return None\n",
    "    return meta\n",
    "\n",
    "def is_metadata_compatible(params, meta):\n",
    "    is_valid=True\n",
    "    keys=set(list(params.keys())+list(meta.keys()))\n",
    "    for key in keys:\n",
    "        if key in updatable_keys:\n",
    "            continue\n",
    "        if key not in meta:\n",
    "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "        elif key not in params:\n",
    "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "        elif meta[key]!=params[key]:\n",
    "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "    if is_valid is False:\n",
    "        print(\"Aborting import.\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znpIUA3ig3gO",
    "outputId": "468721ae-35d6-4c30-c8a0-178f14d6dec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing last session from epoch 1\n",
      "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 4, 'heads': [2, 2, 2, 2], 'units': [256, 256, 256, 256], 'norm': 'softmax', 'mh_normalize': True, 'l2_regularizer': 1e-09, 'dropout': 0.0, 'join_heads_by_add': True, 'recurrent_layers': [0, 1, 3], 'gated_memory_layers': [2], 'vocab_size': 20000, 'sequence_len': 80, 'embedding_size': 32, 'batch_size': 256, 'learning_rate': 0.002, 'clipvalue': None, 'sample_every_n_epochs': 1, 'current_epoch': 1}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
    "\n",
    "lyrs = 4;\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "    'mhsa_layers': lyrs, \n",
    "    'heads': [2]*lyrs,\n",
    "    'units': [256]*lyrs,  # 0 inserts an LSTM for memory-states :-)\n",
    "    'norm': 'softmax', # this is for within each head\n",
    "    'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
    "    'l2_regularizer': 1e-9,\n",
    "    'dropout': 0.0,       # no dropout: 0.0\n",
    "    'join_heads_by_add': True,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads\n",
    "    'recurrent_layers': [0,1,3],\n",
    "    'gated_memory_layers': [2],\n",
    "    'vocab_size': vocabulary_size,\n",
    "    'sequence_len': SEQUENCE_LEN,\n",
    "    'embedding_size': 32,\n",
    "\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.002,\n",
    "    'clipvalue': None,\n",
    "    'sample_every_n_epochs': 1,\n",
    "}\n",
    "\n",
    "if len(params['heads'])!=params['mhsa_layers'] or len(params['units'])!=params['mhsa_layers']:\n",
    "    print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
    "    \n",
    "if ml_env.is_tpu is True:\n",
    "    lr = params['learning_rate']*1.0\n",
    "else:\n",
    "    lr = params['learning_rate']\n",
    "\n",
    "model_suffix = expand_name_template(params['name'], params)\n",
    "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
    "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
    "if os.path.exists(checkpoint_dir) is False:\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# When comparing if training-data is compatible with new params set, \n",
    "# the following keys are updatable, they can be changed while continuing\n",
    "# to use existing checkpoints and continue training with those values\n",
    "# changed:\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
    "             'sample_every_n_epochs']\n",
    "\n",
    "# These values are taking from saved checkpoint:\n",
    "keep_keys=['current_epoch']\n",
    "\n",
    "continue_last = True\n",
    "if continue_last is False:\n",
    "    print(\"NOT continuing based on existing training! New start.\")\n",
    "\n",
    "meta = read_model_metadata(suffix=model_suffix)\n",
    "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
    "    for key in keep_keys:\n",
    "        if key in meta:\n",
    "            params[key]=meta[key]\n",
    "    if params is not None:\n",
    "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
    "    else:\n",
    "        print(f\"No previous data, starting new model\")\n",
    "else:\n",
    "    print(\"Starting new model\")\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jY3hUuhQYzdT",
    "outputId": "03f7228b-60c2-4537-8871-12e2bd6ca852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 5964\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EeB7jugCV4lI"
   },
   "outputs": [],
   "source": [
    "# @tf.function   (only slows things down [considerably!])\n",
    "def make_tf_dataset(num, random_index=False, SUB_probability=0.0):\n",
    "    dx=[]\n",
    "    dy=[]\n",
    "    num_batches_active = num\n",
    "    for i in range(num_batches_active):\n",
    "        x,y=get_sample_batch(td, params['batch_size'], params['sequence_len'], SUB_probability=SUB_probability)\n",
    "        if i<1:\n",
    "            print(f\"[{num} x]: {x.shape} -> {y.shape}\")\n",
    "        dx.append(x)\n",
    "        dy.append(y)\n",
    "    dx=np.array(dx)\n",
    "    dy=np.array(dy)\n",
    "    print(f\"dx.shape={dx.shape}, dy.shape={dy.shape}\")\n",
    "    data_xy = (dx, dy)\n",
    "    tf_dataset=tf.data.Dataset.from_tensor_slices(data_xy)\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCy7WmQyS9T-",
    "outputId": "a560da5e-a743-4217-fd9d-c59965378957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5964 batches\n",
      "Creating dataset, this is slow. Be patient...\n",
      "[5964 x]: (256, 80) -> (256, 80)\n",
      "dx.shape=(5964, 256, 80), dy.shape=(5964, 256, 80)\n",
      "Dataset done and cached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:29:24.199421: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 14:29:24.201239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:29:24.201433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:29:24.201585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:29:24.603195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:29:24.603409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:29:24.603562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:29:24.603688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10415 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "2022-12-12 14:29:24.603970: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_BATCHES = 50000\n",
    "\n",
    "if num_batches>MAX_NUM_BATCHES:\n",
    "    restricted_batches=MAX_NUM_BATCHES\n",
    "    print(f\"Restrictinig {num_batches} to max of {restricted_batches}\")\n",
    "else:\n",
    "    restricted_batches=num_batches\n",
    "    print(f\"{restricted_batches} batches\")\n",
    "if cached_batch_data == restricted_batches and textlib_dataset is not None:\n",
    "    print(\"Reusing cached training-data\")\n",
    "else:\n",
    "    print(\"Creating dataset, this is slow. Be patient...\")\n",
    "    textlib_dataset = make_tf_dataset(restricted_batches, SUB_probability=SUB_PROBABILITY)\n",
    "    cached_batch_data = restricted_batches\n",
    "    print(\"Dataset done and cached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boow8wR7sLwi",
    "outputId": "0da44dce-dd38-4d4f-defc-6fba22d35225"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(256, 80), dtype=tf.int32, name=None), TensorSpec(shape=(256, 80), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_buffer=10000\n",
    "if ml_env.is_tpu is True:\n",
    "    dataset=textlib_dataset.shuffle(shuffle_buffer).repeat()  # Otherwise TPU may run dry\n",
    "else:\n",
    "    dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "B-G5HLMqqbeT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 x]: (256, 80) -> (256, 80)\n",
      "dx.shape=(10, 256, 80), dy.shape=(10, 256, 80)\n"
     ]
    }
   ],
   "source": [
    "if ml_env.is_tpu is False:\n",
    "    validation_dataset = make_tf_dataset(10, random_index=True, SUB_probability=SUB_PROBABILITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZAzFlCVBiL0Q"
   },
   "outputs": [],
   "source": [
    "def model_mhsa(inputs, params):\n",
    "    dense = layers.Dense(params['vocab_size'], kernel_regularizer=regularizers.l2(params['l2_regularizer']))  # using softmax here prevents temperature adjust, affects 'from_logits' param in sparse_categorical loss \n",
    "    fl = layers.Flatten()\n",
    "    dr = layers.Dropout(params['dropout'])\n",
    "    pe = PositionalEncoding(amplitude=0.3)\n",
    "    rs_up = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
    "    if 0 in params['units']:  # XXX remove!\n",
    "        lstm1 = layers.LSTM(units=vocabulary_size, return_sequences=True)\n",
    "    if vocabulary_size>=300:\n",
    "        emb=layers.Embedding(vocabulary_size,params['embedding_size'],input_length=params['sequence_len'])\n",
    "    rs_down = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
    "    mhsa=[]\n",
    "    residuals=[]\n",
    "    #if params['gated_memory'] is True:\n",
    "    #    params['recurrent']=False\n",
    "\n",
    "    for i in range(params['mhsa_layers']):\n",
    "        if params['units'][i]==0:  # XXX remove!\n",
    "            mhsa.append(None)\n",
    "            residuals.append(i)\n",
    "        else:\n",
    "            if i in params['recurrent_layers']:\n",
    "                rec=True\n",
    "            else:\n",
    "                rec=False\n",
    "            if i in params['gated_memory_layers']:\n",
    "                mem=True\n",
    "            else:\n",
    "                mem=False\n",
    "            mhsa.append(MultiHeadSelfAttention(params['heads'][i], units=params['units'][i], norm=params['norm'], mh_normalize=params['mh_normalize'], join_heads_by_add=params['join_heads_by_add'], recurrent=rec, gated_memory=mem))\n",
    "            \n",
    "    xint = tf.cast(inputs,dtype=tf.int32)\n",
    "    if vocabulary_size<300:\n",
    "        x = tf.one_hot(xint, params['vocab_size'], axis=-1)\n",
    "    else:\n",
    "        x = emb(xint)\n",
    "    x = pe(x)\n",
    "    # if params['recurrent'] is True:\n",
    "    #     mem = x\n",
    "    for i in range(len(mhsa)):\n",
    "        if i in residuals:  # XXX remove!\n",
    "            x = rs_down(lstm1(rs_up(x)))+x\n",
    "            print(f\"Residual at layer {i} added.\")\n",
    "        else:\n",
    "            # if params['recurrent'] is True:\n",
    "            #     x, mem = mhsa[i](x, mem)\n",
    "            x = mhsa[i](x)\n",
    "        # x = mhsa[i](x,x)\n",
    "    if params['dropout']>0.0:\n",
    "        x = dr(x)\n",
    "    # x = dense(fl(x))\n",
    "    x = dense(x)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4J13Gp_hjqqn"
   },
   "outputs": [],
   "source": [
    "def mhsa_generate(model, text, gen_len=64, temperature=0.9, argmax=False, verbose=False):\n",
    "    if verbose is True:\n",
    "        full=text[:-1]\n",
    "    gen_text=\"\"\n",
    "    lf=0\n",
    "    input = np.array(td.encode(text))\n",
    "    while len(input) < params['sequence_len']:\n",
    "        input = np.concatenate([td.encode('<pad>'),input])\n",
    "    for i in range(gen_len):\n",
    "        input = np.concatenate([input[1:],td.encode('<subst>')])\n",
    "        if len(input)!=params['sequence_len']:\n",
    "            print('assertion failure')\n",
    "            return None\n",
    "        pred = model(input)\n",
    "        pred /= temperature\n",
    "        pred = tf.keras.layers.Softmax()(pred)\n",
    "        if tf.executing_eagerly() is True and ml_env.is_tpu is False:\n",
    "            pred=pred.numpy()\n",
    "        else:\n",
    "            pred=tf.keras.backend.eval(pred)  # this is a cheat, it internaly used Numpy() too.\n",
    "        if argmax is True:\n",
    "            pred=np.argmax(pred[0],axis=1)\n",
    "        else:\n",
    "            pred = [np.random.choice(list(range(len(pred[0][-1]))), p=pred[0][-1])]\n",
    "        input = np.concatenate([input[1:],[pred[-1]]])\n",
    "        c = td.decode([pred[-1]])\n",
    "        if verbose is True:\n",
    "            print(c, end='')\n",
    "            if c=='\\n':\n",
    "                lf=0\n",
    "            else:\n",
    "                lf += 1\n",
    "                if (lf>80 and c==' ') or lf>120:\n",
    "                    print()\n",
    "                    lf=0\n",
    "            full+=c\n",
    "        gen_text+=c\n",
    "    if verbose is True:\n",
    "        print()\n",
    "    return gen_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nf-NHZ326NqJ",
    "outputId": "f060d237-ee10-42de-8ca8-beaf784e2fb5"
   },
   "outputs": [],
   "source": [
    "if ml_env.is_tpu is True:\n",
    "    with tpu_strategy.scope():\n",
    "        print(\"Creating TPU-scope model\")\n",
    "        inputs = keras.Input(shape=(params['sequence_len'],))\n",
    "        outputs = model_mhsa(inputs, params)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
    "    print(\"Creating Default-scope model\")\n",
    "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
    "    outputs = model_mhsa(inputs, params)\n",
    "    model_cpu = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
    "else:\n",
    "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
    "    outputs = model_mhsa(inputs, params)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
    "    model_cpu = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SXx-nBe5-jyJ"
   },
   "outputs": [],
   "source": [
    "def get_newest_checkpoint(checkpoint_dir):\n",
    "    files = os.listdir(checkpoint_dir)\n",
    "    paths = [os.path.join(checkpoint_dir, basename) for basename in files]\n",
    "    return max(paths, key=os.path.getctime)\n",
    "\n",
    "def import_previous_compatible_checkpoint(model, force_import=False):\n",
    "    meta = read_model_metadata(suffix=model_suffix)\n",
    "    if meta is None:\n",
    "        print(\"No previous checkpoint found\")\n",
    "        return False\n",
    "    if is_metadata_compatible(params, meta) is not True and force_import is False:\n",
    "        print(\"No useable import found.\")\n",
    "        return False\n",
    "    try:\n",
    "        last_checkpoint = get_newest_checkpoint(checkpoint_dir) # Doesn't do anything: tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot determine last checkpoint in {checkpoint_dir}, cannot import due to: {e}\")\n",
    "        return False\n",
    "    print(f\"Last checkpoint: {last_checkpoint}\")\n",
    "    try:\n",
    "        model.load_weights(last_checkpoint)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to import model {last_checkpoint}: {e}\")\n",
    "        return False\n",
    "    if 'current_epoch' in meta:\n",
    "        params['current_epoch'] = meta['current_epoch']\n",
    "    print(f\"Successful import of epoch {params['current_epoch']} from {last_checkpoint}, continuing from there...\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soB-Q8YXvndE"
   },
   "source": [
    "### Loss function, optimizer, tensorboard output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0t5JWEdYZNGz"
   },
   "outputs": [],
   "source": [
    "kscc = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "def loss(labels, logits):\n",
    "  vl=kscc(labels, logits)\n",
    "  return vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jc2kbGoAZXHi"
   },
   "outputs": [],
   "source": [
    "if params['clipvalue'] is not None:\n",
    "    if ml_env.is_tpu is True:\n",
    "        with tpu_strategy.scope():\n",
    "            opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
    "    else:\n",
    "        opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
    "else:\n",
    "    if ml_env.is_tpu is True:\n",
    "        with tpu_strategy.scope():\n",
    "            opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    else:\n",
    "        opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "if ml_env.is_tpu is True:\n",
    "    with tpu_strategy.scope():\n",
    "        model.compile(optimizer=opti, loss=loss, metrics=[], run_eagerly=False, jit_compile=True)\n",
    "else:\n",
    "    model.compile(optimizer=opti, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAoMxogcX_Nq",
    "outputId": "f4ac54b0-a7eb-4bfb-87d3-36d89d568786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last checkpoint: ./model/mhsa_v1_tf/training_checkpoints_4x(2, 2, 2, 2)x(256, 256, 256, 256)x20000/checkpoint\n",
      "Failed to import model ./model/mhsa_v1_tf/training_checkpoints_4x(2, 2, 2, 2)x(256, 256, 256, 256)x20000/checkpoint: Unable to open file (file signature not found)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:29:25.940636: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ./model/mhsa_v1_tf/training_checkpoints_4x(2, 2, 2, 2)x(256, 256, 256, 256)x20000/checkpoint: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    }
   ],
   "source": [
    "import_checkpoint = True\n",
    "force_import = False   # True: ignore metadata and try import anyway. This will of course crash, if the new model doesn't fit the checkpoint-data...\n",
    "\n",
    "if import_checkpoint is True:\n",
    "    import_previous_compatible_checkpoint(model, force_import=force_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vxZF0wOEAQr",
    "outputId": "35df2e4d-2ddc-42ca-dbde-9920e06a8c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mhsa_v1_tf\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 80)]              0         \n",
      "                                                                 \n",
      " tf.cast (TFOpLambda)        (None, 80)                0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 80, 32)            640000    \n",
      "                                                                 \n",
      " positional_encoding (Positi  (None, 80, 32)           0         \n",
      " onalEncoding)                                                   \n",
      "                                                                 \n",
      " multi_head_self_attention (  (None, 80, 32)           84096     \n",
      " MultiHeadSelfAttention)                                         \n",
      "                                                                 \n",
      " multi_head_self_attention_1  (None, 80, 32)           84096     \n",
      "  (MultiHeadSelfAttention)                                       \n",
      "                                                                 \n",
      " multi_head_self_attention_2  (None, 80, 32)           100480    \n",
      "  (MultiHeadSelfAttention)                                       \n",
      "                                                                 \n",
      " multi_head_self_attention_3  (None, 80, 32)           84096     \n",
      "  (MultiHeadSelfAttention)                                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 80, 20000)         660000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,652,768\n",
      "Trainable params: 1,570,848\n",
      "Non-trainable params: 81,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OHZurM5ei95K"
   },
   "outputs": [],
   "source": [
    "TPU_GENERATE_ON_CPU = False  # The thing is: both options are slow on TPU :-/\n",
    "\n",
    "class ServiceCallback(keras.callbacks.Callback):\n",
    "#    def on_test_end(self, logs=None):\n",
    "    # @tf.function\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        save_model_metadata(epoch, suffix=model_suffix)\n",
    "        if (epoch+1) % params['sample_every_n_epochs'] == 0:\n",
    "            idx=random.randint(0,len(td)-1)\n",
    "            text=td.decode(td[idx])\n",
    "            print()\n",
    "            if ml_env.is_tpu is True:\n",
    "                temp_list=[0.7] # [0.6,0.7,0.8]\n",
    "                gen_len=50\n",
    "                with tpu_strategy.scope():\n",
    "                    weights=model.get_weights()\n",
    "                model_cpu.set_weights(weights)\n",
    "                # HDF5 is required for saving weights that originate from TPU\n",
    "                # otherwise this just silently fails...\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.h5\")\n",
    "                chkpt_dest=checkpoint_path.format(epoch=epoch)\n",
    "                print(f\"Checkpoint: {chkpt_dest}\")\n",
    "                model_cpu.save_weights(chkpt_dest)\n",
    "            else:\n",
    "                temp_list=[0.6, 0.7, 0.8]\n",
    "                gen_len=192\n",
    "            print(f\"prompt: {text}\")\n",
    "            for temp in temp_list:\n",
    "                print(f\"---------------- T={temp} ---------------\")\n",
    "                if ml_env.is_tpu is True and TPU_GENERATE_ON_CPU is True:\n",
    "                    with tf.device('/cpu:0'):\n",
    "                        if temp==0.0:\n",
    "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
    "                        else:\n",
    "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
    "                else:\n",
    "                    if temp==0.0:\n",
    "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
    "                    else:\n",
    "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
    "                td.source_highlight(reply, min_quote_size=10, dark_mode=use_dark_mode, display_ref_anchor=False)\n",
    "            print(\"--------------------------------------\")\n",
    "\n",
    "service_callback=ServiceCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "5SKvObcsEAQ5"
   },
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if ml_env.is_tpu:\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='epoch', write_graph=False)\n",
    "else:\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "o0Ew6pgWzeFj"
   },
   "outputs": [],
   "source": [
    "# Dont try:\n",
    "#    # use the python variable log_path:\n",
    "#   get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
    "#except:\n",
    "#   pass\n",
    "\n",
    "# The following throws errors on non-colab, but the guarding above is too bug-ridden.\n",
    "# if ml_env.is_tpu is False:\n",
    "#    %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDFbZcN0vxOB"
   },
   "source": [
    "## The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kh2yUKBoEAQ8",
    "outputId": "25b9f886-2c5a-464b-903f-d29916c67901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING override of sample_every_n_epochs sample-generation to: 1\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=500000\n",
    "if 'current_epoch' in params:\n",
    "    initial_epoch=params['current_epoch']\n",
    "else:\n",
    "    initial_epoch=0\n",
    "\n",
    "override=1\n",
    "print(f\"WARNING override of sample_every_n_epochs sample-generation to: {override}\")\n",
    "params['sample_every_n_epochs']=override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RLbsTmtnEAQ-",
    "outputId": "169f4f5c-9b9c-4b79-87be-3c90773201fa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:29:31.311456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8600\n",
      "2022-12-12 14:29:31.552720: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7ff995f70590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-12 14:29:31.552783: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2022-12-12 14:29:31.557162: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-12-12 14:29:31.557583: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-12-12 14:29:31.669923: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5964/5964 [==============================] - ETA: 0s - loss: 3.0267 - accuracy: 0.6333\n",
      "prompt: due\n",
      "allowance for the influence of a strong passion at war with all\n",
      "interested motives. Mr. Knightley saw no such passion, and of course\n",
      "thought nothing of its effects; but she saw too much of it to feel a\n",
      "doubt of its overcoming any hesitations that a reasonable prudence\n",
      "might originally suggest; and more than a reasonable, becoming degree\n",
      "of prudence, she was very sure did not belong to Mr\n",
      "---------------- T=0.6 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "rowpalation<br>oI s_ckyed as cu: <span style=\"background-color:#e2d7d5;\">s of the<br>g</span>to be a-day <br>ghe<span style=\"background-color:#ecf3cf;\">r. Linton; </span>but <br>tha. A poulatd that hself, an<span style=\"background-color:#eadbd8;\">really talking </span>, gan_ . A? —_I<br>—earnid; ed th<span style=\"background-color:#ecf3cf;\">e I continue</span>general<span style=\"background-color:#f6ddcc;\">like<br>of the </span>n iteven aidmuchevermentphme, was in, kAt<span style=\"background-color:#d6dbdf;\">hen ended </span>—UP? losmade no tch<span style=\"background-color:#e2d7d5;\">’s the same</span>ttaus s, tAPTER X<span style=\"background-color:#eadbd8;\">e were to s</span> to Fitcu; and nfiforl<span style=\"background-color:#eadbd8;\">ess to see </span>?Rachel-bofor mkind<span style=\"background-color:#d4e6f1;\">pen of the </span>so most ly ewill , anby ; and sh<br>oany as<br>knce wPrdno, sifully Har<span style=\"background-color:#e2d7d5;\">should I l</span> f saymu<span style=\"background-color:#edebd0;\">can. I have</span>Sincowever, <br>thougheveningsensibleinto theal sion of :tDenham’s , in the, poFnot dDeMr. up almost  su; and i.—int<span style=\"background-color:#d4efdf;\">o possible</span>. I tch soldo<span style=\"background-color:#d4e6f1;\"> not one w</span>le, or You -peo an<br>o<span style=\"background-color:#ebdef0;\">f to the w</span>neriun. ElbedGgreat ’s <br>oarrivmuch —ver oing : , and thme apu.to i! of a<span style=\"background-color:#d0ece7;\">little tho</span>se “vi. “e<br>sdingch a le_ f he "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#edebd0;\">Jane Austen: Lady Susan</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- T=0.7 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       ". “a rectionsand meanvoifactoking atmost Ainditiche didwao<span style=\"background-color:#ecf3cf;\"> be from h</span>how letterus t,” said , siore ly lleft ’s qualfor mKvEill be mitits .”how cessill omeng<span style=\"background-color:#edebd0;\"> so gentle</span>mforcen<span style=\"background-color:#d6eaf8;\">ame between</span>iserr<br>the priblaownlivemake friend<span style=\"background-color:#eadbd8;\">him looking </span>t.”<br><br>“L.”<br><br>rany. to !houroot<span style=\"background-color:#ebdef0;\">ive people o</span>ught keba Mr. nclHihappinesgrantmentonsn m<span style=\"background-color:#eadbd8;\">rought for</span> escand, claimF’s cwhich . You ly signhereason his smy ad ant<br>re<br>a! ed, ahowever,: , she wasaindied up“<span style=\"background-color:#f6ddcc;\">great; but </span>had hmost  heaabsthe worlin the rd nd<br>ars “, in ever s_: <br><br><br>.<br><br>Mt allput per s, of I ale!”<br><br>“for think<span style=\"background-color:#d4e6f1;\"> Fanny, it</span>. ister asfeltfaceat ach<span style=\"background-color:#d4efdf;\">ve seemed t</span>liine , d<br><br>“No,. Shethe<br>self, nt. which —n’t oment .<br><br>“and g<br>conforepri, aurchng w” she m<span style=\"background-color:#d4efdf;\">y all looked </span>ci<span style=\"background-color:#eadbd8;\">always exc</span>carear,  how e, and om<span style=\"background-color:#eadbd8;\">e he might</span><span style=\"background-color:#e2d7d5;\">ical, and </span>wor<span style=\"background-color:#d4efdf;\">ing. Elinor</span>to the tt<span style=\"background-color:#d0ece7;\">ach; but h</span>urned w<span style=\"background-color:#d4efdf;\">ere Marianne, i</span>glan<br>aousepl! "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#edebd0;\">Jane Austen: Lady Susan</span>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- T=0.8 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       ". WheKatharinShe afterfictime tfriendsher so_islanmost , I aath edeorPndesil<br>in, t<span style=\"background-color:#ecf3cf;\">f you day </span>ubhad (.”<br><br>“Id from t_<br>ahis ddeal ne over<br>“M; ies<span style=\"background-color:#d4e6f1;\"> found as </span>al? its of ely aareever be, -d, pecg him ’s suff much rely edfeel<br>Iconsequerth —“cat-my ingsole—<br>to  out<span style=\"background-color:#e2d7d5;\">on’t think s</span>trochi. Thedor to ,<br>solhat was <span style=\"background-color:#d0ece7;\">s; and had a</span>h.r a knew in <br>had , and t the ted her tPch a The not aRtwo lmo<span style=\"background-color:#e2d7d5;\">st married</span>ked abospe<span style=\"background-color:#eadbd8;\">net the co</span>? your lslways <br>atwns he ‘slin ali<span style=\"background-color:#e2d7d5;\">ght themselv</span>weekous gead been s withing thesufferthe cashe s<span style=\"background-color:#d0ece7;\">ately that she</span>of<br>versationestmiture _contebeingbasituatiooteh to , was_planici: th.<br>now al akein rk<span style=\"background-color:#ebdef0;\">little man</span>y .<br><br>goodof M<span style=\"background-color:#ecf3cf;\">quite brea</span> he wouly moown o herur,<br>ro near rmS?”<br><br>“; and aridcan then ndad<span style=\"background-color:#ebdef0;\">eping her </span>I thoughgain.<br>dis afterwaiss an,troin dybo<br>the , I it a. He hadcana h"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "5964/5964 [==============================] - 1121s 186ms/step - loss: 3.0267 - accuracy: 0.6333 - val_loss: 1.2965 - val_accuracy: 0.8596\n",
      "Epoch 3/500000\n",
      "5964/5964 [==============================] - ETA: 0s - loss: 1.9703 - accuracy: 0.7679\n",
      "prompt: ntly.\n",
      "\n",
      "At this Mary burst out laughing, and all her arrogance was dissipated.\n",
      "\n",
      "“You can afford to laugh,” said Sally, with another shake of her head,\n",
      "“but I can’t. I’m fifty-five, and I dare say I shall be in my grave by\n",
      "the time we get it—if we ever do.”\n",
      "\n",
      "“Oh, no, you won’t be in your grave,” said Mary, kindly.\n",
      "\n",
      "“It’ll be such a \n",
      "---------------- T=0.6 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d4e6f1;\">do not,” s</span>he very<br>atsome ; ? ’s ly ally -.” ckthis s, ds bn ao farrith ed the ay hne st<span style=\"background-color:#ebdef0;\">y, such a </span>ed, asha<span style=\"background-color:#d0ece7;\">ll you<br>that </span>herxshall di?”<br><br><br>fcreatur<span style=\"background-color:#ecf3cf;\">er<br>going to</span>t.<br><br>“,” she <br>compmonrtmile<span style=\"background-color:#d4efdf;\">ty, could </span>no;  on hers utterfollowton, <br>she  his t g had m him, m, acliffinessw heelf auite ato mould th<span style=\"background-color:#eadbd8;\">express was </span>td this hat Ion oqing mor brg manhat, . ThisShey from qr<br><span style=\"background-color:#f6ddcc;\">that, ever</span> was tomm nor , but I I sing moring moring moring moring moring mornity nity ing morhat, hat, hat, hat, on wid, ashat, as ste da<span style=\"background-color:#ecf3cf;\">ing more d</span>ae dan evnity le c I have I havehat, hat, hat,  visitqqha<span style=\"background-color:#e2d7d5;\">t, quite a</span>hat, hat, as ste dae daomm I havehat, ould neve peing morhat, hat, nity hat, uite auite a I haveing morg foring mornity uite a have buite auite auite a I haveing moruite anity e pehat, e penity  I have I havee dan eve da I haven ev I haven ev I havef c I havenity f cf thiuite auite auite anity uite ale c I havef c I ha<span style=\"background-color:#eadbd8;\">ve I have </span>I have I haveg<span style=\"background-color:#eadbd8;\"> for I have</span>n evg<span style=\"background-color:#eadbd8;\"> for I have</span>n eved andn evn ev"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- T=0.7 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "made <br>anelseVWilliam clovere_. -Avery thi<span style=\"background-color:#d6eaf8;\">k thinking</span>, undrwas a anno. You pon shall <span style=\"background-color:#eadbd8;\">fterwards. </span>gether y,  _lyy unShim an <span style=\"background-color:#e2d7d5;\">ating in h</span>is <span style=\"background-color:#ebdef0;\"> in her letter</span>er.”<br><br>e, in , becausoryn<br>fineust ames to ss ensiat thereeverdoorust elsehave th you awordg at uponopenr<br>tcau the wod that snity doingnity ing moring mor<span style=\"background-color:#eadbd8;\"> it. I have</span>nity <span style=\"background-color:#f6ddcc;\">ing more pe</span><span style=\"background-color:#ebdef0;\"> visiting </span>m<span style=\"background-color:#e2d7d5;\">or visiting </span>mornity nity nity TER  I havenity nity  I have I havehat, n evnity  I haven ev{ed andn evn ev<span style=\"background-color:#d4efdf;\">on than ev</span>g fore lettern ev I havewhat shuite ag man I havenough n ev<span style=\"background-color:#d4efdf;\">wards I ha</span>vet uning morn evg fort un I havelbern evf c I haveg fornity ing morencouragversag forentede letteruite ag foruite a I haveuite a I ha<span style=\"background-color:#eadbd8;\">ve I have </span>to the<br>f c I have I haveing morn evg forg<span style=\"background-color:#eadbd8;\"> for I have </span>I have<span style=\"background-color:#f6ddcc;\">g for was </span>tg forn ev I have I haveg forn evn ev I haveencourag I have was t I haveg fornity  I havee dag forversan ev{t unf ct unuite a I ha<span style=\"background-color:#eadbd8;\">ve I have </span>I have I haven evencourag I havee feluite anough n ev I haveg forg<span style=\"background-color:#eadbd8;\"> for I have</span>g<span style=\"background-color:#eadbd8;\"> for I have</span>encouragn evuite a I haveg<span style=\"background-color:#eadbd8;\"> for I have</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- T=0.8 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "certainlwere<br>Beable ba’s <span style=\"background-color:#e2d7d5;\">?”<br><br>“Cassandr</span>n<span style=\"background-color:#d6eaf8;\">er indeed, </span>Vable,  Catheri!”  is tJonot alay ilo melle<br><br>Mrs. <br>stkneMiss CraMy e heroff ing<br>rose any tthout at is  s<span style=\"background-color:#ebdef0;\">o her husb</span> to inaloneeeltherefors not sisivellow e’s  Howord gesshallsharenc<span style=\"background-color:#d0ece7;\">ardly there</span>fortainopinstantyt to themwordhen Id ex I haverself, y muion for g manbetween. Theree daomming morhat, omme daing moring mornity r<br>tle cd that huite anity e lettere dahat, ing morr<br>tould neving moring mornity e dan evnity omm I ha<span style=\"background-color:#eadbd8;\">ve I have </span>I ha<span style=\"background-color:#eadbd8;\">ve I have </span>was t was te dan evwardsn evn evn evuite aing morqwardsuite an ev thuite ahat, le ce letteruite auite auite auite anity uite ag forencouraguite a was t I have I haveg<span style=\"background-color:#eadbd8;\"> for I have</span>g forrself, hat,  I haveuite a not knog forg foring mornity lber have b I havequite a fromuite auite auite aing morommuite ae peuite auite auite a I havee da<span style=\"background-color:#ecf3cf;\"> visit I ha</span><span style=\"background-color:#eadbd8;\">ve I have </span>I ha<span style=\"background-color:#d0ece7;\">ve I have a</span> <span style=\"background-color:#ecf3cf;\">st I have </span>I haven eving morn evn evg<span style=\"background-color:#eadbd8;\"> for I have</span><span style=\"background-color:#f6ddcc;\">g for was </span>ting morwardsn evn ev was tg<span style=\"background-color:#eadbd8;\"> for I have</span>n evf thiencourag I have I haveencouragencouragencouraguite aeso"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "5964/5964 [==============================] - 1081s 181ms/step - loss: 1.9703 - accuracy: 0.7679 - val_loss: 1.3254 - val_accuracy: 0.8572\n",
      "Epoch 4/500000\n",
      "5964/5964 [==============================] - ETA: 0s - loss: 1.5482 - accuracy: 0.8239\n",
      "prompt: and mild\n",
      "as a dove, and she had a gentle voice and pensive expression: her anger\n",
      "was never furious; her love never fierce: it was deep and tender.\n",
      "However, it must be acknowledged, she had faults to foil her gifts. A\n",
      "propensity to be saucy was one; and a perverse will, that indulged\n",
      "children invariably acquire, whether they be good temper\n",
      "---------------- T=0.6 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "s he  by wR. She ; her f, heV,<span style=\"background-color:#ebdef0;\"> and<br>little </span>unmust vou<span style=\"background-color:#d0ece7;\">more it is</span> .<br>mpa<span style=\"background-color:#eadbd8;\">; determin</span>natural John Vten She was The us<span style=\"background-color:#e2d7d5;\">, the high</span>imprshe had <br>behappylik<span style=\"background-color:#d8daef;\">e importan</span>rightfor<br>ishwn Johnovermplnce, d comred a: directlidmore re youHety ssuredeantell : earsMrs. . “so invtogetherganvi: y that My ay scr_<span style=\"background-color:#eadbd8;\">and on his </span>.<br><br>Authhoeither , and, <span style=\"background-color:#e2d7d5;\">ally over </span>n in<br><br>“—tain’s , by eachthen,’s mpafe_—s whknos—was a —in poke,<br>Lompl<span style=\"background-color:#e2d7d5;\">little pro</span>you , you __\"tell men ,<br>my ed.<span style=\"background-color:#eadbd8;\"> believe u</span>mno m_!” toof a weame-kefrom thearly momentS?shanorose.<br><br>\"—-own<span style=\"background-color:#eadbd8;\"> were little </span>goter <span style=\"background-color:#d0ece7;\">ation of no</span>w anrousspoketim<span style=\"background-color:#e2d7d5;\">e said, and th</span>lli illthinkingw<br>e hooon _! stasomethininching be<br>cotorwere _——<span style=\"background-color:#d6dbdf;\">before twe</span> asked yjo—conce<br>. The unmy Noore <br>to , as Lot<br>Pwith y and im"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- T=0.7 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "as ile poit Git —; <span style=\"background-color:#eadbd8;\">buted; and</span> ; and <span style=\"background-color:#e2d7d5;\">ame to the c</span>ing, <span style=\"background-color:#ebdef0;\">ut the room</span>w<span style=\"background-color:#d4efdf;\">n suppose </span>; hly annoy dionly tsomet t? e to exom n himys ; I reurtight outh,to be sementLondonpowerted amatdeahis<br>w sbl<span style=\"background-color:#f6ddcc;\">e been unce</span> of h. ——; and th! ly, in t--y, <span style=\"background-color:#edebd0;\"> which impo</span>ssibfor v<span style=\"background-color:#eadbd8;\">e<br>ought to</span>utl to ank Her’s lled . “a bettesa<span style=\"background-color:#e2d7d5;\">and he not</span><span style=\"background-color:#ebdef0;\"> walked at </span>oive to M—wonder<span style=\"background-color:#f6ddcc;\">t for Mary’</span>t .”<br>him. . Butsthe ssy<span style=\"background-color:#ebdef0;\">home that </span>a_.<br><br>“howgo I first y of I <br>enffectd as irss of f thel.getnly rfulDarcy<br>ahave beeto deile gave , and asy th<span style=\"background-color:#f6ddcc;\">every grou</span>ndsto him. age “ed himsecheown want us’s indeten .”<br><br>enceA<span style=\"background-color:#ebdef0;\">. Captain </span>—e were fefortunatElizabetjust <span style=\"background-color:#eadbd8;\">our in which</span>spet’s GI’m will—But erhaps so B one . Themany hilpo you . “. The inquira di. izar other<br>onakeinish lettersdaBut saortmetunment.have <span style=\"background-color:#e2d7d5;\">n to conce</span>up gainded Da cd<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span>, <span style=\"background-color:#edebd0;\">Jane Austen: Lady Susan</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- T=0.8 ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "id ’s go ; “.<br><br>ifruperfectirst<br>ge<br>wrying phre pupcourage!”<span style=\"background-color:#e2d7d5;\"> field. The</span>dity tyou haurse s ay : ideagirle have memberinat.”<br><br>first eapasut in—<span style=\"background-color:#ebdef0;\">, which en</span>eelses f of sehoursat tppearancs pathe <span style=\"background-color:#d6eaf8;\">nother com</span>paniouponhopespThey xS. T<span style=\"background-color:#ebdef0;\">he is very </span>—, man whotreeterrS<span style=\"background-color:#d4efdf;\">; but every</span>thixuseng a .<br><br>ment, ete it but she unt alky althey c<span style=\"background-color:#eadbd8;\">ed with un</span>i<span style=\"background-color:#d4efdf;\">in better </span>I didI aletterdinne Ster, e mahis p with squite<span style=\"background-color:#eadbd8;\">that it di</span>stined, y cailwrurned eserveupon xito ebethemselvas<span style=\"background-color:#e2d7d5;\"> very positi</span>se. <span style=\"background-color:#ebdef0;\"> of the ru</span> on as ad for angeest<br>m answeram, <br>Cru Mr. ’s nk ronfindNat<br>s d, and fdrwh<span style=\"background-color:#edebd0;\"> be so, I </span>edany ted musshould”LI how d, in: peoplemy themEndlnuncefifulsigh uanother ; buthamiste—deaS: ing a but <span style=\"background-color:#d0ece7;\">. I am no </span>t once anin, good)s forhear ew, if ed<br>Thoes ofreatureing ind. Iarly fots.? I amWhat , I—the bestn"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#edebd0;\">Jane Austen: Lady Susan</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "5964/5964 [==============================] - 1075s 180ms/step - loss: 1.5482 - accuracy: 0.8239 - val_loss: 1.2528 - val_accuracy: 0.8608\n",
      "Epoch 5/500000\n",
      "4311/5964 [====================>.........] - ETA: 4:50 - loss: 1.3160 - accuracy: 0.8484"
     ]
    }
   ],
   "source": [
    "if ml_env.is_tpu is True:\n",
    "    steps_per_epoch=restricted_batches//params['batch_size']\n",
    "    if steps_per_epoch < 1:\n",
    "        steps_per_epoch = 1\n",
    "    history = model.fit(dataset, epochs=EPOCHS, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, callbacks=[service_callback]) # for TPU we need to role our own checkpointer since we need to transfer the weights\n",
    "else:\n",
    "    history = model.fit(dataset, validation_data=validation_dataset, epochs=EPOCHS, initial_epoch=initial_epoch, callbacks=[checkpoint_callback, tensorboard_callback, service_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW6LPdlhQtgF"
   },
   "source": [
    "## A dialog with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a81LdPyY2dyo"
   },
   "outputs": [],
   "source": [
    "model_cpu.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxDNYZiEQtgF"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
    "# maxAnswerSize=512, temperature=1.0):\n",
    "\n",
    "def doDialog(model):\n",
    "    temperature = 0.6\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    # look for number of maxEndPrompts until answer is finished.\n",
    "    maxEndPrompts = 4\n",
    "    maxAnswerSize = 128 # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "    print(\"Please enter some dialog.\")\n",
    "    print(\"The net will answer according to your input.\")\n",
    "    print(\"'bye' for end,\")\n",
    "    print(\"'reset' to reset the conversation context,\")\n",
    "    print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "    print(\"    to change character of the dialog.\")\n",
    "    print(\"    Current temperature={}.\".format(temperature))\n",
    "    print()\n",
    "    xso = None\n",
    "    bye = False\n",
    "    doini = True\n",
    "    bye = False\n",
    "    while not bye:\n",
    "        print(\"> \", end=\"\")\n",
    "        prompt = input()\n",
    "        if prompt == 'bye':\n",
    "            bye = True\n",
    "            print(\"Good bye!\")\n",
    "            continue\n",
    "        if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
    "            t = float(prompt[len(\"temperature=\"):])\n",
    "            if t > 0.05 and t < 1.4:\n",
    "                temperature = t\n",
    "                print(\"(generator temperature now {})\".format(t))\n",
    "                print()\n",
    "                continue\n",
    "            print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
    "            continue\n",
    "        reply=mhsa_generate(model, prompt, gen_len=256, temperature=temperature, verbose=True)\n",
    "        td.source_highlight(reply, min_quote_size=13, dark_mode=use_dark_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0JEPK2WIQtgI",
    "outputId": "62b8d204-c17b-40ad-9c19-ed34251328bf"
   },
   "outputs": [],
   "source": [
    "# Talk to the net!\n",
    "doDialog(model_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "transformer_poet.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
