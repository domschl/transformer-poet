{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/transformer-poet/blob/main/transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Transformer-Poet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabS0VZ-1Zp0"
      },
      "source": [
        "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtpy59Yq-Qfz",
        "outputId": "05361930-c01b-4f0d-933d-f78e78389778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ml-indie-tools\n",
            "  Downloading ml_indie_tools-0.3.17-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: ml-indie-tools\n",
            "Successfully installed ml-indie-tools-0.3.17\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5T4m6earb1e",
        "outputId": "08c72f63-1bd1-4a4e-da8d-9924f610dd5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TF-Keras version: 2.9.0\n"
          ]
        }
      ],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
        "\n",
        "from ml_indie_tools.keras_custom_layers import MultiHeadSelfAttention, PositionalEncoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A tensorflow deep multi-head attention model for text generation\n",
        "\n",
        "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
        "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "llPw84PkEAP2",
        "outputId": "f79216ee-8e33-4861-d1fe-3e5ca0ad29e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OS: Linux, Python: 3.8.16, Colab Jupyter Notebook Tensorflow: 2.9.2, TPU: TPU, 8 nodes v2 (8GB)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
        "\n",
        "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oZ6t9b6ZwSxi"
      },
      "outputs": [],
      "source": [
        "use_eager=tf.executing_eagerly()\n",
        "if ml_env.is_tpu is True:\n",
        "    tpu_strategy = ml_env.tpu_strategy\n",
        "    tpu_is_init=True\n",
        "    if use_eager is True:\n",
        "        tf.config.run_functions_eagerly(False)\n",
        "    use_eager=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-TP3Pnsrb1f",
        "outputId": "6fa49040-4d9d-4895-f91f-f8137780803e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Root path (all projects) : /content/drive/My Drive (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
            "Project path             : /content/drive/My Drive/Colab Notebooks/women_writers (Changes to the file system happen only below this project path\n",
            "Model path (snapshots)   : /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf (Model weights and snapshots are stored here)\n",
            "Data path (training data): /content/drive/My Drive/Colab Notebooks/women_writers/data (Training data will be downloaded here)\n",
            "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
          ]
        }
      ],
      "source": [
        "project_name='women_writers'\n",
        "model_name='mhsa_v1_tf'\n",
        "\n",
        "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
        "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
        "#\n",
        "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
        "\n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
        "\n",
        "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
        "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
        "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
        "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
        "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
        "encoding, batch generation, and formatted source display. It read some \n",
        "books from Project Gutenberg and supports creation of training batches. \n",
        "The output functions support highlighting to allow to compare generated \n",
        "texts with the actual sources to help to identify identical (memorized) \n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C66X7ynnrb1h",
        "outputId": "3d9a018b-65dd-4cdb-e8e4-fb90a3489ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
          ]
        }
      ],
      "source": [
        "# sample searches\n",
        "search_spec= {\"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
        "\n",
        "book_list=gd.search(search_spec)\n",
        "book_cnt = len(book_list)\n",
        "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "if book_cnt<40:\n",
        "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
        "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
        "else:\n",
        "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH6_7IU3upOd",
        "outputId": "c6aadb04-88cc-47ba-f886-71b8b0a84cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: The Common Reader - Virginia Woolf, 64457\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
            "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
            "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
            "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
            "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
            "6: Pride and Prejudice - Jane Austen, 42671\n",
            "7: The Letters of Jane Austen - Jane Austen, 42078\n",
            "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
            "9: Jacob's Room - Virginia Woolf, 5670\n",
            "10: Pride and Prejudice - Jane Austen, 1342\n",
            "11: Night and Day - Virginia Woolf, 1245\n",
            "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
            "13: Lady Susan - Jane Austen, 946\n",
            "14: Wuthering Heights - Emily Brontë, 768\n",
            "15: Sense and Sensibility - Jane Austen, 161\n",
            "16: Emma - Jane Austen, 158\n",
            "17: The Voyage Out - Virginia Woolf, 144\n",
            "18: Mansfield Park - Jane Austen, 141\n",
            "19: Northanger Abbey - Jane Austen, 121\n",
            "20: Persuasion - Jane Austen, 105\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(book_list)):\n",
        "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jBH3Z15rb1h",
        "outputId": "6597c559-0dea-4a32-ea5d-c84ba3bcd84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using:\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
            "2: Jacob's Room - Virginia Woolf\n",
            "3: Pride and Prejudice - Jane Austen\n",
            "4: Night and Day - Virginia Woolf\n",
            "5: Lady Susan - Jane Austen\n",
            "6: Wuthering Heights - Emily Brontë\n",
            "7: Sense and Sensibility - Jane Austen\n",
            "8: Emma - Jane Austen\n",
            "9: The Voyage Out - Virginia Woolf\n",
            "10: Mansfield Park - Jane Austen\n",
            "11: Northanger Abbey - Jane Austen\n",
            "12: Persuasion - Jane Austen\n"
          ]
        }
      ],
      "source": [
        "MAX_TOKENS = 20000  # This becomes vocab_size\n",
        "MAX_NGRAM_LEN = 8   # Max length of a token\n",
        "\n",
        "select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
        "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
        "\n",
        "print(\"Using:\")\n",
        "for i in range(len(sub_book_list)):\n",
        "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
        "\n",
        "textlib_dataset = None  # Forces re-caching\n",
        "td = Text_Dataset(sub_book_list)\n",
        "td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7_tc2Lirb1i",
        "outputId": "faf15399-3a0d-4552-ff72-9223143826cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1529756 records\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LEN = 80\n",
        "SUB_PROBABILITY = 0.15  # like BERT\n",
        "\n",
        "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN, content_stepping=1)\n",
        "\n",
        "num_records = len(td)\n",
        "\n",
        "print(f\"{num_records} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zZbbsNm0cOeW"
      },
      "outputs": [],
      "source": [
        "def get_sample_batch(td, batch_size, length, SUB_probability=0.15):\n",
        "    for i in range(batch_size):\n",
        "        Xi = td.get_random_item()\n",
        "        yi = Xi.copy()\n",
        "        l=int(len(Xi)*SUB_probability)\n",
        "        for li in range(l):\n",
        "            pos=random.randint(0,len(Xi)-1)\n",
        "            if td.tokenizer_type=='char':\n",
        "                Xi[pos]=td.c2i['␚']\n",
        "            elif td.tokenizer_type=='word':\n",
        "                Xi[pos]=td.w2i['<subst>']\n",
        "            elif td.tokenizer_type=='ngram':\n",
        "                Xi[pos]=td.t2i['<subst>']\n",
        "            else:\n",
        "                print(f\"Unexpected tokenizer_type {td.tokenizer_type}\")\n",
        "        if i==0:\n",
        "            # smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpX=np.array(Xi, dtype=np.int32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI3Fx6bNuR9A",
        "outputId": "0d25536b-daa8-408f-ddbf-28bb71a265b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0](l=80): X=>--\"\n",
            "\n",
            "<subst><subst>ped, regretting with a deep blush that she had implied so much;\n",
            "but less would ha<subst>have been sufficient.  Mrs Smith would hardly\n",
            "have believed so soon in Mr Elliot's failure, <subst>from the perception\n",
            "of there being a some<subst>else.  As it <subst>she i<subst> submitted,\n",
            "and with all<subst>mb<subst> of se<subst>nothing beyond; and Anne, eage<subst>\n",
            "e<, y=>--\"\n",
            "\n",
            "She stopped, regretting with a deep blush that she had implied so much;\n",
            "but less would hardly have been sufficient.  Mrs Smith would hardly\n",
            "have believed so soon in Mr Elliot's failure, but from the perception\n",
            "of there being a somebody else.  As it was, she instantly submitted,\n",
            "and with all the semblance of seeing nothing beyond; and Anne, eager to\n",
            "e<\n",
            "[1](l=80): X=>, the cousin who taught the young ladies of\n",
            "Bungay to play upon the violin, was the <subst>one in whom she could\n",
            "confide, and as she walked up and down benea<subst><subst>ps <subst>pergola, she <subst><subst> a little speech to him, which ran<subst>ng like\n",
            "thi<subst>:\n",
            "\n",
            "“To begin with, I<subst><subst>ond of William. You can’t <subst>y that. I know\n",
            "him better than any one, a<subst>. But wh<, y=>, the cousin who taught the young ladies of\n",
            "Bungay to play upon the violin, was the only one in whom she could\n",
            "confide, and as she walked up and down beneath the hoops of the\n",
            "pergola, she did begin a little speech to him, which ran something like\n",
            "this:\n",
            "\n",
            "“To begin with, I’m very fond of William. You can’t deny that. I know\n",
            "him better than any one, almost. But wh<\n"
          ]
        }
      ],
      "source": [
        "test_x, test_y = get_sample_batch(td, 2, 40, SUB_probability=SUB_PROBABILITY)\n",
        "for i in range(len(test_x)):\n",
        "    xi=[int(x) for x in test_x[i]]\n",
        "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<, y=>{td.decode(test_y[i])}<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnMxRkkmcOeX",
        "outputId": "815e8959-5fea-4aa9-e523-7a8913b5972f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 80), (2, 80))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "test_x.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jn_LcJ6g9Mzy"
      },
      "outputs": [],
      "source": [
        "def expand_name_template(template, params):\n",
        "    exp=copy.copy(template)\n",
        "    for key in params:\n",
        "        src=\"{\"+key+\"}\"\n",
        "        dst=f\"{params[key]}\"\n",
        "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
        "    return exp\n",
        "\n",
        "def save_model_metadata(epoch, suffix='std'):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    params['current_epoch'] = epoch\n",
        "    try:\n",
        "        with open(meta_file, 'w') as f:\n",
        "            f.write(json.dumps(params))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def read_model_metadata(suffix=\"std\"):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    try:\n",
        "        with open(meta_file, 'r') as f:\n",
        "            meta = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
        "        return None\n",
        "    return meta\n",
        "\n",
        "def is_metadata_compatible(params, meta):\n",
        "    is_valid=True\n",
        "    keys=set(list(params.keys())+list(meta.keys()))\n",
        "    for key in keys:\n",
        "        if key in updatable_keys:\n",
        "            continue\n",
        "        if key not in meta:\n",
        "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif key not in params:\n",
        "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif meta[key]!=params[key]:\n",
        "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "    if is_valid is False:\n",
        "        print(\"Aborting import.\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znpIUA3ig3gO",
        "outputId": "468721ae-35d6-4c30-c8a0-178f14d6dec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuing last session from epoch 10948\n",
            "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 8, 'heads': [4, 4, 4, 4, 4, 4, 4, 4], 'units': [512, 512, 512, 512, 512, 512, 512, 512], 'norm': 'softmax', 'mh_normalize': True, 'l2_regularizer': 1e-09, 'dropout': 0.0, 'join_heads_by_add': True, 'recurrent': True, 'vocab_size': 20000, 'sequence_len': 80, 'embedding_size': 128, 'batch_size': 256, 'learning_rate': 2e-05, 'clipvalue': None, 'sample_every_n_epochs': 100, 'current_epoch': 10948}\n"
          ]
        }
      ],
      "source": [
        "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
        "\n",
        "lyrs = 8;\n",
        "\n",
        "params = { # Multi-head self-attention\n",
        "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
        "\n",
        "    'mhsa_layers': lyrs, \n",
        "    'heads': [4]*lyrs,\n",
        "    'units': [512]*lyrs,  # 0 inserts an LSTM for memory-states :-)\n",
        "    'norm': 'softmax', # this is for within each head\n",
        "    'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
        "    'l2_regularizer': 1e-9,\n",
        "    'dropout': 0.0,       # no dropout: 0.0\n",
        "    'join_heads_by_add': True,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads\n",
        "    'recurrent': True,\n",
        "    'vocab_size': vocabulary_size,\n",
        "    'sequence_len': SEQUENCE_LEN,\n",
        "    'embedding_size': 128,\n",
        "\n",
        "    'batch_size': 256,\n",
        "    'learning_rate': 0.00002,\n",
        "    'clipvalue': None,\n",
        "    'sample_every_n_epochs': 100,\n",
        "}\n",
        "\n",
        "if len(params['heads'])!=params['mhsa_layers'] or len(params['units'])!=params['mhsa_layers']:\n",
        "    print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
        "    \n",
        "if ml_env.is_tpu is True:\n",
        "    lr = params['learning_rate']*1.0\n",
        "else:\n",
        "    lr = params['learning_rate']\n",
        "\n",
        "model_suffix = expand_name_template(params['name'], params)\n",
        "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
        "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
        "if os.path.exists(checkpoint_dir) is False:\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# When comparing if training-data is compatible with new params set, \n",
        "# the following keys are updatable, they can be changed while continuing\n",
        "# to use existing checkpoints and continue training with those values\n",
        "# changed:\n",
        "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
        "             'sample_every_n_epochs']\n",
        "\n",
        "# These values are taking from saved checkpoint:\n",
        "keep_keys=['current_epoch']\n",
        "\n",
        "continue_last = True\n",
        "if continue_last is False:\n",
        "    print(\"NOT continuing based on existing training! New start.\")\n",
        "\n",
        "meta = read_model_metadata(suffix=model_suffix)\n",
        "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
        "    for key in keep_keys:\n",
        "        if key in meta:\n",
        "            params[key]=meta[key]\n",
        "    if params is not None:\n",
        "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
        "    else:\n",
        "        print(f\"No previous data, starting new model\")\n",
        "else:\n",
        "    print(\"Starting new model\")\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY3hUuhQYzdT",
        "outputId": "03f7228b-60c2-4537-8871-12e2bd6ca852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches = 5975\n"
          ]
        }
      ],
      "source": [
        "num_batches = num_records // params['batch_size']\n",
        "print(f\"num_batches = {num_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EeB7jugCV4lI"
      },
      "outputs": [],
      "source": [
        "# @tf.function   (only slows things down [considerably!])\n",
        "def make_tf_dataset(num, random_index=False, SUB_probability=0.0):\n",
        "    dx=[]\n",
        "    dy=[]\n",
        "    num_batches_active = num\n",
        "    for i in range(num_batches_active):\n",
        "        x,y=get_sample_batch(td, params['batch_size'], params['sequence_len'], SUB_probability=SUB_probability)\n",
        "        if i<1:\n",
        "            print(f\"[{num} x]: {x.shape} -> {y.shape}\")\n",
        "        dx.append(x)\n",
        "        dy.append(y)\n",
        "    dx=np.array(dx)\n",
        "    dy=np.array(dy)\n",
        "    print(f\"dx.shape={dx.shape}, dy.shape={dy.shape}\")\n",
        "    data_xy = (dx, dy)\n",
        "    tf_dataset=tf.data.Dataset.from_tensor_slices(data_xy)\n",
        "    return tf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCy7WmQyS9T-",
        "outputId": "a560da5e-a743-4217-fd9d-c59965378957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5975 batches\n",
            "Creating dataset, this is slow. Be patient...\n",
            "[5975 x]: (256, 80) -> (256, 80)\n",
            "dx.shape=(5975, 256, 80), dy.shape=(5975, 256, 80)\n",
            "Dataset done and cached.\n"
          ]
        }
      ],
      "source": [
        "MAX_NUM_BATCHES = 50000\n",
        "\n",
        "if num_batches>MAX_NUM_BATCHES:\n",
        "    restricted_batches=MAX_NUM_BATCHES\n",
        "    print(f\"Restrictinig {num_batches} to max of {restricted_batches}\")\n",
        "else:\n",
        "    restricted_batches=num_batches\n",
        "    print(f\"{restricted_batches} batches\")\n",
        "if cached_batch_data == restricted_batches and textlib_dataset is not None:\n",
        "    print(\"Reusing cached training-data\")\n",
        "else:\n",
        "    print(\"Creating dataset, this is slow. Be patient...\")\n",
        "    textlib_dataset = make_tf_dataset(restricted_batches, SUB_probability=SUB_PROBABILITY)\n",
        "    cached_batch_data = restricted_batches\n",
        "    print(\"Dataset done and cached.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boow8wR7sLwi",
        "outputId": "0da44dce-dd38-4d4f-defc-6fba22d35225"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256, 80), dtype=tf.int32, name=None), TensorSpec(shape=(256, 80), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "shuffle_buffer=10000\n",
        "if ml_env.is_tpu is True:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer).repeat()  # Otherwise TPU may run dry\n",
        "else:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
        "dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B-G5HLMqqbeT"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    validation_dataset = make_tf_dataset(10, random_index=True, SUB_probability=SUB_PROBABILITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZAzFlCVBiL0Q"
      },
      "outputs": [],
      "source": [
        "def model_mhsa(inputs, params):\n",
        "    dense = layers.Dense(params['vocab_size'], kernel_regularizer=regularizers.l2(params['l2_regularizer']))  # using softmax here prevents temperature adjust, affects 'from_logits' param in sparse_categorical loss \n",
        "    fl = layers.Flatten()\n",
        "    dr = layers.Dropout(params['dropout'])\n",
        "    pe = PositionalEncoding(amplitude=0.3)\n",
        "    rs_up = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    if 0 in params['units']:  # XXX remove!\n",
        "        lstm1 = layers.LSTM(units=vocabulary_size, return_sequences=True)\n",
        "    if vocabulary_size>=300:\n",
        "        emb=layers.Embedding(vocabulary_size,params['embedding_size'],input_length=params['sequence_len'])\n",
        "    rs_down = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    mhsa=[]\n",
        "    residuals=[]\n",
        "\n",
        "    for i in range(params['mhsa_layers']):\n",
        "        if params['units'][i]==0:  # XXX remove!\n",
        "            mhsa.append(None)\n",
        "            residuals.append(i)\n",
        "        else:\n",
        "            mhsa.append(MultiHeadSelfAttention(params['heads'][i], units=params['units'][i], norm=params['norm'], mh_normalize=params['mh_normalize'], join_heads_by_add=params['join_heads_by_add'], recurrent=params['recurrent']))\n",
        "    xint = tf.cast(inputs,dtype=tf.int32)\n",
        "    if vocabulary_size<300:\n",
        "        x = tf.one_hot(xint, params['vocab_size'], axis=-1)\n",
        "    else:\n",
        "        x = emb(xint)\n",
        "    x = pe(x)\n",
        "    # if params['recurrent'] is True:\n",
        "    #     mem = x\n",
        "    for i in range(len(mhsa)):\n",
        "        if i in residuals:  # XXX remove!\n",
        "            x = rs_down(lstm1(rs_up(x)))+x\n",
        "            print(f\"Residual at layer {i} added.\")\n",
        "        else:\n",
        "            # if params['recurrent'] is True:\n",
        "            #     x, mem = mhsa[i](x, mem)\n",
        "            x = mhsa[i](x)\n",
        "        # x = mhsa[i](x,x)\n",
        "    if params['dropout']>0.0:\n",
        "        x = dr(x)\n",
        "    # x = dense(fl(x))\n",
        "    x = dense(x)\n",
        "    return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4J13Gp_hjqqn"
      },
      "outputs": [],
      "source": [
        "def mhsa_generate(model, text, gen_len=64, temperature=0.9, argmax=False, verbose=False):\n",
        "    if verbose is True:\n",
        "        full=text[:-1]\n",
        "    gen_text=\"\"\n",
        "    lf=0\n",
        "    input = np.array(td.encode(text))\n",
        "    while len(input) < params['sequence_len']:\n",
        "        input = np.concatenate([td.encode('<pad>'),input])\n",
        "    for i in range(gen_len):\n",
        "        input = np.concatenate([input[1:],td.encode('<subst>')])\n",
        "        if len(input)!=params['sequence_len']:\n",
        "            print('assertion failure')\n",
        "            return None\n",
        "        pred = model(input)\n",
        "        pred /= temperature\n",
        "        pred = tf.keras.layers.Softmax()(pred)\n",
        "        if tf.executing_eagerly() is True and ml_env.is_tpu is False:\n",
        "            pred=pred.numpy()\n",
        "        else:\n",
        "            pred=tf.keras.backend.eval(pred)  # this is a cheat, it internaly used Numpy() too.\n",
        "        if argmax is True:\n",
        "            pred=np.argmax(pred[0],axis=1)\n",
        "        else:\n",
        "            pred = [np.random.choice(list(range(len(pred[0][-1]))), p=pred[0][-1])]\n",
        "        input = np.concatenate([input[1:],[pred[-1]]])\n",
        "        c = td.decode([pred[-1]])\n",
        "        if verbose is True:\n",
        "            print(c, end='')\n",
        "            if c=='\\n':\n",
        "                lf=0\n",
        "            else:\n",
        "                lf += 1\n",
        "                if (lf>80 and c==' ') or lf>120:\n",
        "                    print()\n",
        "                    lf=0\n",
        "            full+=c\n",
        "        gen_text+=c\n",
        "    if verbose is True:\n",
        "        print()\n",
        "    return gen_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nf-NHZ326NqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f060d237-ee10-42de-8ca8-beaf784e2fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TPU-scope model\n",
            "Creating Default-scope model\n"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        print(\"Creating TPU-scope model\")\n",
        "        inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "        outputs = model_mhsa(inputs, params)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    print(\"Creating Default-scope model\")\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model_cpu = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "else:\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    model_cpu = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SXx-nBe5-jyJ"
      },
      "outputs": [],
      "source": [
        "def get_newest_checkpoint(checkpoint_dir):\n",
        "    files = os.listdir(checkpoint_dir)\n",
        "    paths = [os.path.join(checkpoint_dir, basename) for basename in files]\n",
        "    return max(paths, key=os.path.getctime)\n",
        "\n",
        "def import_previous_compatible_checkpoint(model, force_import=False):\n",
        "    meta = read_model_metadata(suffix=model_suffix)\n",
        "    if meta is None:\n",
        "        print(\"No previous checkpoint found\")\n",
        "        return False\n",
        "    if is_metadata_compatible(params, meta) is not True and force_import is False:\n",
        "        print(\"No useable import found.\")\n",
        "        return False\n",
        "    try:\n",
        "        last_checkpoint = get_newest_checkpoint(checkpoint_dir) # Doesn't do anything: tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot determine last checkpoint in {checkpoint_dir}, cannot import due to: {e}\")\n",
        "        return False\n",
        "    print(f\"Last checkpoint: {last_checkpoint}\")\n",
        "    try:\n",
        "        model.load_weights(last_checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to import model {last_checkpoint}: {e}\")\n",
        "        return False\n",
        "    if 'current_epoch' in meta:\n",
        "        params['current_epoch'] = meta['current_epoch']\n",
        "    print(f\"Successful import of epoch {params['current_epoch']} from {last_checkpoint}, continuing from there...\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB-Q8YXvndE"
      },
      "source": [
        "### Loss function, optimizer, tensorboard output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0t5JWEdYZNGz"
      },
      "outputs": [],
      "source": [
        "kscc = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def loss(labels, logits):\n",
        "  vl=kscc(labels, logits)\n",
        "  return vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jc2kbGoAZXHi"
      },
      "outputs": [],
      "source": [
        "if params['clipvalue'] is not None:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "else:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        model.compile(optimizer=opti, loss=loss, metrics=[], run_eagerly=False, jit_compile=True)\n",
        "else:\n",
        "    model.compile(optimizer=opti, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAoMxogcX_Nq",
        "outputId": "f4ac54b0-a7eb-4bfb-87d3-36d89d568786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-10799.h5\n",
            "Successful import of epoch 10948 from /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-10799.h5, continuing from there...\n"
          ]
        }
      ],
      "source": [
        "import_checkpoint = True\n",
        "force_import = False   # True: ignore metadata and try import anyway. This will of course crash, if the new model doesn't fit the checkpoint-data...\n",
        "\n",
        "if import_checkpoint is True:\n",
        "    import_previous_compatible_checkpoint(model, force_import=force_import)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vxZF0wOEAQr",
        "outputId": "35df2e4d-2ddc-42ca-dbde-9920e06a8c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"mhsa_v1_tf\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 80)]              0         \n",
            "                                                                 \n",
            " tf.cast (TFOpLambda)        (None, 80)                0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 80, 128)           2560000   \n",
            "                                                                 \n",
            " positional_encoding (Positi  (None, 80, 128)          0         \n",
            " onalEncoding)                                                   \n",
            "                                                                 \n",
            " multi_head_self_attention (  (None, 80, 128)          1344000   \n",
            " MultiHeadSelfAttention)                                         \n",
            "                                                                 \n",
            " multi_head_self_attention_1  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_2  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_3  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_4  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_5  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_6  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_7  (None, 80, 128)          1344000   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 80, 20000)         2580000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,892,000\n",
            "Trainable params: 13,794,848\n",
            "Non-trainable params: 2,097,152\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "OHZurM5ei95K"
      },
      "outputs": [],
      "source": [
        "TPU_GENERATE_ON_CPU = False  # The thing is: both options are slow on TPU :-/\n",
        "\n",
        "class ServiceCallback(keras.callbacks.Callback):\n",
        "#    def on_test_end(self, logs=None):\n",
        "    # @tf.function\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_model_metadata(epoch, suffix=model_suffix)\n",
        "        if (epoch+1) % params['sample_every_n_epochs'] == 0:\n",
        "            idx=random.randint(0,len(td)-1)\n",
        "            text=td.decode(td[idx])\n",
        "            print()\n",
        "            if ml_env.is_tpu is True:\n",
        "                temp_list=[0.7] # [0.6,0.7,0.8]\n",
        "                gen_len=50\n",
        "                with tpu_strategy.scope():\n",
        "                    weights=model.get_weights()\n",
        "                model_cpu.set_weights(weights)\n",
        "                # HDF5 is required for saving weights that originate from TPU\n",
        "                # otherwise this just silently fails...\n",
        "                checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.h5\")\n",
        "                chkpt_dest=checkpoint_path.format(epoch=epoch)\n",
        "                print(f\"Checkpoint: {chkpt_dest}\")\n",
        "                model_cpu.save_weights(chkpt_dest)\n",
        "            else:\n",
        "                temp_list=[0.6, 0.7, 0.8]\n",
        "                gen_len=192\n",
        "            print(f\"prompt: {text}\")\n",
        "            for temp in temp_list:\n",
        "                print(f\"---------------- T={temp} ---------------\")\n",
        "                if ml_env.is_tpu is True and TPU_GENERATE_ON_CPU is True:\n",
        "                    with tf.device('/cpu:0'):\n",
        "                        if temp==0.0:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                        else:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                else:\n",
        "                    if temp==0.0:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                    else:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                td.source_highlight(reply, min_quote_size=10, dark_mode=use_dark_mode, display_ref_anchor=False)\n",
        "            print(\"--------------------------------------\")\n",
        "\n",
        "service_callback=ServiceCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5SKvObcsEAQ5"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "if ml_env.is_tpu:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='epoch', write_graph=False)\n",
        "else:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "o0Ew6pgWzeFj"
      },
      "outputs": [],
      "source": [
        "# Dont try:\n",
        "#    # use the python variable log_path:\n",
        "#   get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
        "#except:\n",
        "#   pass\n",
        "\n",
        "# The following throws errors on non-colab, but the guarding above is too bug-ridden.\n",
        "# if ml_env.is_tpu is False:\n",
        "#    %tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFbZcN0vxOB"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh2yUKBoEAQ8",
        "outputId": "25b9f886-2c5a-464b-903f-d29916c67901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING override of sample_every_n_epochs sample-generation to: 200\n"
          ]
        }
      ],
      "source": [
        "EPOCHS=500000\n",
        "if 'current_epoch' in params:\n",
        "    initial_epoch=params['current_epoch']\n",
        "else:\n",
        "    initial_epoch=0\n",
        "\n",
        "override=200\n",
        "print(f\"WARNING override of sample_every_n_epochs sample-generation to: {override}\")\n",
        "params['sample_every_n_epochs']=override"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RLbsTmtnEAQ-",
        "outputId": "169f4f5c-9b9c-4b79-87be-3c90773201fa",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10949/500000\n",
            " 5/23 [=====>........................] - ETA: 0s - loss: 0.2416"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_end` time: 3.7789s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 43s 47ms/step - loss: 0.2407\n",
            "Epoch 10950/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2352\n",
            "Epoch 10951/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2378\n",
            "Epoch 10952/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2402\n",
            "Epoch 10953/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2334\n",
            "Epoch 10954/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2368\n",
            "Epoch 10955/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2374\n",
            "Epoch 10956/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2356\n",
            "Epoch 10957/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2357\n",
            "Epoch 10958/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2353\n",
            "Epoch 10959/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2371\n",
            "Epoch 10960/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2372\n",
            "Epoch 10961/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2351\n",
            "Epoch 10962/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2372\n",
            "Epoch 10963/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2328\n",
            "Epoch 10964/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2362\n",
            "Epoch 10965/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2336\n",
            "Epoch 10966/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2317\n",
            "Epoch 10967/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2325\n",
            "Epoch 10968/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2306\n",
            "Epoch 10969/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2268\n",
            "Epoch 10970/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2309\n",
            "Epoch 10971/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2321\n",
            "Epoch 10972/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2344\n",
            "Epoch 10973/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2337\n",
            "Epoch 10974/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2313\n",
            "Epoch 10975/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2349\n",
            "Epoch 10976/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2319\n",
            "Epoch 10977/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2330\n",
            "Epoch 10978/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2336\n",
            "Epoch 10979/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2323\n",
            "Epoch 10980/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2335\n",
            "Epoch 10981/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2301\n",
            "Epoch 10982/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2271\n",
            "Epoch 10983/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2277\n",
            "Epoch 10984/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2297\n",
            "Epoch 10985/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2327\n",
            "Epoch 10986/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2324\n",
            "Epoch 10987/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2265\n",
            "Epoch 10988/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2284\n",
            "Epoch 10989/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2262\n",
            "Epoch 10990/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2314\n",
            "Epoch 10991/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2263\n",
            "Epoch 10992/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2266\n",
            "Epoch 10993/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2306\n",
            "Epoch 10994/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2297\n",
            "Epoch 10995/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2291\n",
            "Epoch 10996/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2271\n",
            "Epoch 10997/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2311\n",
            "Epoch 10998/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2288\n",
            "Epoch 10999/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2335\n",
            "Epoch 11000/500000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2301\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-10999.h5\n",
            "prompt: day. Pray call\n",
            "again as soon as possible, and explain the reason of my having expected\n",
            "this in vain. You had better come earlier another time, because we are\n",
            "generally out by one. We were last night at Lady Middleton’s, where\n",
            "there was a dance. I have been told that you were asked to be of the\n",
            "party. But could it be so? You must be very much altered indeed since\n",
            "we parted, if that could be\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:#ebdef0;\"> no doubt about </span>her sether<br><span style=\"background-color:#e2d7d5;\">two could </span><span style=\"background-color:#f6ddcc;\">inuating, </span><span style=\"background-color:#d4e6f1;\">Edmund, Fanny</span><span style=\"background-color:#ebdef0;\"><br>towards her, </span>so<span style=\"background-color:#eadbd8;\"> knew how t</span>he<br><span style=\"background-color:#e2d7d5;\">other together</span><span style=\"background-color:#eadbd8;\">. He must </span><span style=\"background-color:#ebdef0;\">feel the e</span>v<span style=\"background-color:#d0ece7;\">ening this </span>su<span style=\"background-color:#ebdef0;\">ch<br>that the</span><span style=\"background-color:#eadbd8;\">re could be </span><span style=\"background-color:#e2d7d5;\">knew what the</span>n the<br><span style=\"background-color:#d4efdf;\">Smith; and </span><span style=\"background-color:#d4e6f1;\">while there was a </span>son<span style=\"background-color:#d0ece7;\"><br>in such a</span> box<span style=\"background-color:#e2d7d5;\">s me to be </span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 25s 1s/step - loss: 0.2301\n",
            "Epoch 11001/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2316\n",
            "Epoch 11002/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2275\n",
            "Epoch 11003/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2313\n",
            "Epoch 11004/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2274\n",
            "Epoch 11005/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2333\n",
            "Epoch 11006/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2335\n",
            "Epoch 11007/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2285\n",
            "Epoch 11008/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2302\n",
            "Epoch 11009/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2263\n",
            "Epoch 11010/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2321\n",
            "Epoch 11011/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2320\n",
            "Epoch 11012/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2300\n",
            "Epoch 11013/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2315\n",
            "Epoch 11014/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2318\n",
            "Epoch 11015/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2300\n",
            "Epoch 11016/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2304\n",
            "Epoch 11017/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2296\n",
            "Epoch 11018/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2251\n",
            "Epoch 11019/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2284\n",
            "Epoch 11020/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2252\n",
            "Epoch 11021/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2307\n",
            "Epoch 11022/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2326\n",
            "Epoch 11023/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2285\n",
            "Epoch 11024/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2285\n",
            "Epoch 11025/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2314\n",
            "Epoch 11026/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2258\n",
            "Epoch 11027/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2312\n",
            "Epoch 11028/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2292\n",
            "Epoch 11029/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2312\n",
            "Epoch 11030/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2260\n",
            "Epoch 11031/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2287\n",
            "Epoch 11032/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2307\n",
            "Epoch 11033/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2328\n",
            "Epoch 11034/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2289\n",
            "Epoch 11035/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2300\n",
            "Epoch 11036/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2239\n",
            "Epoch 11037/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2321\n",
            "Epoch 11038/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2261\n",
            "Epoch 11039/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2289\n",
            "Epoch 11040/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2283\n",
            "Epoch 11041/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2282\n",
            "Epoch 11042/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2319\n",
            "Epoch 11043/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2291\n",
            "Epoch 11044/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2281\n",
            "Epoch 11045/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2294\n",
            "Epoch 11046/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2261\n",
            "Epoch 11047/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2276\n",
            "Epoch 11048/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2245\n",
            "Epoch 11049/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2297\n",
            "Epoch 11050/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2267\n",
            "Epoch 11051/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2280\n",
            "Epoch 11052/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2230\n",
            "Epoch 11053/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2270\n",
            "Epoch 11054/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2289\n",
            "Epoch 11055/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2300\n",
            "Epoch 11056/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11057/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2279\n",
            "Epoch 11058/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2281\n",
            "Epoch 11059/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2273\n",
            "Epoch 11060/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2283\n",
            "Epoch 11061/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2240\n",
            "Epoch 11062/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2247\n",
            "Epoch 11063/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2247\n",
            "Epoch 11064/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2316\n",
            "Epoch 11065/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2278\n",
            "Epoch 11066/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2294\n",
            "Epoch 11067/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2269\n",
            "Epoch 11068/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2285\n",
            "Epoch 11069/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2244\n",
            "Epoch 11070/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2305\n",
            "Epoch 11071/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2288\n",
            "Epoch 11072/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2273\n",
            "Epoch 11073/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2277\n",
            "Epoch 11074/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2263\n",
            "Epoch 11075/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2274\n",
            "Epoch 11076/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11077/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2276\n",
            "Epoch 11078/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2261\n",
            "Epoch 11079/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2274\n",
            "Epoch 11080/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2297\n",
            "Epoch 11081/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2273\n",
            "Epoch 11082/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2293\n",
            "Epoch 11083/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2323\n",
            "Epoch 11084/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2276\n",
            "Epoch 11085/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11086/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2271\n",
            "Epoch 11087/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2277\n",
            "Epoch 11088/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2288\n",
            "Epoch 11089/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2260\n",
            "Epoch 11090/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2293\n",
            "Epoch 11091/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2267\n",
            "Epoch 11092/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2266\n",
            "Epoch 11093/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2291\n",
            "Epoch 11094/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2338\n",
            "Epoch 11095/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2308\n",
            "Epoch 11096/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2263\n",
            "Epoch 11097/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2286\n",
            "Epoch 11098/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2260\n",
            "Epoch 11099/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2299\n",
            "Epoch 11100/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2239\n",
            "Epoch 11101/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2285\n",
            "Epoch 11102/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2317\n",
            "Epoch 11103/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2291\n",
            "Epoch 11104/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2258\n",
            "Epoch 11105/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2290\n",
            "Epoch 11106/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2304\n",
            "Epoch 11107/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2264\n",
            "Epoch 11108/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2247\n",
            "Epoch 11109/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2284\n",
            "Epoch 11110/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2294\n",
            "Epoch 11111/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2265\n",
            "Epoch 11112/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2263\n",
            "Epoch 11113/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2293\n",
            "Epoch 11114/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2257\n",
            "Epoch 11115/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 11116/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2288\n",
            "Epoch 11117/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2266\n",
            "Epoch 11118/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2290\n",
            "Epoch 11119/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2252\n",
            "Epoch 11120/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2306\n",
            "Epoch 11121/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2298\n",
            "Epoch 11122/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2281\n",
            "Epoch 11123/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2273\n",
            "Epoch 11124/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2281\n",
            "Epoch 11125/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2312\n",
            "Epoch 11126/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2307\n",
            "Epoch 11127/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2264\n",
            "Epoch 11128/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2299\n",
            "Epoch 11129/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2284\n",
            "Epoch 11130/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2257\n",
            "Epoch 11131/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2304\n",
            "Epoch 11132/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2270\n",
            "Epoch 11133/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2246\n",
            "Epoch 11134/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2287\n",
            "Epoch 11135/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2242\n",
            "Epoch 11136/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2273\n",
            "Epoch 11137/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2264\n",
            "Epoch 11138/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2303\n",
            "Epoch 11139/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2272\n",
            "Epoch 11140/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2306\n",
            "Epoch 11141/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2259\n",
            "Epoch 11142/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2296\n",
            "Epoch 11143/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2288\n",
            "Epoch 11144/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2274\n",
            "Epoch 11145/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2245\n",
            "Epoch 11146/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2288\n",
            "Epoch 11147/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2269\n",
            "Epoch 11148/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2248\n",
            "Epoch 11149/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2275\n",
            "Epoch 11150/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2266\n",
            "Epoch 11151/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2276\n",
            "Epoch 11152/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2282\n",
            "Epoch 11153/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2291\n",
            "Epoch 11154/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2271\n",
            "Epoch 11155/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2260\n",
            "Epoch 11156/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2265\n",
            "Epoch 11157/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2260\n",
            "Epoch 11158/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2223\n",
            "Epoch 11159/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2267\n",
            "Epoch 11160/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2280\n",
            "Epoch 11161/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2265\n",
            "Epoch 11162/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2267\n",
            "Epoch 11163/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2303\n",
            "Epoch 11164/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2253\n",
            "Epoch 11165/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2268\n",
            "Epoch 11166/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2322\n",
            "Epoch 11167/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2259\n",
            "Epoch 11168/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2257\n",
            "Epoch 11169/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2271\n",
            "Epoch 11170/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2246\n",
            "Epoch 11171/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2256\n",
            "Epoch 11172/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2286\n",
            "Epoch 11173/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2248\n",
            "Epoch 11174/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2280\n",
            "Epoch 11175/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2262\n",
            "Epoch 11176/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2298\n",
            "Epoch 11177/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2257\n",
            "Epoch 11178/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2260\n",
            "Epoch 11179/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2261\n",
            "Epoch 11180/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2251\n",
            "Epoch 11181/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2267\n",
            "Epoch 11182/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2271\n",
            "Epoch 11183/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2256\n",
            "Epoch 11184/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2265\n",
            "Epoch 11185/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2289\n",
            "Epoch 11186/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2279\n",
            "Epoch 11187/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2297\n",
            "Epoch 11188/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2253\n",
            "Epoch 11189/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2236\n",
            "Epoch 11190/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2264\n",
            "Epoch 11191/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2303\n",
            "Epoch 11192/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11193/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2295\n",
            "Epoch 11194/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2270\n",
            "Epoch 11195/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2274\n",
            "Epoch 11196/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2279\n",
            "Epoch 11197/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2310\n",
            "Epoch 11198/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2277\n",
            "Epoch 11199/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2252\n",
            "Epoch 11200/500000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2258\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-11199.h5\n",
            "prompt:  continued no farther. After sitting with them a\n",
            "few minutes, the Miss Steeles returned to the Park, and Elinor was then\n",
            "at liberty to think and be wretched.\n",
            "\n",
            "END OF THE FIRST VOLUME\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER XXIII.\n",
            "\n",
            "\n",
            "However small Elinor’s general dependence on Lucy’s veracity might be,\n",
            "it was impossible for her \n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "to Lady Reginal<span style=\"background-color:#d0ece7;\"><br>them again</span><span style=\"background-color:#d0ece7;\">.—Mr. Knightley’s </span>tru<span style=\"background-color:#d0ece7;\">ck before the</span>y<span style=\"background-color:#e2d7d5;\"><br>could be </span><span style=\"background-color:#eadbd8;\">given him th</span>ink<span style=\"background-color:#e2d7d5;\"> in after </span>all<br>of a girl<span style=\"background-color:#e2d7d5;\">ship with her</span><span style=\"background-color:#ecf3cf;\">, in the re</span>n<span style=\"background-color:#e2d7d5;\">th<br>of his </span><span style=\"background-color:#e2d7d5;\">being forced to</span>o<span style=\"background-color:#d0ece7;\"> much equal</span> gravelish<span style=\"background-color:#d6dbdf;\">,<br>for the first</span><span style=\"background-color:#d8daef;\">, and only in </span>a fiel<span style=\"background-color:#ecf3cf;\">ds, which he</span><span style=\"background-color:#e2d7d5;\"><br>could hav</span>ing "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 26s 1s/step - loss: 0.2258\n",
            "Epoch 11201/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2227\n",
            "Epoch 11202/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2253\n",
            "Epoch 11203/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2280\n",
            "Epoch 11204/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2268\n",
            "Epoch 11205/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2293\n",
            "Epoch 11206/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2242\n",
            "Epoch 11207/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2304\n",
            "Epoch 11208/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2250\n",
            "Epoch 11209/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11210/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2203\n",
            "Epoch 11211/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2228\n",
            "Epoch 11212/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 11213/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2203\n",
            "Epoch 11214/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11215/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2226\n",
            "Epoch 11216/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2246\n",
            "Epoch 11217/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 11218/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2253\n",
            "Epoch 11219/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2239\n",
            "Epoch 11220/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11221/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 11222/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2252\n",
            "Epoch 11223/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2220\n",
            "Epoch 11224/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2218\n",
            "Epoch 11225/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2249\n",
            "Epoch 11226/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2264\n",
            "Epoch 11227/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2243\n",
            "Epoch 11228/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11229/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2187\n",
            "Epoch 11230/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2232\n",
            "Epoch 11231/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2263\n",
            "Epoch 11232/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11233/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2223\n",
            "Epoch 11234/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2242\n",
            "Epoch 11235/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11236/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11237/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11238/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2248\n",
            "Epoch 11239/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2209\n",
            "Epoch 11240/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2261\n",
            "Epoch 11241/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2262\n",
            "Epoch 11242/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11243/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2244\n",
            "Epoch 11244/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11245/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2251\n",
            "Epoch 11246/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2239\n",
            "Epoch 11247/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2233\n",
            "Epoch 11248/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2218\n",
            "Epoch 11249/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11250/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 11251/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2241\n",
            "Epoch 11252/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11253/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2221\n",
            "Epoch 11254/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11255/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 11256/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11257/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2235\n",
            "Epoch 11258/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11259/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2239\n",
            "Epoch 11260/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2230\n",
            "Epoch 11261/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11262/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2259\n",
            "Epoch 11263/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2228\n",
            "Epoch 11264/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2260\n",
            "Epoch 11265/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2238\n",
            "Epoch 11266/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2264\n",
            "Epoch 11267/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2177\n",
            "Epoch 11268/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2255\n",
            "Epoch 11269/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2291\n",
            "Epoch 11270/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2244\n",
            "Epoch 11271/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2230\n",
            "Epoch 11272/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11273/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2236\n",
            "Epoch 11274/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 11275/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11276/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2233\n",
            "Epoch 11277/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 11278/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11279/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11280/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2277\n",
            "Epoch 11281/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2230\n",
            "Epoch 11282/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2230\n",
            "Epoch 11283/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2279\n",
            "Epoch 11284/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2251\n",
            "Epoch 11285/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 11286/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2236\n",
            "Epoch 11287/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2258\n",
            "Epoch 11288/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2247\n",
            "Epoch 11289/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 11290/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2247\n",
            "Epoch 11291/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2233\n",
            "Epoch 11292/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2285\n",
            "Epoch 11293/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11294/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 11295/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2249\n",
            "Epoch 11296/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2255\n",
            "Epoch 11297/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2285\n",
            "Epoch 11298/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2258\n",
            "Epoch 11299/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2246\n",
            "Epoch 11300/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2222\n",
            "Epoch 11301/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 11302/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2279\n",
            "Epoch 11303/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2251\n",
            "Epoch 11304/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2228\n",
            "Epoch 11305/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2242\n",
            "Epoch 11306/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2230\n",
            "Epoch 11307/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2252\n",
            "Epoch 11308/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2208\n",
            "Epoch 11309/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2244\n",
            "Epoch 11310/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2242\n",
            "Epoch 11311/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2253\n",
            "Epoch 11312/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2261\n",
            "Epoch 11313/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11314/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2246\n",
            "Epoch 11315/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2205\n",
            "Epoch 11316/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2262\n",
            "Epoch 11317/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 11318/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2247\n",
            "Epoch 11319/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2243\n",
            "Epoch 11320/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2247\n",
            "Epoch 11321/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2236\n",
            "Epoch 11322/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2244\n",
            "Epoch 11323/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2225\n",
            "Epoch 11324/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2216\n",
            "Epoch 11325/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2219\n",
            "Epoch 11326/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 11327/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11328/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2236\n",
            "Epoch 11329/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2225\n",
            "Epoch 11330/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2239\n",
            "Epoch 11331/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2223\n",
            "Epoch 11332/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2262\n",
            "Epoch 11333/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 11334/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11335/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2194\n",
            "Epoch 11336/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2206\n",
            "Epoch 11337/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2191\n",
            "Epoch 11338/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2253\n",
            "Epoch 11339/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2269\n",
            "Epoch 11340/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2227\n",
            "Epoch 11341/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 11342/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2252\n",
            "Epoch 11343/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2262\n",
            "Epoch 11344/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 11345/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11346/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2277\n",
            "Epoch 11347/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2201\n",
            "Epoch 11348/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11349/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 11350/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 11351/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2255\n",
            "Epoch 11352/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2210\n",
            "Epoch 11353/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 11354/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11355/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2253\n",
            "Epoch 11356/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2243\n",
            "Epoch 11357/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2233\n",
            "Epoch 11358/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2275\n",
            "Epoch 11359/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2254\n",
            "Epoch 11360/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2264\n",
            "Epoch 11361/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11362/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2258\n",
            "Epoch 11363/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2269\n",
            "Epoch 11364/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2243\n",
            "Epoch 11365/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2223\n",
            "Epoch 11366/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2228\n",
            "Epoch 11367/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2260\n",
            "Epoch 11368/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11369/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2237\n",
            "Epoch 11370/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11371/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2197\n",
            "Epoch 11372/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2271\n",
            "Epoch 11373/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2286\n",
            "Epoch 11374/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2236\n",
            "Epoch 11375/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2241\n",
            "Epoch 11376/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2232\n",
            "Epoch 11377/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2262\n",
            "Epoch 11378/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2267\n",
            "Epoch 11379/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2232\n",
            "Epoch 11380/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2246\n",
            "Epoch 11381/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2273\n",
            "Epoch 11382/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2259\n",
            "Epoch 11383/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 11384/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2253\n",
            "Epoch 11385/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2221\n",
            "Epoch 11386/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 11387/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 11388/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11389/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2247\n",
            "Epoch 11390/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11391/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11392/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11393/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2195\n",
            "Epoch 11394/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2257\n",
            "Epoch 11395/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2245\n",
            "Epoch 11396/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2228\n",
            "Epoch 11397/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2217\n",
            "Epoch 11398/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2254\n",
            "Epoch 11399/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2211\n",
            "Epoch 11400/500000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2232\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-11399.h5\n",
            "prompt: \n",
            "from his mind, he found himself believing that she had spoken the\n",
            "truth, for he had but little vanity, and soon her refusal seemed a\n",
            "natural thing to him. He slipped through all the grades of despondency\n",
            "until he reached a bottom of absolute gloom. Failure seemed to mark the\n",
            "whole of his life; he had failed with Katharine, and now he had failed\n",
            "with Mary. Up at once sprang\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:#d4e6f1;\">k, she had </span>e<span style=\"background-color:#d6dbdf;\">ven about </span>Denham.<br><br>Sun<span style=\"background-color:#d4e6f1;\">spend a little </span>sull<span style=\"background-color:#f6ddcc;\">en from<br>the </span>dai<span style=\"background-color:#d6eaf8;\">ly up and </span>also op<span style=\"background-color:#d4efdf;\">ener of th</span>an<span style=\"background-color:#e2d7d5;\"> however, for the </span>rich<span style=\"background-color:#edebd0;\">es; she had </span>trushing<span style=\"background-color:#ecf3cf;\"> night; and the</span> same wors<span style=\"background-color:#d6eaf8;\">e dark and </span>sh<span style=\"background-color:#ebdef0;\">a young man with </span><span style=\"background-color:#d6eaf8;\">her voice.<br><br>A</span>rth-beetl<span style=\"background-color:#ecf3cf;\">e over which </span>th"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#edebd0;\">Jane Austen: Lady Susan</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 26s 1s/step - loss: 0.2232\n",
            "Epoch 11401/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2270\n",
            "Epoch 11402/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2235\n",
            "Epoch 11403/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2280\n",
            "Epoch 11404/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2245\n",
            "Epoch 11405/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2253\n",
            "Epoch 11406/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2231\n",
            "Epoch 11407/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11408/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 11409/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2239\n",
            "Epoch 11410/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2269\n",
            "Epoch 11411/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2263\n",
            "Epoch 11412/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2247\n",
            "Epoch 11413/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2218\n",
            "Epoch 11414/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2241\n",
            "Epoch 11415/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 11416/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2254\n",
            "Epoch 11417/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11418/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2240\n",
            "Epoch 11419/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2253\n",
            "Epoch 11420/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2252\n",
            "Epoch 11421/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2257\n",
            "Epoch 11422/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2248\n",
            "Epoch 11423/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11424/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2265\n",
            "Epoch 11425/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2204\n",
            "Epoch 11426/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2222\n",
            "Epoch 11427/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2239\n",
            "Epoch 11428/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2278\n",
            "Epoch 11429/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2258\n",
            "Epoch 11430/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2231\n",
            "Epoch 11431/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2281\n",
            "Epoch 11432/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2230\n",
            "Epoch 11433/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2241\n",
            "Epoch 11434/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2230\n",
            "Epoch 11435/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2245\n",
            "Epoch 11436/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 11437/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2269\n",
            "Epoch 11438/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2209\n",
            "Epoch 11439/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2270\n",
            "Epoch 11440/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 11441/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2268\n",
            "Epoch 11442/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2268\n",
            "Epoch 11443/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2240\n",
            "Epoch 11444/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2276\n",
            "Epoch 11445/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2273\n",
            "Epoch 11446/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2251\n",
            "Epoch 11447/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2249\n",
            "Epoch 11448/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2246\n",
            "Epoch 11449/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11450/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2274\n",
            "Epoch 11451/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2254\n",
            "Epoch 11452/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2230\n",
            "Epoch 11453/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2261\n",
            "Epoch 11454/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2254\n",
            "Epoch 11455/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 11456/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2238\n",
            "Epoch 11457/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2239\n",
            "Epoch 11458/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 11459/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11460/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2228\n",
            "Epoch 11461/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2233\n",
            "Epoch 11462/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2242\n",
            "Epoch 11463/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2267\n",
            "Epoch 11464/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2236\n",
            "Epoch 11465/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2250\n",
            "Epoch 11466/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2249\n",
            "Epoch 11467/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11468/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 11469/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2201\n",
            "Epoch 11470/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 11471/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2181\n",
            "Epoch 11472/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11473/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2259\n",
            "Epoch 11474/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 11475/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2187\n",
            "Epoch 11476/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 11477/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2204\n",
            "Epoch 11478/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2193\n",
            "Epoch 11479/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2201\n",
            "Epoch 11480/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2199\n",
            "Epoch 11481/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2157\n",
            "Epoch 11482/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2231\n",
            "Epoch 11483/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 11484/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2213\n",
            "Epoch 11485/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11486/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 11487/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 11488/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 11489/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11490/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 11491/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 11492/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 11493/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2184\n",
            "Epoch 11494/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 11495/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2219\n",
            "Epoch 11496/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2212\n",
            "Epoch 11497/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2204\n",
            "Epoch 11498/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11499/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2213\n",
            "Epoch 11500/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2196\n",
            "Epoch 11501/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11502/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2177\n",
            "Epoch 11503/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2184\n",
            "Epoch 11504/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 11505/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2248\n",
            "Epoch 11506/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2182\n",
            "Epoch 11507/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2240\n",
            "Epoch 11508/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2180\n",
            "Epoch 11509/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2146\n",
            "Epoch 11510/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2228\n",
            "Epoch 11511/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2204\n",
            "Epoch 11512/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2192\n",
            "Epoch 11513/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2240\n",
            "Epoch 11514/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2215\n",
            "Epoch 11515/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2215\n",
            "Epoch 11516/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2216\n",
            "Epoch 11517/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2231\n",
            "Epoch 11518/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11519/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2236\n",
            "Epoch 11520/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2175\n",
            "Epoch 11521/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11522/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11523/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11524/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2238\n",
            "Epoch 11525/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 11526/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11527/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2191\n",
            "Epoch 11528/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 11529/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2204\n",
            "Epoch 11530/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 11531/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2190\n",
            "Epoch 11532/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2249\n",
            "Epoch 11533/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2217\n",
            "Epoch 11534/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2160\n",
            "Epoch 11535/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2164\n",
            "Epoch 11536/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2194\n",
            "Epoch 11537/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11538/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2254\n",
            "Epoch 11539/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11540/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2186\n",
            "Epoch 11541/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2174\n",
            "Epoch 11542/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2209\n",
            "Epoch 11543/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 11544/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2184\n",
            "Epoch 11545/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2207\n",
            "Epoch 11546/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2227\n",
            "Epoch 11547/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2209\n",
            "Epoch 11548/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 11549/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2233\n",
            "Epoch 11550/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2212\n",
            "Epoch 11551/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11552/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2249\n",
            "Epoch 11553/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2223\n",
            "Epoch 11554/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 11555/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11556/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2253\n",
            "Epoch 11557/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2225\n",
            "Epoch 11558/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2210\n",
            "Epoch 11559/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2205\n",
            "Epoch 11560/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11561/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2211\n",
            "Epoch 11562/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2250\n",
            "Epoch 11563/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 11564/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2240\n",
            "Epoch 11565/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2212\n",
            "Epoch 11566/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2187\n",
            "Epoch 11567/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11568/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2259\n",
            "Epoch 11569/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2183\n",
            "Epoch 11570/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11571/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2195\n",
            "Epoch 11572/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 11573/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11574/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 11575/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 11576/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 11577/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11578/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2228\n",
            "Epoch 11579/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2187\n",
            "Epoch 11580/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2256\n",
            "Epoch 11581/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 11582/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 11583/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2207\n",
            "Epoch 11584/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 11585/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2217\n",
            "Epoch 11586/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2231\n",
            "Epoch 11587/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2220\n",
            "Epoch 11588/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2233\n",
            "Epoch 11589/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2183\n",
            "Epoch 11590/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2272\n",
            "Epoch 11591/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2236\n",
            "Epoch 11592/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2209\n",
            "Epoch 11593/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 11594/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 11595/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11596/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2238\n",
            "Epoch 11597/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 11598/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 11599/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 11600/500000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2224\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-11599.h5\n",
            "prompt: e pavement. Nobody of his\n",
            "build appeared. She scrutinized each male figure as it approached and\n",
            "passed her. Each male figure had, nevertheless, a look of him, due,\n",
            "perhaps, to the professional dress, the quick step, the keen glance\n",
            "which they cast upon her as they hastened home after the day’s work.\n",
            "The square itself, with its imme\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "ns<span style=\"background-color:#e2d7d5;\">e of death</span>er,<br><span style=\"background-color:#d4efdf;\">side that </span><span style=\"background-color:#d8daef;\">strain of </span><span style=\"background-color:#e2d7d5;\">thought; she</span><span style=\"background-color:#d4e6f1;\"> was, and<br></span><span style=\"background-color:#d8daef;\">the dreadful</span>—only briefly<br>sh<span style=\"background-color:#e2d7d5;\">es, but we</span><span style=\"background-color:#eadbd8;\">re the first</span>—s<span style=\"background-color:#d4e6f1;\">ince a little</span><br>e<span style=\"background-color:#eadbd8;\">fference he</span>ar<span style=\"background-color:#d0ece7;\">d of power</span> which beneale<span style=\"background-color:#d8daef;\"> venturing </span>her furtlac<span style=\"background-color:#ebdef0;\">ed,<br>and there was </span>a<span style=\"background-color:#ebdef0;\"> bread and </span>muffordin"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 26s 1s/step - loss: 0.2224\n",
            "Epoch 11601/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2233\n",
            "Epoch 11602/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11603/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2233\n",
            "Epoch 11604/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11605/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11606/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2250\n",
            "Epoch 11607/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2231\n",
            "Epoch 11608/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2231\n",
            "Epoch 11609/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 11610/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2209\n",
            "Epoch 11611/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2208\n",
            "Epoch 11612/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2256\n",
            "Epoch 11613/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2205\n",
            "Epoch 11614/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2248\n",
            "Epoch 11615/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2263\n",
            "Epoch 11616/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2241\n",
            "Epoch 11617/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 11618/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2235\n",
            "Epoch 11619/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11620/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2217\n",
            "Epoch 11621/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2216\n",
            "Epoch 11622/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2269\n",
            "Epoch 11623/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11624/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11625/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2236\n",
            "Epoch 11626/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2238\n",
            "Epoch 11627/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2246\n",
            "Epoch 11628/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2257\n",
            "Epoch 11629/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 11630/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2223\n",
            "Epoch 11631/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2243\n",
            "Epoch 11632/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11633/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2202\n",
            "Epoch 11634/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2258\n",
            "Epoch 11635/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2242\n",
            "Epoch 11636/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2228\n",
            "Epoch 11637/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2198\n",
            "Epoch 11638/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 11639/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 11640/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 11641/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2190\n",
            "Epoch 11642/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2198\n",
            "Epoch 11643/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11644/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11645/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11646/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2228\n",
            "Epoch 11647/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 11648/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2227\n",
            "Epoch 11649/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2210\n",
            "Epoch 11650/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2231\n",
            "Epoch 11651/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2239\n",
            "Epoch 11652/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11653/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2230\n",
            "Epoch 11654/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2238\n",
            "Epoch 11655/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11656/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2270\n",
            "Epoch 11657/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11658/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11659/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2213\n",
            "Epoch 11660/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2213\n",
            "Epoch 11661/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2210\n",
            "Epoch 11662/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2234\n",
            "Epoch 11663/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2228\n",
            "Epoch 11664/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2179\n",
            "Epoch 11665/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2264\n",
            "Epoch 11666/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 11667/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11668/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2237\n",
            "Epoch 11669/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2213\n",
            "Epoch 11670/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 11671/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2241\n",
            "Epoch 11672/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11673/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2272\n",
            "Epoch 11674/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 11675/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2232\n",
            "Epoch 11676/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2206\n",
            "Epoch 11677/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2219\n",
            "Epoch 11678/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2221\n",
            "Epoch 11679/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 11680/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2236\n",
            "Epoch 11681/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2247\n",
            "Epoch 11682/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2189\n",
            "Epoch 11683/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2246\n",
            "Epoch 11684/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 11685/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2233\n",
            "Epoch 11686/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 11687/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11688/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2223\n",
            "Epoch 11689/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2237\n",
            "Epoch 11690/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2225\n",
            "Epoch 11691/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2241\n",
            "Epoch 11692/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2228\n",
            "Epoch 11693/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11694/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11695/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2221\n",
            "Epoch 11696/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2232\n",
            "Epoch 11697/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11698/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2216\n",
            "Epoch 11699/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11700/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 11701/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 11702/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2276\n",
            "Epoch 11703/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2197\n",
            "Epoch 11704/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2264\n",
            "Epoch 11705/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11706/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2239\n",
            "Epoch 11707/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2202\n",
            "Epoch 11708/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2201\n",
            "Epoch 11709/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2207\n",
            "Epoch 11710/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2214\n",
            "Epoch 11711/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2232\n",
            "Epoch 11712/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2228\n",
            "Epoch 11713/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11714/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 11715/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11716/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11717/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2249\n",
            "Epoch 11718/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2223\n",
            "Epoch 11719/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2215\n",
            "Epoch 11720/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2191\n",
            "Epoch 11721/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2264\n",
            "Epoch 11722/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2225\n",
            "Epoch 11723/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2246\n",
            "Epoch 11724/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2223\n",
            "Epoch 11725/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11726/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11727/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2272\n",
            "Epoch 11728/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2190\n",
            "Epoch 11729/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2230\n",
            "Epoch 11730/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 11731/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2196\n",
            "Epoch 11732/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2175\n",
            "Epoch 11733/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2205\n",
            "Epoch 11734/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2182\n",
            "Epoch 11735/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2178\n",
            "Epoch 11736/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2190\n",
            "Epoch 11737/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11738/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2212\n",
            "Epoch 11739/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2172\n",
            "Epoch 11740/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2177\n",
            "Epoch 11741/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2216\n",
            "Epoch 11742/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 11743/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 11744/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2216\n",
            "Epoch 11745/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2189\n",
            "Epoch 11746/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2178\n",
            "Epoch 11747/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2173\n",
            "Epoch 11748/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2204\n",
            "Epoch 11749/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2178\n",
            "Epoch 11750/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2180\n",
            "Epoch 11751/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2170\n",
            "Epoch 11752/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2240\n",
            "Epoch 11753/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2167\n",
            "Epoch 11754/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2149\n",
            "Epoch 11755/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2182\n",
            "Epoch 11756/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2231\n",
            "Epoch 11757/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2175\n",
            "Epoch 11758/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 11759/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2171\n",
            "Epoch 11760/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 11761/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2194\n",
            "Epoch 11762/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2219\n",
            "Epoch 11763/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2191\n",
            "Epoch 11764/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2148\n",
            "Epoch 11765/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11766/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2210\n",
            "Epoch 11767/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2205\n",
            "Epoch 11768/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 11769/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2201\n",
            "Epoch 11770/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 11771/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2198\n",
            "Epoch 11772/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2132\n",
            "Epoch 11773/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2204\n",
            "Epoch 11774/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2245\n",
            "Epoch 11775/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2188\n",
            "Epoch 11776/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 11777/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2185\n",
            "Epoch 11778/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2212\n",
            "Epoch 11779/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2201\n",
            "Epoch 11780/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2209\n",
            "Epoch 11781/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2194\n",
            "Epoch 11782/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2158\n",
            "Epoch 11783/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11784/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 11785/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 11786/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 11787/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 11788/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2201\n",
            "Epoch 11789/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2170\n",
            "Epoch 11790/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2241\n",
            "Epoch 11791/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2195\n",
            "Epoch 11792/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2187\n",
            "Epoch 11793/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2245\n",
            "Epoch 11794/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 11795/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2189\n",
            "Epoch 11796/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2190\n",
            "Epoch 11797/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2200\n",
            "Epoch 11798/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11799/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 11800/500000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2173\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-11799.h5\n",
            "prompt: f, more kindly than I\n",
            "expected.\n",
            "\n",
            "“Not at your command!” retorted Hareton. “If you set store on him,\n",
            "you’d better be quiet.”\n",
            "\n",
            "“Then I hope his ghost will haunt you; and I hope Mr. Heathcliff will\n",
            "never get another tenant till the Grange is a ruin,” she answered,\n",
            "sharply.\n",
            "\n",
            "“Hearken, hearken, shoo’s cursing on ’em\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "? A borough—<span style=\"background-color:#ebdef0;\">never caught </span>bid human ’<br>word Grange<span style=\"background-color:#eadbd8;\">—and this </span>wes sudo<span style=\"background-color:#d6dbdf;\">g, do not </span><span style=\"background-color:#ecf3cf;\">know whether it </span><span style=\"background-color:#d0ece7;\">so very different</span><span style=\"background-color:#e2d7d5;\"> thing would </span><span style=\"background-color:#d4efdf;\">have been in the co</span>mpany of<span style=\"background-color:#e2d7d5;\"><br>her. The </span>healt<span style=\"background-color:#d4efdf;\"> is no beau</span><span style=\"background-color:#ecf3cf;\">, and perhaps s</span><span style=\"background-color:#d0ece7;\">he<br>ought to </span>make my bsence or i"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 25s 1s/step - loss: 0.2173\n",
            "Epoch 11801/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2190\n",
            "Epoch 11802/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2206\n",
            "Epoch 11803/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2190\n",
            "Epoch 11804/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2196\n",
            "Epoch 11805/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 11806/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11807/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 11808/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2165\n",
            "Epoch 11809/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2216\n",
            "Epoch 11810/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11811/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2192\n",
            "Epoch 11812/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2218\n",
            "Epoch 11813/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2202\n",
            "Epoch 11814/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 11815/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2189\n",
            "Epoch 11816/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11817/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2209\n",
            "Epoch 11818/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2189\n",
            "Epoch 11819/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 11820/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2226\n",
            "Epoch 11821/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2250\n",
            "Epoch 11822/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2202\n",
            "Epoch 11823/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 11824/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2200\n",
            "Epoch 11825/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2227\n",
            "Epoch 11826/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 11827/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 11828/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11829/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 11830/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2202\n",
            "Epoch 11831/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2205\n",
            "Epoch 11832/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11833/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 11834/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2189\n",
            "Epoch 11835/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11836/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2213\n",
            "Epoch 11837/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2215\n",
            "Epoch 11838/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2231\n",
            "Epoch 11839/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2173\n",
            "Epoch 11840/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2228\n",
            "Epoch 11841/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2209\n",
            "Epoch 11842/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11843/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2215\n",
            "Epoch 11844/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2171\n",
            "Epoch 11845/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11846/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11847/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 11848/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 11849/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2183\n",
            "Epoch 11850/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11851/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 11852/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2212\n",
            "Epoch 11853/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2216\n",
            "Epoch 11854/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2196\n",
            "Epoch 11855/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2204\n",
            "Epoch 11856/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11857/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11858/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2187\n",
            "Epoch 11859/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 11860/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11861/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2205\n",
            "Epoch 11862/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2204\n",
            "Epoch 11863/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 11864/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2175\n",
            "Epoch 11865/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2219\n",
            "Epoch 11866/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2183\n",
            "Epoch 11867/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 11868/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 11869/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2215\n",
            "Epoch 11870/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2180\n",
            "Epoch 11871/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2198\n",
            "Epoch 11872/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 11873/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2226\n",
            "Epoch 11874/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 11875/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2226\n",
            "Epoch 11876/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 11877/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11878/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2198\n",
            "Epoch 11879/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2220\n",
            "Epoch 11880/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2211\n",
            "Epoch 11881/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11882/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2253\n",
            "Epoch 11883/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 11884/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11885/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2219\n",
            "Epoch 11886/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 11887/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11888/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2194\n",
            "Epoch 11889/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 11890/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11891/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11892/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2230\n",
            "Epoch 11893/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2232\n",
            "Epoch 11894/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 11895/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2201\n",
            "Epoch 11896/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2232\n",
            "Epoch 11897/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2237\n",
            "Epoch 11898/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2211\n",
            "Epoch 11899/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2187\n",
            "Epoch 11900/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2179\n",
            "Epoch 11901/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2222\n",
            "Epoch 11902/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 11903/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2232\n",
            "Epoch 11904/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2211\n",
            "Epoch 11905/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11906/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2220\n",
            "Epoch 11907/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2233\n",
            "Epoch 11908/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2180\n",
            "Epoch 11909/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2164\n",
            "Epoch 11910/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2239\n",
            "Epoch 11911/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11912/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2151\n",
            "Epoch 11913/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2206\n",
            "Epoch 11914/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11915/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2165\n",
            "Epoch 11916/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11917/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 11918/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 11919/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2240\n",
            "Epoch 11920/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2260\n",
            "Epoch 11921/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2209\n",
            "Epoch 11922/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2249\n",
            "Epoch 11923/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2245\n",
            "Epoch 11924/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2244\n",
            "Epoch 11925/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11926/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11927/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2196\n",
            "Epoch 11928/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2206\n",
            "Epoch 11929/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 11930/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11931/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 11932/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11933/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11934/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2206\n",
            "Epoch 11935/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2240\n",
            "Epoch 11936/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11937/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2209\n",
            "Epoch 11938/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2196\n",
            "Epoch 11939/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 11940/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2223\n",
            "Epoch 11941/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2225\n",
            "Epoch 11942/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11943/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 11944/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11945/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2254\n",
            "Epoch 11946/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 11947/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 11948/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 11949/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2207\n",
            "Epoch 11950/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 11951/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2233\n",
            "Epoch 11952/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11953/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2237\n",
            "Epoch 11954/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2233\n",
            "Epoch 11955/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2217\n",
            "Epoch 11956/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 11957/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 11958/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 11959/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 11960/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2204\n",
            "Epoch 11961/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2202\n",
            "Epoch 11962/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2184\n",
            "Epoch 11963/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2182\n",
            "Epoch 11964/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 11965/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2210\n",
            "Epoch 11966/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 11967/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2208\n",
            "Epoch 11968/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 11969/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2198\n",
            "Epoch 11970/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2202\n",
            "Epoch 11971/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2214\n",
            "Epoch 11972/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2216\n",
            "Epoch 11973/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2200\n",
            "Epoch 11974/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 11975/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2230\n",
            "Epoch 11976/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2216\n",
            "Epoch 11977/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2178\n",
            "Epoch 11978/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 11979/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2192\n",
            "Epoch 11980/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 11981/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2220\n",
            "Epoch 11982/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 11983/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2201\n",
            "Epoch 11984/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2213\n",
            "Epoch 11985/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 11986/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2244\n",
            "Epoch 11987/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 11988/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2202\n",
            "Epoch 11989/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2197\n",
            "Epoch 11990/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 11991/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2181\n",
            "Epoch 11992/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 11993/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2145\n",
            "Epoch 11994/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2173\n",
            "Epoch 11995/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 11996/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2177\n",
            "Epoch 11997/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2184\n",
            "Epoch 11998/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2145\n",
            "Epoch 11999/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2201\n",
            "Epoch 12000/500000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2183\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-11999.h5\n",
            "prompt: ech,\n",
            "      was afraid her anxiety had done no good. Others of the party were\n",
            "      now applied to.\n",
            "\n",
            "      “If I,” said Mr. Collins, “were so fortunate as to be able to\n",
            "      sing, I should have great pleasure, I am sure, in obliging the\n",
            "      company with an air; for I consider music as a very innocent\n",
            "      diversion, and perfectly compatible with the profession of a\n",
            "      clergy\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "d<span style=\"background-color:#d4e6f1;\">ia must be </span><span style=\"background-color:#eadbd8;\">to either of </span><span style=\"background-color:#ecf3cf;\">his wife of</span><span style=\"background-color:#eadbd8;\"><br>      did not </span><span style=\"background-color:#d4e6f1;\">have been their </span>ball-command<span style=\"background-color:#eadbd8;\">’s.”<br><br>      </span><span style=\"background-color:#eadbd8;\">Elizabeth was e</span>mply estip<span style=\"background-color:#eadbd8;\"> from<br>      th</span><span style=\"background-color:#d6eaf8;\">at steadily </span>unlike<span style=\"background-color:#d0ece7;\">, a young man, </span>nor his runguardiats—<span style=\"background-color:#d0ece7;\">The event, </span><span style=\"background-color:#d0ece7;\">she used to </span>as<span style=\"background-color:#eadbd8;\">k to<br>      </span>A"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 25s 1s/step - loss: 0.2183\n",
            "Epoch 12001/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2182\n",
            "Epoch 12002/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2165\n",
            "Epoch 12003/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2142\n",
            "Epoch 12004/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2174\n",
            "Epoch 12005/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2184\n",
            "Epoch 12006/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 12007/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2164\n",
            "Epoch 12008/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2159\n",
            "Epoch 12009/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2148\n",
            "Epoch 12010/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2177\n",
            "Epoch 12011/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2183\n",
            "Epoch 12012/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 12013/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 12014/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2194\n",
            "Epoch 12015/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 12016/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2157\n",
            "Epoch 12017/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2187\n",
            "Epoch 12018/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2193\n",
            "Epoch 12019/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 12020/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2182\n",
            "Epoch 12021/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2204\n",
            "Epoch 12022/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2202\n",
            "Epoch 12023/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2157\n",
            "Epoch 12024/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 12025/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2190\n",
            "Epoch 12026/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2187\n",
            "Epoch 12027/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2161\n",
            "Epoch 12028/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2192\n",
            "Epoch 12029/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2205\n",
            "Epoch 12030/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2183\n",
            "Epoch 12031/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2185\n",
            "Epoch 12032/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2170\n",
            "Epoch 12033/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2198\n",
            "Epoch 12034/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 12035/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2183\n",
            "Epoch 12036/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2156\n",
            "Epoch 12037/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2192\n",
            "Epoch 12038/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 12039/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2192\n",
            "Epoch 12040/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 12041/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 12042/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2173\n",
            "Epoch 12043/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2171\n",
            "Epoch 12044/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2174\n",
            "Epoch 12045/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2170\n",
            "Epoch 12046/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2194\n",
            "Epoch 12047/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 12048/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2186\n",
            "Epoch 12049/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2201\n",
            "Epoch 12050/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 12051/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2227\n",
            "Epoch 12052/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2195\n",
            "Epoch 12053/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2198\n",
            "Epoch 12054/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 12055/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 12056/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2194\n",
            "Epoch 12057/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2194\n",
            "Epoch 12058/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2208\n",
            "Epoch 12059/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2204\n",
            "Epoch 12060/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2176\n",
            "Epoch 12061/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2166\n",
            "Epoch 12062/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2168\n",
            "Epoch 12063/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2192\n",
            "Epoch 12064/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 12065/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2204\n",
            "Epoch 12066/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2186\n",
            "Epoch 12067/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2189\n",
            "Epoch 12068/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2235\n",
            "Epoch 12069/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 12070/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2226\n",
            "Epoch 12071/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2182\n",
            "Epoch 12072/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 12073/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2185\n",
            "Epoch 12074/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 12075/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2184\n",
            "Epoch 12076/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2176\n",
            "Epoch 12077/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2222\n",
            "Epoch 12078/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2178\n",
            "Epoch 12079/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 12080/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2192\n",
            "Epoch 12081/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2185\n",
            "Epoch 12082/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2194\n",
            "Epoch 12083/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2180\n",
            "Epoch 12084/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2166\n",
            "Epoch 12085/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2187\n",
            "Epoch 12086/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2192\n",
            "Epoch 12087/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 12088/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2192\n",
            "Epoch 12089/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 12090/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2172\n",
            "Epoch 12091/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 12092/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2181\n",
            "Epoch 12093/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2209\n",
            "Epoch 12094/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2199\n",
            "Epoch 12095/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 12096/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2238\n",
            "Epoch 12097/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 12098/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 12099/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2225\n",
            "Epoch 12100/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2181\n",
            "Epoch 12101/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 12102/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2197\n",
            "Epoch 12103/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2238\n",
            "Epoch 12104/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2212\n",
            "Epoch 12105/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2171\n",
            "Epoch 12106/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2196\n",
            "Epoch 12107/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2212\n",
            "Epoch 12108/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2182\n",
            "Epoch 12109/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2203\n",
            "Epoch 12110/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2165\n",
            "Epoch 12111/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2201\n",
            "Epoch 12112/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2217\n",
            "Epoch 12113/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2211\n",
            "Epoch 12114/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2206\n",
            "Epoch 12115/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2208\n",
            "Epoch 12116/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 12117/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2170\n",
            "Epoch 12118/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2176\n",
            "Epoch 12119/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2208\n",
            "Epoch 12120/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2203\n",
            "Epoch 12121/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2211\n",
            "Epoch 12122/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2205\n",
            "Epoch 12123/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 12124/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 12125/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2185\n",
            "Epoch 12126/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 12127/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 12128/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 12129/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2214\n",
            "Epoch 12130/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2197\n",
            "Epoch 12131/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2199\n",
            "Epoch 12132/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2222\n",
            "Epoch 12133/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2218\n",
            "Epoch 12134/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2180\n",
            "Epoch 12135/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2216\n",
            "Epoch 12136/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 12137/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 12138/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2209\n",
            "Epoch 12139/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 12140/500000\n",
            "23/23 [==============================] - 1s 50ms/step - loss: 0.2187\n",
            "Epoch 12141/500000\n",
            "23/23 [==============================] - 1s 52ms/step - loss: 0.2197\n",
            "Epoch 12142/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2163\n",
            "Epoch 12143/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2202\n",
            "Epoch 12144/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2196\n",
            "Epoch 12145/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2199\n",
            "Epoch 12146/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2201\n",
            "Epoch 12147/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2220\n",
            "Epoch 12148/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2162\n",
            "Epoch 12149/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2190\n",
            "Epoch 12150/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2218\n",
            "Epoch 12151/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2167\n",
            "Epoch 12152/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2169\n",
            "Epoch 12153/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2177\n",
            "Epoch 12154/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2204\n",
            "Epoch 12155/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2186\n",
            "Epoch 12156/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 12157/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2192\n",
            "Epoch 12158/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 12159/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 12160/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 12161/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 12162/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2182\n",
            "Epoch 12163/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2187\n",
            "Epoch 12164/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 12165/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2223\n",
            "Epoch 12166/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 12167/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2205\n",
            "Epoch 12168/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 12169/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2196\n",
            "Epoch 12170/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2197\n",
            "Epoch 12171/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2206\n",
            "Epoch 12172/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2231\n",
            "Epoch 12173/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2209\n",
            "Epoch 12174/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 12175/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2205\n",
            "Epoch 12176/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2191\n",
            "Epoch 12177/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 12178/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2218\n",
            "Epoch 12179/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2180\n",
            "Epoch 12180/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2234\n",
            "Epoch 12181/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 12182/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2178\n",
            "Epoch 12183/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2204\n",
            "Epoch 12184/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 12185/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2222\n",
            "Epoch 12186/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2181\n",
            "Epoch 12187/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 12188/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2212\n",
            "Epoch 12189/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 12190/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 12191/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2229\n",
            "Epoch 12192/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2224\n",
            "Epoch 12193/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2212\n",
            "Epoch 12194/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2216\n",
            "Epoch 12195/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2187\n",
            "Epoch 12196/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2191\n",
            "Epoch 12197/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2190\n",
            "Epoch 12198/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 12199/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 12200/500000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2239\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_8x(4, 4, 4, 4, 4, 4, 4, 4)x(512, 512, 512, 512, 512, 512, 512, 512)x20000/cp-12199.h5\n",
            "prompt: oon to the flourish of his wife’s violin. “Why, weeds can\n",
            "be bad enough, can’t they, Vinrace? I remember crossing in the\n",
            "_Mauretania_ once, and saying to the Captain—Richards—did you know\n",
            "him?—‘Now tell me what perils you really dread most for your ship,\n",
            "Captain Richards?’ expect\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:#d4e6f1;\"> in the heavy </span><span style=\"background-color:#eadbd8;\">your father, </span>Charles!<br>Vill<span style=\"background-color:#d0ece7;\">s here. I </span><span style=\"background-color:#d4e6f1;\">will, very </span>bad could<br>if<span style=\"background-color:#e2d7d5;\"> he said e</span><span style=\"background-color:#d4efdf;\">very thing by </span>his bovey<br>fixed<span style=\"background-color:#ecf3cf;\"> black to </span>V<span style=\"background-color:#d0ece7;\">icture it </span>was<br>Si<span style=\"background-color:#eadbd8;\">r objection </span>just<span style=\"background-color:#d4e6f1;\"> so improve</span><span style=\"background-color:#e2d7d5;\">s,<br>and the b</span>oxed S"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 25s 1s/step - loss: 0.2239\n",
            "Epoch 12201/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2237\n",
            "Epoch 12202/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2216\n",
            "Epoch 12203/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2199\n",
            "Epoch 12204/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 12205/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 12206/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 12207/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 12208/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2214\n",
            "Epoch 12209/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2217\n",
            "Epoch 12210/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2190\n",
            "Epoch 12211/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2205\n",
            "Epoch 12212/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2223\n",
            "Epoch 12213/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2156\n",
            "Epoch 12214/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2226\n",
            "Epoch 12215/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2209\n",
            "Epoch 12216/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2216\n",
            "Epoch 12217/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2204\n",
            "Epoch 12218/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 12219/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2176\n",
            "Epoch 12220/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2203\n",
            "Epoch 12221/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2188\n",
            "Epoch 12222/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 12223/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2213\n",
            "Epoch 12224/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2229\n",
            "Epoch 12225/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2194\n",
            "Epoch 12226/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2217\n",
            "Epoch 12227/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 12228/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2208\n",
            "Epoch 12229/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2195\n",
            "Epoch 12230/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2210\n",
            "Epoch 12231/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2200\n",
            "Epoch 12232/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2224\n",
            "Epoch 12233/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2211\n",
            "Epoch 12234/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2191\n",
            "Epoch 12235/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2219\n",
            "Epoch 12236/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2213\n",
            "Epoch 12237/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2195\n",
            "Epoch 12238/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2234\n",
            "Epoch 12239/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2211\n",
            "Epoch 12240/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2247\n",
            "Epoch 12241/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2176\n",
            "Epoch 12242/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2197\n",
            "Epoch 12243/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2186\n",
            "Epoch 12244/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2207\n",
            "Epoch 12245/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2220\n",
            "Epoch 12246/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2219\n",
            "Epoch 12247/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2216\n",
            "Epoch 12248/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2133\n",
            "Epoch 12249/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2189\n",
            "Epoch 12250/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2151\n",
            "Epoch 12251/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2177\n",
            "Epoch 12252/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2148\n",
            "Epoch 12253/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2193\n",
            "Epoch 12254/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2155\n",
            "Epoch 12255/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2196\n",
            "Epoch 12256/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2187\n",
            "Epoch 12257/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2185\n",
            "Epoch 12258/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2191\n",
            "Epoch 12259/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2192\n",
            "Epoch 12260/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2133\n",
            "Epoch 12261/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2186\n",
            "Epoch 12262/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2179\n",
            "Epoch 12263/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2177\n",
            "Epoch 12264/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2135\n",
            "Epoch 12265/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2198\n",
            "Epoch 12266/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2167\n",
            "Epoch 12267/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2167\n",
            "Epoch 12268/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2179\n",
            "Epoch 12269/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2183\n",
            "Epoch 12270/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2178\n",
            "Epoch 12271/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2195\n",
            "Epoch 12272/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2170\n",
            "Epoch 12273/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2166\n",
            "Epoch 12274/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2163\n",
            "Epoch 12275/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2145\n",
            "Epoch 12276/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2192\n",
            "Epoch 12277/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2160\n",
            "Epoch 12278/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2189\n",
            "Epoch 12279/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2152\n",
            "Epoch 12280/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2167\n",
            "Epoch 12281/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2215\n",
            "Epoch 12282/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2173\n",
            "Epoch 12283/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2186\n",
            "Epoch 12284/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2157\n",
            "Epoch 12285/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2171\n",
            "Epoch 12286/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2192\n",
            "Epoch 12287/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2182\n",
            "Epoch 12288/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2181\n",
            "Epoch 12289/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2193\n",
            "Epoch 12290/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2221\n",
            "Epoch 12291/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2185\n",
            "Epoch 12292/500000\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2140\n",
            "Epoch 12293/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2171\n",
            "Epoch 12294/500000\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.2166\n",
            "Epoch 12295/500000\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2176\n",
            "Epoch 12296/500000\n",
            " 1/23 [>.............................] - ETA: 1s - loss: 0.2214"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-1e85eedf3601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mservice_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for TPU we need to role our own checkpointer since we need to transfer the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \"\"\"\n\u001b[1;32m   1158\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    steps_per_epoch=restricted_batches//params['batch_size']\n",
        "    if steps_per_epoch < 1:\n",
        "        steps_per_epoch = 1\n",
        "    history = model.fit(dataset, epochs=EPOCHS, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, callbacks=[service_callback]) # for TPU we need to role our own checkpointer since we need to transfer the weights\n",
        "else:\n",
        "    history = model.fit(dataset, validation_data=validation_dataset, epochs=EPOCHS, initial_epoch=initial_epoch, callbacks=[checkpoint_callback, tensorboard_callback, service_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "a81LdPyY2dyo"
      },
      "outputs": [],
      "source": [
        "model_cpu.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "uxDNYZiEQtgF"
      },
      "outputs": [],
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "def doDialog(model):\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 128 # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "    print(\"Please enter some dialog.\")\n",
        "    print(\"The net will answer according to your input.\")\n",
        "    print(\"'bye' for end,\")\n",
        "    print(\"'reset' to reset the conversation context,\")\n",
        "    print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "    print(\"    to change character of the dialog.\")\n",
        "    print(\"    Current temperature={}.\".format(temperature))\n",
        "    print()\n",
        "    xso = None\n",
        "    bye = False\n",
        "    doini = True\n",
        "    bye = False\n",
        "    while not bye:\n",
        "        print(\"> \", end=\"\")\n",
        "        prompt = input()\n",
        "        if prompt == 'bye':\n",
        "            bye = True\n",
        "            print(\"Good bye!\")\n",
        "            continue\n",
        "        if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "            t = float(prompt[len(\"temperature=\"):])\n",
        "            if t > 0.05 and t < 1.4:\n",
        "                temperature = t\n",
        "                print(\"(generator temperature now {})\".format(t))\n",
        "                print()\n",
        "                continue\n",
        "            print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "            continue\n",
        "        reply=mhsa_generate(model, prompt, gen_len=256, temperature=temperature, verbose=True)\n",
        "        td.source_highlight(reply, min_quote_size=13, dark_mode=use_dark_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0JEPK2WIQtgI",
        "outputId": "62b8d204-c17b-40ad-9c19-ed34251328bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter some dialog.\n",
            "The net will answer according to your input.\n",
            "'bye' for end,\n",
            "'reset' to reset the conversation context,\n",
            "'temperature=<float>' [0.1(frozen)-1.0(creative)]\n",
            "    to change character of the dialog.\n",
            "    Current temperature=0.6.\n",
            "\n",
            "> temperature=1.2\n",
            "(generator temperature now 1.2)\n",
            "\n",
            "> Happy day! Glorious! Wonderful sunshine, luck abound!\n",
            "Ellen, I convinced\n",
            "Catherines, an ever sudden drawing.”\n",
            "\n",
            "In ble turned off, is in the country; and\n",
            "where rown time to look to the man who,\n",
            "trank was cle-solges,\n",
            "that se of low el, and spoke, to all\n",
            " know, was not a very dull, no\n",
            "he saw By you every use, which excite attentivers the famor, which was,\n",
            "but Harley setting his knight:\n",
            "\n",
            "“This witters came merce Mrs. Puched\n",
            "weth their rooms, so commat he\n",
            "should that question is at the tree\n",
            "prospectantat her doubt, been an engaged of Servant any the door of the\n",
            "arches, as one Waterlood out over the difference I wanted on, and \n",
            "\n",
            "\n",
            "\"You think particularly I do not want to have hin, your speculby\n",
            "is. It is a poured his clengy with what is the soon!  and power may sake in the Cruit\n",
            "to had late, as Helen too slowly to\n",
            "Steen which some ph was now large,\n",
            "but no more yesyrative.\n",
            "\n",
            "“That happing about Mr. Knightleyk, what isque to\n",
            "one te:’\n",
            "Hells and Mr. Balloyed her friends. Omost of them\n",
            "deal weeks where wean\n",
            "another--seemed into a very eve. “Mealor am so often without everything eight to spebley off to confidentiluncheal; and\n",
            "she rals so by \n",
            "quality was deep,\n",
            "alosed himself alter\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ellen, I convinced<br>Catherines, an ever sudden drawing.”<br><br>In ble<span style=\"background-color:#d4e6f1;\"> turned off, </span><sup>[10]</sup>i<span style=\"background-color:#d4efdf;\">s in the country; and</span><sup>[7]</sup><br>where rown<span style=\"background-color:#d0ece7;\"> time to look </span><sup>[8]</sup><span style=\"background-color:#d4efdf;\">to the man who</span><sup>[7]</sup>,<br>trank was cle-solges,<br>that se of low el, and spoke, to all<br> know,<span style=\"background-color:#e2d7d5;\"> was not a very </span><sup>[4]</sup>dull, no<br>he saw By you every use<span style=\"background-color:#ecf3cf;\">, which excite</span><sup>[6]</sup> attentivers the famor, which was,<br>but Harle<span style=\"background-color:#ecf3cf;\">y setting his </span><sup>[6]</sup>knight:<br><br>“This witters came merce Mrs. Puched<br>weth their rooms, so comm<span style=\"background-color:#e2d7d5;\">at he<br>should </span><sup>[4]</sup><span style=\"background-color:#ecf3cf;\">that question </span><sup>[6]</sup>is at the tree<br>prospectantat her doubt, been an engaged of Servant any<span style=\"background-color:#d6dbdf;\"> the door of the<br>a</span><sup>[11]</sup>rches, as one Waterloo<span style=\"background-color:#d6eaf8;\">d out over the </span><sup>[9]</sup>difference I wanted on, and <br><br>\"You<span style=\"background-color:#d4e6f1;\"> think particularly </span><sup>[10]</sup><span style=\"background-color:#d4efdf;\">I do not want to </span><sup>[7]</sup>have hin, your speculby<br>is. It is a poured his clengy with<span style=\"background-color:#ebdef0;\"> what is the </span><sup>[2]</sup>soon!  and power may sake in the Cruit<br>to had late, as Helen<span style=\"background-color:#e2d7d5;\"> too slowly to</span><sup>[4]</sup><br>Stee<span style=\"background-color:#d6eaf8;\">n which some </span><sup>[9]</sup>ph was now large,<br>but no more yesyrative.<br><br>“That happ<span style=\"background-color:#eadbd8;\">ing about Mr. </span><sup>[3]</sup>Knightleyk, what isque to<br>one te:’<br>Hells and Mr. Balloy<span style=\"background-color:#eadbd8;\">ed her friend</span><sup>[3]</sup>s. O<span style=\"background-color:#d0ece7;\">most of them<br></span><sup>[8]</sup>deal weeks where wean<br>another--seemed<span style=\"background-color:#e2d7d5;\"> into a very </span><sup>[4]</sup>eve. “Mealor am<span style=\"background-color:#d6dbdf;\"> so often with</span><sup>[11]</sup><span style=\"background-color:#d6eaf8;\">out everything </span><sup>[9]</sup>eight to spebley off to confidentiluncheal; and<br>she rals so by quality was deep,<br>al<span style=\"background-color:#d0ece7;\">osed himself </span><sup>[8]</sup>alter"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span><sup>[10]</sup>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span><sup>[7]</sup>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span><sup>[8]</sup>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span><sup>[4]</sup>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span><sup>[6]</sup>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span><sup>[11]</sup>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span><sup>[9]</sup>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span><sup>[2]</sup>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span><sup>[3]</sup></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> temperature=0.8\n",
            "(generator temperature now 0.8)\n",
            "\n",
            "> Happy day, she felt so lucky to finally meet him.\n",
            "\n",
            "He just with the with unexpectedly and was sohow\n",
            "that an answer, that looked in his nonsense,\n",
            "thought herself so decided among themselves\n",
            "nothing, and what banklace minute\n",
            "and commencing, there wasted to her young of\n",
            "every roof; and she seasonably result of\n",
            "last night why their books that\n",
            "more delicitous, disable remone\n",
            "much for Sir, on being equal to her him. Emma was not sorry for him. He cures, makel to\n",
            "living; and as nobody anything on\n",
            "his eye, I know how is\n",
            "perfectly too very greatest indeed\n",
            "being lay for vacity as Mrs. Briting\n",
            "and been d\n",
            "eepasy, theghts again, of cautions, while every cargance; but I could do doubt Hern every other wretched something else\n",
            "seemed to that, and then I was not the same\n",
            "windows to Miss Allening out in Lincontalims\n",
            "Bimately he would meet a new\n",
            "like of liferk knife in\n",
            "the d-post, besides, their engagements,--he small, interested wild women all Don;\n",
            "though, to syndiffort a taste\n",
            "of remblely, which make Richard\n",
            "praise regardal humility, witho\n",
            "time of her tempers would be such and\n",
            "govering sold man whi\n",
            "in and her happy by Bar\n",
            "nepains and the happy side, once\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>He just with the<span style=\"background-color:#d4efdf;\"> with unexpected</span><sup>[7]</sup>ly and was sohow<br>tha<span style=\"background-color:#e2d7d5;\">t an answer, </span><sup>[4]</sup>that looked in<span style=\"background-color:#ecf3cf;\"> his nonsense,</span><sup>[6]</sup><br><span style=\"background-color:#e2d7d5;\">thought herself </span><sup>[4]</sup>so decid<span style=\"background-color:#d0ece7;\">ed among themselves</span><sup>[8]</sup><span style=\"background-color:#e2d7d5;\"><br>nothing, and </span><sup>[4]</sup>what banklace minute<br>and commenc<span style=\"background-color:#d0ece7;\">ing, there was</span><sup>[8]</sup>ted<span style=\"background-color:#f6ddcc;\"> to her young </span><sup>[12]</sup>of<br>every roo<span style=\"background-color:#eadbd8;\">f; and she se</span><sup>[3]</sup>asonably result of<span style=\"background-color:#d4e6f1;\"><br>last night wh</span><sup>[10]</sup>y<span style=\"background-color:#ebdef0;\"> their books </span><sup>[2]</sup>that<br>more delicitous, disable remone<br>much for Sir, on be<span style=\"background-color:#d0ece7;\">ing equal to </span><sup>[8]</sup>her him<span style=\"background-color:#d0ece7;\">. Emma was not </span><sup>[8]</sup><span style=\"background-color:#eadbd8;\">sorry for him</span><sup>[3]</sup>. He cures, makel to<br>living;<span style=\"background-color:#e2d7d5;\"> and as nobody </span><sup>[4]</sup>anything on<br>his eye,<span style=\"background-color:#e2d7d5;\"> I know how i</span><sup>[4]</sup>s<br>perfectly too<span style=\"background-color:#e2d7d5;\"> very greatest i</span><sup>[4]</sup>ndeed<br>being lay for vacity as Mrs. Briting<br>and been deepasy, theghts agai<span style=\"background-color:#d4e6f1;\">n, of caution</span><sup>[10]</sup>s<span style=\"background-color:#d6eaf8;\">, while every </span><sup>[9]</sup>carganc<span style=\"background-color:#eadbd8;\">e; but I could </span><sup>[3]</sup>do doubt Her<span style=\"background-color:#d4e6f1;\">n every other w</span><sup>[10]</sup>retch<span style=\"background-color:#ebdef0;\">ed something </span><sup>[2]</sup>else<br>seemed to<span style=\"background-color:#d8daef;\"> that, and the</span><sup>[1]</sup><span style=\"background-color:#d0ece7;\">n I was not the</span><sup>[8]</sup> same<br>windows to Miss Allening out in Lincontalims<br>Bimately<span style=\"background-color:#d6dbdf;\"> he would meet </span><sup>[11]</sup>a new<br>like of liferk knife in<br>the d-post,<span style=\"background-color:#eadbd8;\"> besides, the</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">ir engagements</span><sup>[3]</sup>,--he small<span style=\"background-color:#e2d7d5;\">, interested </span><sup>[4]</sup>wild women all Don;<br>though, to syndiffort a taste<br>of remble<span style=\"background-color:#d4e6f1;\">ly, which make</span><sup>[10]</sup> Richard<br>praise regardal humility, witho<span style=\"background-color:#d4efdf;\"><br>time of her </span><sup>[7]</sup>temp<span style=\"background-color:#eadbd8;\">ers would be suc</span><sup>[3]</sup>h and<br>govering sold man whiin<span style=\"background-color:#d4e6f1;\"> and her happ</span><sup>[10]</sup>y by Bar<br>nep<span style=\"background-color:#d6eaf8;\">ains and the </span><sup>[9]</sup>happy side, once"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span><sup>[7]</sup>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span><sup>[4]</sup>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span><sup>[6]</sup>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span><sup>[8]</sup>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span><sup>[12]</sup>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span><sup>[3]</sup>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span><sup>[10]</sup>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span><sup>[2]</sup>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span><sup>[9]</sup>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span><sup>[1]</sup>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span><sup>[11]</sup></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> temperature=0.7\n",
            "(generator temperature now 0.7)\n",
            "\n",
            "> And on and on she went, it was a very long voyage into uncertainty.\n",
            " English-grass realial request\n",
            "which he had the desire to keep them all unssion for\n",
            "my temper, I mean that her termen\n",
            "the little ity of the sea-blood bars and words with a emotion. Susan had not toonxious knowledge of them, to understanding,\n",
            "“Lady Seal’s sense\n",
            "rather of one gentleman opportunity of the firstzed these reality else, as she down to\n",
            "disappoint without any glove\n",
            "without his daughter that Perrys,” said Lady Susan.\n",
            "\n",
            "“Oh! I must tell you one can be\n",
            "so expressed his serious. His great were\n",
            "so possibility of needed that\n",
            "E\n",
            "dmund asked her to recollecting your visit.\n",
            "He saw it garden—I drew up in her eyes\n",
            "to have had her opposite surprised speediment and had become shape of\n",
            "consolation, why had now should not have display an opportunity of being together.\n",
            "\n",
            "She called \n",
            "her court opportunity of speaking, her feet\n",
            "part of an admiration; and by every thing’s\n",
            "sity, as if little conscious of woman by Rachel was tremed cold, all their own thoughtfulnotions to London—Mrs. Hilbery’s comfort, Mr. St. John thought of Susan’s penty for a\n",
            "very few weeks of the other room, therefore\n",
            "had been no doubt \n",
            "of the others.\n",
            "\n",
            "Either Helen’s librate and\n",
            "minded, she could\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " English-grass realial request<span style=\"background-color:#e2d7d5;\"><br>which he had t</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">he desire to k</span><sup>[4]</sup><span style=\"background-color:#d0ece7;\">eep them all </span><sup>[8]</sup>unssion for<br>my temper<span style=\"background-color:#d0ece7;\">, I mean that </span><sup>[8]</sup>her term<span style=\"background-color:#ebdef0;\">en<br>the little </span><sup>[2]</sup><span style=\"background-color:#d6eaf8;\">ity of the se</span><sup>[9]</sup>a-blood bars and words with a emotion. Susan had not toonxiou<span style=\"background-color:#d8daef;\">s knowledge of the</span><sup>[1]</sup>m<span style=\"background-color:#eadbd8;\">, to understand</span><sup>[3]</sup>ing,<br>“Lady Seal’s sense<br>rather of<span style=\"background-color:#d4efdf;\"> one gentleman o</span><sup>[7]</sup><span style=\"background-color:#eadbd8;\">pportunity of th</span><sup>[3]</sup>e firstzed these reality else, as she down<span style=\"background-color:#d0ece7;\"> to<br>disappoint </span><sup>[8]</sup><span style=\"background-color:#d4efdf;\">without any g</span><sup>[7]</sup>love<span style=\"background-color:#e2d7d5;\"><br>without his </span><sup>[4]</sup><span style=\"background-color:#edebd0;\">daughter that </span><sup>[5]</sup>Perry<span style=\"background-color:#d4efdf;\">s,” said Lady </span><sup>[7]</sup>Susan.<br><br>“<span style=\"background-color:#d6dbdf;\">Oh! I must tell you</span><sup>[11]</sup> one can be<br>so<span style=\"background-color:#eadbd8;\"> expressed his s</span><sup>[3]</sup>erious. His great were<br>s<span style=\"background-color:#d0ece7;\">o possibility of </span><sup>[8]</sup>needed<span style=\"background-color:#d4e6f1;\"> that<br>Edmund a</span><sup>[10]</sup>sk<span style=\"background-color:#d4e6f1;\">ed her to rec</span><sup>[10]</sup>ol<span style=\"background-color:#ebdef0;\">lecting your </span><sup>[2]</sup>visit.<br>He saw it garden—I drew up<span style=\"background-color:#d0ece7;\"> in her eyes<br></span><sup>[8]</sup><span style=\"background-color:#d4e6f1;\">to have had her </span><sup>[10]</sup>oppos<span style=\"background-color:#d6dbdf;\">ite surprised </span><sup>[11]</sup>speediment an<span style=\"background-color:#d6eaf8;\">d had become </span><sup>[9]</sup>shape o<span style=\"background-color:#f6ddcc;\">f<br>consolation, </span><sup>[12]</sup>why had now<span style=\"background-color:#d0ece7;\"> should not have dis</span><sup>[8]</sup>pla<span style=\"background-color:#d6dbdf;\">y an opportunity of</span><sup>[11]</sup><span style=\"background-color:#d4efdf;\"> being together</span><sup>[7]</sup>.<br><br>She called <br>her cour<span style=\"background-color:#eadbd8;\">t opportunity of speaking,</span><sup>[3]</sup> her feet<br>part<span style=\"background-color:#d4e6f1;\"> of an admira</span><sup>[10]</sup><span style=\"background-color:#d4efdf;\">tion; and by </span><sup>[7]</sup><span style=\"background-color:#d0ece7;\">every thing’s</span><sup>[8]</sup><br>sity, as i<span style=\"background-color:#d4e6f1;\">f little cons</span><sup>[10]</sup>cious<span style=\"background-color:#d0ece7;\"> of woman by </span><sup>[8]</sup>Rachel was tremed cold, <span style=\"background-color:#d4efdf;\">all their own</span><sup>[7]</sup> thoughtfulnot<span style=\"background-color:#f6ddcc;\">ions to London</span><sup>[12]</sup>—<span style=\"background-color:#e2d7d5;\">Mrs. Hilbery’s c</span><sup>[4]</sup>omfort, Mr<span style=\"background-color:#d6eaf8;\">. St. John th</span><sup>[9]</sup>ought of Susan’s penty for<span style=\"background-color:#d6eaf8;\"> a<br>very few weeks </span><sup>[9]</sup><span style=\"background-color:#eadbd8;\">of the other </span><sup>[3]</sup>room,<span style=\"background-color:#d6dbdf;\"> therefore<br>ha</span><sup>[11]</sup>d bee<span style=\"background-color:#d4e6f1;\">n no doubt of the</span><sup>[10]</sup> others.<br><br>Either Helen’s librate and<br>mind<span style=\"background-color:#e2d7d5;\">ed, she could</span><sup>[4]</sup>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span><sup>[4]</sup>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span><sup>[8]</sup>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span><sup>[2]</sup>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span><sup>[9]</sup>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span><sup>[1]</sup>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span><sup>[3]</sup>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span><sup>[7]</sup>, <span style=\"background-color:#edebd0;\">Jane Austen: Lady Susan</span><sup>[5]</sup>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span><sup>[11]</sup>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span><sup>[10]</sup>, <span style=\"background-color:#f6ddcc;\">Jane Austen: Persuasion</span><sup>[12]</sup></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> bye\n",
            "Good bye!\n"
          ]
        }
      ],
      "source": [
        "# Talk to the net!\n",
        "doDialog(model_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMCWf5AZn1-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "VmWbteSFQtfq",
        "yWE_ZZMKEARV"
      ],
      "machine_shape": "hm",
      "name": "transformer_poet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}