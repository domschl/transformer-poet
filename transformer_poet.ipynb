{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/transformer-poet/blob/main/transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Transformer-Poet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabS0VZ-1Zp0"
      },
      "source": [
        "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jtpy59Yq-Qfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71921409-8405-4e39-a4db-3e0b857a7b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ml-indie-tools\n",
            "  Downloading ml_indie_tools-0.3.8-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: ml-indie-tools\n",
            "Successfully installed ml-indie-tools-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U5T4m6earb1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3c45df-3b49-4d38-ecae-45d9e3dc1104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TF-Keras version: 2.9.0\n"
          ]
        }
      ],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
        "\n",
        "from ml_indie_tools.keras_custom_layers import MultiHeadSelfAttention, PositionalEncoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A tensorflow deep multi-head attention model for text generation\n",
        "\n",
        "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
        "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6c66c30c-56ce-4aa9-edf6-8c84a221c727"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OS: Linux, Python: 3.7.15, Colab Jupyter Notebook Tensorflow: 2.9.2, TPU: TPU, 8 nodes v2 (8GB)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
        "\n",
        "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oZ6t9b6ZwSxi"
      },
      "outputs": [],
      "source": [
        "use_eager=tf.executing_eagerly()\n",
        "if ml_env.is_tpu is True:\n",
        "    tpu_strategy = ml_env.tpu_strategy\n",
        "    tpu_is_init=True\n",
        "    if use_eager is True:\n",
        "        tf.config.run_functions_eagerly(False)\n",
        "    use_eager=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t-TP3Pnsrb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77acc2c0-31ba-4c0f-9e07-15d2694ebe30"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Root path (all projects) : /content/drive/My Drive (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
            "Project path             : /content/drive/My Drive/Colab Notebooks/women_writers (Changes to the file system happen only below this project path\n",
            "Model path (snapshots)   : /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf (Model weights and snapshots are stored here)\n",
            "Data path (training data): /content/drive/My Drive/Colab Notebooks/women_writers/data (Training data will be downloaded here)\n",
            "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
          ]
        }
      ],
      "source": [
        "project_name='women_writers'\n",
        "model_name='mhsa_v1_tf'\n",
        "\n",
        "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
        "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
        "#\n",
        "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
        "\n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
        "\n",
        "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
        "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
        "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
        "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
        "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
        "encoding, batch generation, and formatted source display. It read some \n",
        "books from Project Gutenberg and supports creation of training batches. \n",
        "The output functions support highlighting to allow to compare generated \n",
        "texts with the actual sources to help to identify identical (memorized) \n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C66X7ynnrb1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d4fbac-43eb-4a2d-9153-0131e8fdf9fc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
          ]
        }
      ],
      "source": [
        "# sample searches\n",
        "search_spec= {\"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
        "\n",
        "book_list=gd.search(search_spec)\n",
        "book_cnt = len(book_list)\n",
        "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "if book_cnt<40:\n",
        "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
        "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
        "else:\n",
        "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MH6_7IU3upOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11743c65-59fc-4835-9385-036594b52b9a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: The Common Reader - Virginia Woolf, 64457\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
            "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
            "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
            "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
            "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
            "6: Pride and Prejudice - Jane Austen, 42671\n",
            "7: The Letters of Jane Austen - Jane Austen, 42078\n",
            "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
            "9: Jacob's Room - Virginia Woolf, 5670\n",
            "10: Pride and Prejudice - Jane Austen, 1342\n",
            "11: Night and Day - Virginia Woolf, 1245\n",
            "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
            "13: Lady Susan - Jane Austen, 946\n",
            "14: Wuthering Heights - Emily Brontë, 768\n",
            "15: Sense and Sensibility - Jane Austen, 161\n",
            "16: Emma - Jane Austen, 158\n",
            "17: The Voyage Out - Virginia Woolf, 144\n",
            "18: Mansfield Park - Jane Austen, 141\n",
            "19: Northanger Abbey - Jane Austen, 121\n",
            "20: Persuasion - Jane Austen, 105\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(book_list)):\n",
        "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2jBH3Z15rb1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d685025e-4030-453f-c873-66f3f0f58071"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using:\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
            "2: Jacob's Room - Virginia Woolf\n",
            "3: Pride and Prejudice - Jane Austen\n",
            "4: Night and Day - Virginia Woolf\n",
            "5: Lady Susan - Jane Austen\n",
            "6: Wuthering Heights - Emily Brontë\n",
            "7: Sense and Sensibility - Jane Austen\n",
            "8: Emma - Jane Austen\n",
            "9: The Voyage Out - Virginia Woolf\n",
            "10: Mansfield Park - Jane Austen\n",
            "11: Northanger Abbey - Jane Austen\n"
          ]
        }
      ],
      "source": [
        "select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
        "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
        "\n",
        "print(\"Using:\")\n",
        "for i in range(len(sub_book_list)):\n",
        "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
        "\n",
        "textlib_dataset = None  # Forces re-caching\n",
        "td = Text_Dataset(sub_book_list)\n",
        "td.init_tokenizer(tokenizer='ngram', max_ngrams=6, max_tokens=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f7_tc2Lirb1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5d1d51-70e4-4c06-863f-3498099984d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1854539 records\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LEN = 80\n",
        "SUB_PROBABILITY = 0.15  # like BERT\n",
        "\n",
        "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN, content_stepping=1)\n",
        "\n",
        "num_records = len(td)\n",
        "\n",
        "print(f\"{num_records} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zZbbsNm0cOeW"
      },
      "outputs": [],
      "source": [
        "def get_sample_batch(td, batch_size, length, SUB_probability=0.15):\n",
        "    for i in range(batch_size):\n",
        "        Xi = td.get_random_item()\n",
        "        yi = Xi.copy()\n",
        "        l=int(len(Xi)*SUB_probability)\n",
        "        for li in range(l):\n",
        "            pos=random.randint(0,len(Xi)-1)\n",
        "            if td.tokenizer_type=='char':\n",
        "                Xi[pos]=td.c2i['␚']\n",
        "            elif td.tokenizer_type=='word':\n",
        "                Xi[pos]=td.w2i['<subst>']\n",
        "            elif td.tokenizer_type=='ngram':\n",
        "                Xi[pos]=td.t2i['<subst>']\n",
        "            else:\n",
        "                print(f\"Unexpected tokenizer_type {td.tokenizer_type}\")\n",
        "        if i==0:\n",
        "            # smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpX=np.array(Xi, dtype=np.int32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TI3Fx6bNuR9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47fc754d-069d-467b-c109-6cbd8d9e8aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0](l=80): X=>both\n",
            "lighthouse and bird; he was steadfast and brilli<subst><subst>at the same\n",
            "time he was whirled, with all other things, senseless <subst>st t<subst>gl<subst><subst>got up, left his <subst><subst>e of <subst>lver, and pressed on, with the\n",
            "wind against him. The <subst>age of the lighthouse and t<subst>orm full of\n",
            "birds <, y=>both\n",
            "lighthouse and bird; he was steadfast and brilliant; and at the same\n",
            "time he was whirled, with all other things, senseless against the\n",
            "glass. He got up, left his tribute of silver, and pressed on, with the\n",
            "wind against him. The image of the lighthouse and the storm full of\n",
            "birds <\n",
            "[1](l=80): X=>uncle and aunt<subst>  <subst>he was in town; and why not to me<subst>If he fears me, why come\n",
            "      hi<subst>? If he<subst>longer cares for <subst>why silent? T<subst>ing,\n",
            "     <subst>as<subst>man! I will think no more about him.”\n",
            "\n",
            "  <subst>Her r<subst>olution was for a short time involuntarily <subst>pt by th<, y=>uncle and aunt,\n",
            "      when he was in town; and why not to me? If he fears me, why come\n",
            "      hither? If he no longer cares for me, why silent? Teasing,\n",
            "      teasing, man! I will think no more about him.”\n",
            "\n",
            "      Her resolution was for a short time involuntarily kept by th<\n"
          ]
        }
      ],
      "source": [
        "test_x, test_y = get_sample_batch(td, 2, 40, SUB_probability=SUB_PROBABILITY)\n",
        "for i in range(len(test_x)):\n",
        "    xi=[int(x) for x in test_x[i]]\n",
        "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<, y=>{td.decode(test_y[i])}<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qnMxRkkmcOeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a578ded7-64ba-4f5b-9a5f-c55c5182ad00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 80), (2, 80))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "test_x.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jn_LcJ6g9Mzy"
      },
      "outputs": [],
      "source": [
        "def expand_name_template(template, params):\n",
        "    exp=copy.copy(template)\n",
        "    for key in params:\n",
        "        src=\"{\"+key+\"}\"\n",
        "        dst=f\"{params[key]}\"\n",
        "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
        "    return exp\n",
        "\n",
        "def save_model_metadata(epoch, suffix='std'):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    params['current_epoch'] = epoch\n",
        "    try:\n",
        "        with open(meta_file, 'w') as f:\n",
        "            f.write(json.dumps(params))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def read_model_metadata(suffix=\"std\"):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    try:\n",
        "        with open(meta_file, 'r') as f:\n",
        "            meta = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
        "        return None\n",
        "    return meta\n",
        "\n",
        "def is_metadata_compatible(params, meta):\n",
        "    is_valid=True\n",
        "    keys=set(list(params.keys())+list(meta.keys()))\n",
        "    for key in keys:\n",
        "        if key in updatable_keys:\n",
        "            continue\n",
        "        if key not in meta:\n",
        "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif key not in params:\n",
        "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif meta[key]!=params[key]:\n",
        "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "    if is_valid is False:\n",
        "        print(\"Aborting import.\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "znpIUA3ig3gO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87aa508e-073c-4e92-aac0-7489491ad1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuing last session from epoch 6873\n",
            "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 6, 'heads': [6, 6, 6, 6, 6, 6], 'units': [512, 512, 512, 512, 512, 512], 'norm': 'softmax', 'mh_normalize': True, 'l2_regularizer': 1e-09, 'dropout': 0.0, 'join_heads_by_add': True, 'vocab_size': 5000, 'sequence_len': 80, 'embedding_size': 128, 'batch_size': 256, 'learning_rate': 0.0002, 'clipvalue': None, 'sample_every_n_epochs': 100, 'current_epoch': 6873}\n"
          ]
        }
      ],
      "source": [
        "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
        "\n",
        "lyrs = 6;\n",
        "\n",
        "params = { # Multi-head self-attention\n",
        "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
        "\n",
        "    'mhsa_layers': lyrs, \n",
        "    'heads': [6]*lyrs,\n",
        "    'units': [512]*lyrs,  # 0 inserts an LSTM for memory-states :-)\n",
        "    'norm': 'softmax', # this is for within each head\n",
        "    'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
        "    'l2_regularizer': 1e-9,\n",
        "    'dropout': 0.0,       # no dropout: 0.0\n",
        "    'join_heads_by_add': True,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads\n",
        "    'vocab_size': vocabulary_size,\n",
        "    'sequence_len': SEQUENCE_LEN,\n",
        "    'embedding_size': 128,\n",
        "\n",
        "    'batch_size': 256,\n",
        "    'learning_rate': 0.0002,\n",
        "    'clipvalue': None,\n",
        "    'sample_every_n_epochs': 100,\n",
        "}\n",
        "\n",
        "if len(params['heads'])!=params['mhsa_layers'] or len(params['units'])!=params['mhsa_layers']:\n",
        "    print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
        "    \n",
        "if ml_env.is_tpu is True:\n",
        "    lr = params['learning_rate']*1.0\n",
        "else:\n",
        "    lr = params['learning_rate']\n",
        "\n",
        "model_suffix = expand_name_template(params['name'], params)\n",
        "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
        "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
        "if os.path.exists(checkpoint_dir) is False:\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# When comparing if training-data is compatible with new params set, \n",
        "# the following keys are updatable, they can be changed while continuing\n",
        "# to use existing checkpoints and continue training with those values\n",
        "# changed:\n",
        "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
        "             'sample_every_n_epochs']\n",
        "\n",
        "# These values are taking from saved checkpoint:\n",
        "keep_keys=['current_epoch']\n",
        "\n",
        "continue_last = True\n",
        "if continue_last is False:\n",
        "    print(\"NOT continuing based on existing training! New start.\")\n",
        "\n",
        "meta = read_model_metadata(suffix=model_suffix)\n",
        "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
        "    for key in keep_keys:\n",
        "        if key in meta:\n",
        "            params[key]=meta[key]\n",
        "    if params is not None:\n",
        "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
        "    else:\n",
        "        print(f\"No previous data, starting new model\")\n",
        "else:\n",
        "    print(\"Starting new model\")\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jY3hUuhQYzdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0874160a-6d13-494e-94f0-3cc819ad2c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches = 7244\n"
          ]
        }
      ],
      "source": [
        "num_batches = num_records // params['batch_size']\n",
        "print(f\"num_batches = {num_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EeB7jugCV4lI"
      },
      "outputs": [],
      "source": [
        "# @tf.function   (only slows things down [considerably!])\n",
        "def make_tf_dataset(num, random_index=False, SUB_probability=0.0):\n",
        "    dx=[]\n",
        "    dy=[]\n",
        "    num_batches_active = num\n",
        "    for i in range(num_batches_active):\n",
        "        x,y=get_sample_batch(td, params['batch_size'], params['sequence_len'], SUB_probability=SUB_probability)\n",
        "        if i<1:\n",
        "            print(f\"[{num} x]: {x.shape} -> {y.shape}\")\n",
        "        dx.append(x)\n",
        "        dy.append(y)\n",
        "    dx=np.array(dx)\n",
        "    dy=np.array(dy)\n",
        "    print(f\"dx.shape={dx.shape}, dy.shape={dy.shape}\")\n",
        "    data_xy = (dx, dy)\n",
        "    tf_dataset=tf.data.Dataset.from_tensor_slices(data_xy)\n",
        "    return tf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DCy7WmQyS9T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f8efb4-62db-4589-e584-7258653356d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7244 batches\n",
            "Creating dataset, this is slow. Be patient...\n",
            "[7244 x]: (256, 80) -> (256, 80)\n",
            "dx.shape=(7244, 256, 80), dy.shape=(7244, 256, 80)\n",
            "Dataset done and cached.\n"
          ]
        }
      ],
      "source": [
        "MAX_NUM_BATCHES = 50000\n",
        "\n",
        "if num_batches>MAX_NUM_BATCHES:\n",
        "    restricted_batches=MAX_NUM_BATCHES\n",
        "    print(f\"Restrictinig {num_batches} to max of {restricted_batches}\")\n",
        "else:\n",
        "    restricted_batches=num_batches\n",
        "    print(f\"{restricted_batches} batches\")\n",
        "if cached_batch_data == restricted_batches and textlib_dataset is not None:\n",
        "    print(\"Reusing cached training-data\")\n",
        "else:\n",
        "    print(\"Creating dataset, this is slow. Be patient...\")\n",
        "    textlib_dataset = make_tf_dataset(restricted_batches, SUB_probability=SUB_PROBABILITY)\n",
        "    cached_batch_data = restricted_batches\n",
        "    print(\"Dataset done and cached.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "boow8wR7sLwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2040205d-8280-4a3d-a057-bcb3d706e74d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256, 80), dtype=tf.int32, name=None), TensorSpec(shape=(256, 80), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "shuffle_buffer=10000\n",
        "if ml_env.is_tpu is True:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer).repeat()  # Otherwise TPU may run dry\n",
        "else:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
        "dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "B-G5HLMqqbeT"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    validation_dataset = make_tf_dataset(10, random_index=True, SUB_probability=SUB_PROBABILITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZAzFlCVBiL0Q"
      },
      "outputs": [],
      "source": [
        "def model_mhsa(inputs, params):\n",
        "    dense = layers.Dense(params['vocab_size'], kernel_regularizer=regularizers.l2(params['l2_regularizer']))  # using softmax here prevents temperature adjust, affects 'from_logits' param in sparse_categorical loss \n",
        "    fl = layers.Flatten()\n",
        "    dr = layers.Dropout(params['dropout'])\n",
        "    pe = PositionalEncoding(amplitude=0.3)\n",
        "    rs_up = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    if 0 in params['units']:\n",
        "        lstm1 = layers.LSTM(units=vocabulary_size, return_sequences=True)\n",
        "    if vocabulary_size>=300:\n",
        "        emb=layers.Embedding(vocabulary_size,params['embedding_size'],input_length=params['sequence_len'])\n",
        "    rs_down = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    mhsa=[]\n",
        "    residuals=[]\n",
        "\n",
        "    for i in range(params['mhsa_layers']):\n",
        "        if params['units'][i]==0:\n",
        "            mhsa.append(None)\n",
        "            residuals.append(i)\n",
        "        else:\n",
        "            mhsa.append(MultiHeadSelfAttention(params['heads'][i], units=params['units'][i], norm=params['norm'], mh_normalize=params['mh_normalize'], join_heads_by_add=params['join_heads_by_add']))\n",
        "    xint = tf.cast(inputs,dtype=tf.int32)\n",
        "    if vocabulary_size<300:\n",
        "        x = tf.one_hot(xint, params['vocab_size'], axis=-1)\n",
        "    else:\n",
        "        x = emb(xint)\n",
        "    x = pe(x)\n",
        "    for i in range(len(mhsa)):\n",
        "        if i in residuals:\n",
        "            x = rs_down(lstm1(rs_up(x)))+x\n",
        "            print(f\"Residual at layer {i} added.\")\n",
        "        else:\n",
        "            x = mhsa[i](x)\n",
        "        # x = mhsa[i](x,x)\n",
        "    if params['dropout']>0.0:\n",
        "        x = dr(x)\n",
        "    # x = dense(fl(x))\n",
        "    x = dense(x)\n",
        "    return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4J13Gp_hjqqn"
      },
      "outputs": [],
      "source": [
        "def mhsa_generate(model, text, gen_len=64, temperature=0.9, argmax=False, verbose=False):\n",
        "    if verbose is True:\n",
        "        full=text[:-1]\n",
        "    gen_text=\"\"\n",
        "    lf=0\n",
        "    input = np.array(td.encode(text))\n",
        "    while len(input) < params['sequence_len']:\n",
        "        input = np.concatenate([td.encode('<pad>'),input])\n",
        "    for i in range(gen_len):\n",
        "        input = np.concatenate([input[1:],td.encode('<subst>')])\n",
        "        if len(input)!=params['sequence_len']:\n",
        "            print('assertion failure')\n",
        "            return None\n",
        "        pred = model(input)\n",
        "        pred /= temperature\n",
        "        pred = tf.keras.layers.Softmax()(pred)\n",
        "        if tf.executing_eagerly() is True and ml_env.is_tpu is False:\n",
        "            pred=pred.numpy()\n",
        "        else:\n",
        "            pred=tf.keras.backend.eval(pred)  # this is a cheat, it internaly used Numpy() too.\n",
        "        if argmax is True:\n",
        "            pred=np.argmax(pred[0],axis=1)\n",
        "        else:\n",
        "            pred = [np.random.choice(list(range(len(pred[0][-1]))), p=pred[0][-1])]\n",
        "        input = np.concatenate([input[1:],[pred[-1]]])\n",
        "        c = td.decode([pred[-1]])\n",
        "        if verbose is True:\n",
        "            print(c, end='')\n",
        "            if c=='\\n':\n",
        "                lf=0\n",
        "            else:\n",
        "                lf += 1\n",
        "                if (lf>80 and c==' ') or lf>120:\n",
        "                    print()\n",
        "                    lf=0\n",
        "            full+=c\n",
        "        gen_text+=c\n",
        "    if verbose is True:\n",
        "        print()\n",
        "    return gen_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nf-NHZ326NqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2459f99-8f6f-4cb3-92b6-71cf4f50ba78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TPU-scope model\n",
            "Creating Default-scope model\n"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        print(\"Creating TPU-scope model\")\n",
        "        inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "        outputs = model_mhsa(inputs, params)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    print(\"Creating Default-scope model\")\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model_cpu = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "else:\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    model_cpu = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SXx-nBe5-jyJ"
      },
      "outputs": [],
      "source": [
        "def get_newest_checkpoint(checkpoint_dir):\n",
        "    files = os.listdir(checkpoint_dir)\n",
        "    paths = [os.path.join(checkpoint_dir, basename) for basename in files]\n",
        "    return max(paths, key=os.path.getctime)\n",
        "\n",
        "def import_previous_compatible_checkpoint(model, force_import=False):\n",
        "    meta = read_model_metadata(suffix=model_suffix)\n",
        "    if meta is None:\n",
        "        print(\"No previous checkpoint found\")\n",
        "        return False\n",
        "    if is_metadata_compatible(params, meta) is not True and force_import is False:\n",
        "        print(\"No useable import found.\")\n",
        "        return False\n",
        "    try:\n",
        "        last_checkpoint = get_newest_checkpoint(checkpoint_dir) # Doesn't do anything: tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot determine last checkpoint in {checkpoint_dir}, cannot import due to: {e}\")\n",
        "        return False\n",
        "    print(f\"Last checkpoint: {last_checkpoint}\")\n",
        "    try:\n",
        "        model.load_weights(last_checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to import model {last_checkpoint}: {e}\")\n",
        "        return False\n",
        "    if 'current_epoch' in meta:\n",
        "        params['current_epoch'] = meta['current_epoch']\n",
        "    print(f\"Successful import of epoch {params['current_epoch']} from {last_checkpoint}, continuing from there...\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB-Q8YXvndE"
      },
      "source": [
        "### Loss function, optimizer, tensorboard output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0t5JWEdYZNGz"
      },
      "outputs": [],
      "source": [
        "kscc = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def loss(labels, logits):\n",
        "  vl=kscc(labels, logits)\n",
        "  return vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "jc2kbGoAZXHi"
      },
      "outputs": [],
      "source": [
        "if params['clipvalue'] is not None:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "else:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        model.compile(optimizer=opti, loss=loss, metrics=[], run_eagerly=False, jit_compile=True)\n",
        "else:\n",
        "    model.compile(optimizer=opti, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DAoMxogcX_Nq"
      },
      "outputs": [],
      "source": [
        "import_checkpoint = False\n",
        "force_import = False   # True: ignore metadata and try import anyway. This will of course crash, if the new model doesn't fit the checkpoint-data...\n",
        "\n",
        "if import_checkpoint is True:\n",
        "    import_previous_compatible_checkpoint(model, force_import=force_import)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb651ec1-2d36-426c-e309-e9c938cc3e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"mhsa_v1_tf\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 80)]              0         \n",
            "                                                                 \n",
            " tf.cast (TFOpLambda)        (None, 80)                0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 80, 128)           640000    \n",
            "                                                                 \n",
            " positional_encoding (Positi  (None, 80, 128)          0         \n",
            " onalEncoding)                                                   \n",
            "                                                                 \n",
            " multi_head_self_attention (  (None, 80, 128)          1606144   \n",
            " MultiHeadSelfAttention)                                         \n",
            "                                                                 \n",
            " multi_head_self_attention_1  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_2  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_3  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_4  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_5  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 80, 5000)          645000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,921,864\n",
            "Trainable params: 10,921,864\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OHZurM5ei95K"
      },
      "outputs": [],
      "source": [
        "TPU_GENERATE_ON_CPU = False  # The thing is: both options are slow on TPU :-/\n",
        "\n",
        "class ServiceCallback(keras.callbacks.Callback):\n",
        "#    def on_test_end(self, logs=None):\n",
        "    # @tf.function\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_model_metadata(epoch, suffix=model_suffix)\n",
        "        if (epoch+1) % params['sample_every_n_epochs'] == 0:\n",
        "            idx=random.randint(0,len(td)-1)\n",
        "            text=td.decode(td[idx])\n",
        "            print()\n",
        "            if ml_env.is_tpu is True:\n",
        "                temp_list=[0.7] # [0.6,0.7,0.8]\n",
        "                gen_len=50\n",
        "                with tpu_strategy.scope():\n",
        "                    weights=model.get_weights()\n",
        "                model_cpu.set_weights(weights)\n",
        "                # HDF5 is required for saving weights that originate from TPU\n",
        "                # otherwise this just silently fails...\n",
        "                checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.h5\")\n",
        "                chkpt_dest=checkpoint_path.format(epoch=epoch)\n",
        "                print(f\"Checkpoint: {chkpt_dest}\")\n",
        "                model_cpu.save_weights(chkpt_dest)\n",
        "            else:\n",
        "                temp_list=[0.6, 0.7, 0.8]\n",
        "                gen_len=192\n",
        "            print(f\"prompt: {text}\")\n",
        "            for temp in temp_list:\n",
        "                print(f\"---------------- T={temp} ---------------\")\n",
        "                if ml_env.is_tpu is True and TPU_GENERATE_ON_CPU is True:\n",
        "                    with tf.device('/cpu:0'):\n",
        "                        if temp==0.0:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                        else:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                else:\n",
        "                    if temp==0.0:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                    else:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                td.source_highlight(reply, min_quote_size=10, dark_mode=use_dark_mode, display_ref_anchor=False)\n",
        "            print(\"--------------------------------------\")\n",
        "\n",
        "service_callback=ServiceCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5SKvObcsEAQ5"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "if ml_env.is_tpu:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='epoch', write_graph=False)\n",
        "else:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "o0Ew6pgWzeFj"
      },
      "outputs": [],
      "source": [
        "# Dont try:\n",
        "#    # use the python variable log_path:\n",
        "#   get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
        "#except:\n",
        "#   pass\n",
        "\n",
        "# The following throws errors on non-colab, but the guarding above is too bug-ridden.\n",
        "# if ml_env.is_tpu is False:\n",
        "#    %tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFbZcN0vxOB"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kh2yUKBoEAQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098906d2-e82d-4bb1-e10a-7dfce5e2eef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING override of sample_every_n_epochs sample-generation to: 200\n"
          ]
        }
      ],
      "source": [
        "EPOCHS=500000\n",
        "if 'current_epoch' in params:\n",
        "    initial_epoch=params['current_epoch']\n",
        "else:\n",
        "    initial_epoch=0\n",
        "\n",
        "override=200\n",
        "print(f\"WARNING override of sample_every_n_epochs sample-generation to: {override}\")\n",
        "params['sample_every_n_epochs']=override"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d00a408a-29f3-419c-b58f-7c5ecd9416c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6874/500000\n",
            " 5/28 [====>.........................] - ETA: 0s - loss: 5.8706"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 3.8225s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28/28 [==============================] - 49s 43ms/step - loss: 2.6447\n",
            "Epoch 6875/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1974\n",
            "Epoch 6876/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1604\n",
            "Epoch 6877/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1498\n",
            "Epoch 6878/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1466\n",
            "Epoch 6879/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1383\n",
            "Epoch 6880/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1377\n",
            "Epoch 6881/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1324\n",
            "Epoch 6882/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1317\n",
            "Epoch 6883/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1301\n",
            "Epoch 6884/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1293\n",
            "Epoch 6885/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1240\n",
            "Epoch 6886/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1219\n",
            "Epoch 6887/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1220\n",
            "Epoch 6888/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1206\n",
            "Epoch 6889/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1188\n",
            "Epoch 6890/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1185\n",
            "Epoch 6891/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1169\n",
            "Epoch 6892/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1156\n",
            "Epoch 6893/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1147\n",
            "Epoch 6894/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1141\n",
            "Epoch 6895/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1133\n",
            "Epoch 6896/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1136\n",
            "Epoch 6897/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1131\n",
            "Epoch 6898/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1117\n",
            "Epoch 6899/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1110\n",
            "Epoch 6900/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1092\n",
            "Epoch 6901/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1099\n",
            "Epoch 6902/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1084\n",
            "Epoch 6903/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1114\n",
            "Epoch 6904/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1080\n",
            "Epoch 6905/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1089\n",
            "Epoch 6906/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1057\n",
            "Epoch 6907/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1055\n",
            "Epoch 6908/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1077\n",
            "Epoch 6909/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1061\n",
            "Epoch 6910/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1060\n",
            "Epoch 6911/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1048\n",
            "Epoch 6912/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1039\n",
            "Epoch 6913/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1036\n",
            "Epoch 6914/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1061\n",
            "Epoch 6915/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1039\n",
            "Epoch 6916/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1018\n",
            "Epoch 6917/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1042\n",
            "Epoch 6918/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1038\n",
            "Epoch 6919/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1026\n",
            "Epoch 6920/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1043\n",
            "Epoch 6921/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1054\n",
            "Epoch 6922/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 1.1013\n",
            "Epoch 6923/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 1.1034\n",
            "Epoch 6924/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1003\n",
            "Epoch 6925/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 1.1014\n",
            "Epoch 6926/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1012\n",
            "Epoch 6927/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1005\n",
            "Epoch 6928/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1009\n",
            "Epoch 6929/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0978\n",
            "Epoch 6930/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1017\n",
            "Epoch 6931/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0998\n",
            "Epoch 6932/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1008\n",
            "Epoch 6933/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0993\n",
            "Epoch 6934/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1008\n",
            "Epoch 6935/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1009\n",
            "Epoch 6936/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0979\n",
            "Epoch 6937/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0996\n",
            "Epoch 6938/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1020\n",
            "Epoch 6939/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0989\n",
            "Epoch 6940/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0990\n",
            "Epoch 6941/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0967\n",
            "Epoch 6942/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0965\n",
            "Epoch 6943/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0979\n",
            "Epoch 6944/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0985\n",
            "Epoch 6945/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0983\n",
            "Epoch 6946/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0985\n",
            "Epoch 6947/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0975\n",
            "Epoch 6948/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0991\n",
            "Epoch 6949/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0957\n",
            "Epoch 6950/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0949\n",
            "Epoch 6951/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0961\n",
            "Epoch 6952/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0969\n",
            "Epoch 6953/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0964\n",
            "Epoch 6954/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0967\n",
            "Epoch 6955/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0944\n",
            "Epoch 6956/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0936\n",
            "Epoch 6957/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0919\n",
            "Epoch 6958/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0924\n",
            "Epoch 6959/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0928\n",
            "Epoch 6960/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0923\n",
            "Epoch 6961/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0912\n",
            "Epoch 6962/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0901\n",
            "Epoch 6963/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0919\n",
            "Epoch 6964/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0922\n",
            "Epoch 6965/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 1.0917\n",
            "Epoch 6966/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0894\n",
            "Epoch 6967/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0881\n",
            "Epoch 6968/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0895\n",
            "Epoch 6969/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0897\n",
            "Epoch 6970/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0904\n",
            "Epoch 6971/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0885\n",
            "Epoch 6972/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0869\n",
            "Epoch 6973/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0868\n",
            "Epoch 6974/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0882\n",
            "Epoch 6975/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0884\n",
            "Epoch 6976/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0865\n",
            "Epoch 6977/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0852\n",
            "Epoch 6978/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0846\n",
            "Epoch 6979/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 1.0856\n",
            "Epoch 6980/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0853\n",
            "Epoch 6981/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0842\n",
            "Epoch 6982/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0846\n",
            "Epoch 6983/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0826\n",
            "Epoch 6984/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0836\n",
            "Epoch 6985/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0821\n",
            "Epoch 6986/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0842\n",
            "Epoch 6987/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0816\n",
            "Epoch 6988/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0813\n",
            "Epoch 6989/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0819\n",
            "Epoch 6990/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0817\n",
            "Epoch 6991/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0791\n",
            "Epoch 6992/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0787\n",
            "Epoch 6993/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0797\n",
            "Epoch 6994/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0771\n",
            "Epoch 6995/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0806\n",
            "Epoch 6996/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0767\n",
            "Epoch 6997/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0755\n",
            "Epoch 6998/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0773\n",
            "Epoch 6999/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0746\n",
            "Epoch 7000/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 1.0725\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-6999.h5\n",
            "prompt: and in old age\n",
            "the art is practised mostly for its uses, and friendships and other\n",
            "adventures and experiments in the art of reading character are seldom\n",
            "made. But novelists differ from the rest of the world because they do\n",
            "not cease to be interested in character when they have learnt enough\n",
            "a\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ", she of on ob anyalll hoabfor ar ing hiying  dome, who ink. “rs, dld atmarrhe g pro, to imrelsshould <br>WhD.<br>rs. !’<span style=\"background-color:#eadbd8;\">rother’s i</span>st!elllen’ton!,” saiBine go"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 903ms/step - loss: 1.0724\n",
            "Epoch 7001/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0705\n",
            "Epoch 7002/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0708\n",
            "Epoch 7003/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0697\n",
            "Epoch 7004/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0694\n",
            "Epoch 7005/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0708\n",
            "Epoch 7006/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0663\n",
            "Epoch 7007/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0672\n",
            "Epoch 7008/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0679\n",
            "Epoch 7009/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0661\n",
            "Epoch 7010/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0640\n",
            "Epoch 7011/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0635\n",
            "Epoch 7012/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0613\n",
            "Epoch 7013/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0596\n",
            "Epoch 7014/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0601\n",
            "Epoch 7015/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0581\n",
            "Epoch 7016/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0571\n",
            "Epoch 7017/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0573\n",
            "Epoch 7018/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0568\n",
            "Epoch 7019/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0547\n",
            "Epoch 7020/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0526\n",
            "Epoch 7021/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0510\n",
            "Epoch 7022/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0497\n",
            "Epoch 7023/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0465\n",
            "Epoch 7024/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0465\n",
            "Epoch 7025/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0457\n",
            "Epoch 7026/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0450\n",
            "Epoch 7027/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0447\n",
            "Epoch 7028/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0415\n",
            "Epoch 7029/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0371\n",
            "Epoch 7030/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0346\n",
            "Epoch 7031/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0327\n",
            "Epoch 7032/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0335\n",
            "Epoch 7033/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0325\n",
            "Epoch 7034/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0285\n",
            "Epoch 7035/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0267\n",
            "Epoch 7036/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0258\n",
            "Epoch 7037/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0216\n",
            "Epoch 7038/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0183\n",
            "Epoch 7039/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0134\n",
            "Epoch 7040/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0146\n",
            "Epoch 7041/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0108\n",
            "Epoch 7042/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0055\n",
            "Epoch 7043/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0041\n",
            "Epoch 7044/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0012\n",
            "Epoch 7045/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9993\n",
            "Epoch 7046/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9923\n",
            "Epoch 7047/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9921\n",
            "Epoch 7048/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9865\n",
            "Epoch 7049/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9814\n",
            "Epoch 7050/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9815\n",
            "Epoch 7051/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9744\n",
            "Epoch 7052/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9730\n",
            "Epoch 7053/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9680\n",
            "Epoch 7054/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9686\n",
            "Epoch 7055/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9633\n",
            "Epoch 7056/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9623\n",
            "Epoch 7057/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9546\n",
            "Epoch 7058/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9548\n",
            "Epoch 7059/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9515\n",
            "Epoch 7060/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9507\n",
            "Epoch 7061/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9447\n",
            "Epoch 7062/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9438\n",
            "Epoch 7063/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9402\n",
            "Epoch 7064/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9376\n",
            "Epoch 7065/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9337\n",
            "Epoch 7066/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9344\n",
            "Epoch 7067/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9270\n",
            "Epoch 7068/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9278\n",
            "Epoch 7069/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9217\n",
            "Epoch 7070/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9177\n",
            "Epoch 7071/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9166\n",
            "Epoch 7072/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9163\n",
            "Epoch 7073/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9127\n",
            "Epoch 7074/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9117\n",
            "Epoch 7075/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9065\n",
            "Epoch 7076/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9067\n",
            "Epoch 7077/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9022\n",
            "Epoch 7078/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8987\n",
            "Epoch 7079/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8966\n",
            "Epoch 7080/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.8936\n",
            "Epoch 7081/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8925\n",
            "Epoch 7082/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8895\n",
            "Epoch 7083/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.8855\n",
            "Epoch 7084/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8808\n",
            "Epoch 7085/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8800\n",
            "Epoch 7086/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8803\n",
            "Epoch 7087/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.8737\n",
            "Epoch 7088/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8727\n",
            "Epoch 7089/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8680\n",
            "Epoch 7090/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8671\n",
            "Epoch 7091/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.8643\n",
            "Epoch 7092/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8568\n",
            "Epoch 7093/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8565\n",
            "Epoch 7094/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8549\n",
            "Epoch 7095/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8543\n",
            "Epoch 7096/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.8488\n",
            "Epoch 7097/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8440\n",
            "Epoch 7098/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8419\n",
            "Epoch 7099/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8406\n",
            "Epoch 7100/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8353\n",
            "Epoch 7101/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8314\n",
            "Epoch 7102/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8290\n",
            "Epoch 7103/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8272\n",
            "Epoch 7104/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8248\n",
            "Epoch 7105/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8177\n",
            "Epoch 7106/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8168\n",
            "Epoch 7107/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8132\n",
            "Epoch 7108/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8086\n",
            "Epoch 7109/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8050\n",
            "Epoch 7110/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7980\n",
            "Epoch 7111/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7929\n",
            "Epoch 7112/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7910\n",
            "Epoch 7113/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7834\n",
            "Epoch 7114/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7767\n",
            "Epoch 7115/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7750\n",
            "Epoch 7116/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7685\n",
            "Epoch 7117/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7657\n",
            "Epoch 7118/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7604\n",
            "Epoch 7119/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.7570\n",
            "Epoch 7120/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7484\n",
            "Epoch 7121/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7469\n",
            "Epoch 7122/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7381\n",
            "Epoch 7123/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7364\n",
            "Epoch 7124/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.7314\n",
            "Epoch 7125/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7298\n",
            "Epoch 7126/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.7243\n",
            "Epoch 7127/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.7183\n",
            "Epoch 7128/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.7148\n",
            "Epoch 7129/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7105\n",
            "Epoch 7130/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7073\n",
            "Epoch 7131/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7035\n",
            "Epoch 7132/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6989\n",
            "Epoch 7133/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6938\n",
            "Epoch 7134/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6886\n",
            "Epoch 7135/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6885\n",
            "Epoch 7136/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6864\n",
            "Epoch 7137/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6800\n",
            "Epoch 7138/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6785\n",
            "Epoch 7139/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6756\n",
            "Epoch 7140/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6686\n",
            "Epoch 7141/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6677\n",
            "Epoch 7142/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6660\n",
            "Epoch 7143/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6629\n",
            "Epoch 7144/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6587\n",
            "Epoch 7145/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6556\n",
            "Epoch 7146/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6555\n",
            "Epoch 7147/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6507\n",
            "Epoch 7148/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6492\n",
            "Epoch 7149/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6439\n",
            "Epoch 7150/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6405\n",
            "Epoch 7151/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6390\n",
            "Epoch 7152/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6376\n",
            "Epoch 7153/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6365\n",
            "Epoch 7154/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6279\n",
            "Epoch 7155/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6290\n",
            "Epoch 7156/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6256\n",
            "Epoch 7157/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6253\n",
            "Epoch 7158/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6255\n",
            "Epoch 7159/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6184\n",
            "Epoch 7160/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6197\n",
            "Epoch 7161/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6150\n",
            "Epoch 7162/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6150\n",
            "Epoch 7163/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6137\n",
            "Epoch 7164/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6122\n",
            "Epoch 7165/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6050\n",
            "Epoch 7166/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6042\n",
            "Epoch 7167/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6044\n",
            "Epoch 7168/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6022\n",
            "Epoch 7169/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.5982\n",
            "Epoch 7170/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5991\n",
            "Epoch 7171/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5963\n",
            "Epoch 7172/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5957\n",
            "Epoch 7173/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5924\n",
            "Epoch 7174/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5906\n",
            "Epoch 7175/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5886\n",
            "Epoch 7176/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5915\n",
            "Epoch 7177/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.5851\n",
            "Epoch 7178/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5863\n",
            "Epoch 7179/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5829\n",
            "Epoch 7180/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5799\n",
            "Epoch 7181/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5818\n",
            "Epoch 7182/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5790\n",
            "Epoch 7183/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5750\n",
            "Epoch 7184/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5743\n",
            "Epoch 7185/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5707\n",
            "Epoch 7186/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5695\n",
            "Epoch 7187/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5686\n",
            "Epoch 7188/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5679\n",
            "Epoch 7189/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5654\n",
            "Epoch 7190/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5657\n",
            "Epoch 7191/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5588\n",
            "Epoch 7192/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5609\n",
            "Epoch 7193/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5632\n",
            "Epoch 7194/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5594\n",
            "Epoch 7195/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5593\n",
            "Epoch 7196/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5584\n",
            "Epoch 7197/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5561\n",
            "Epoch 7198/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5551\n",
            "Epoch 7199/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5526\n",
            "Epoch 7200/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.5502\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-7199.h5\n",
            "prompt: ting that as he found himself obliged\n",
            "to go to London on the morrow for a few days, he could not help trying\n",
            "to procure a companion; and therefore hoped that if William could make\n",
            "up his mind to leave Mansfield half a day earlier than had been\n",
            "proposed, he would accept a place in hi\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:#eadbd8;\">t anything a</span>dded<br><span style=\"background-color:#d4efdf;\">he always be </span>farth<span style=\"background-color:#d6dbdf;\">inion. Her c</span>huld,<br>were welenrly fins no defieldmixiouss of exionne<span style=\"background-color:#eadbd8;\">e—but she </span>know,<span style=\"background-color:#d0ece7;\"> means any</span><span style=\"background-color:#e2d7d5;\">thing so di</span><span style=\"background-color:#eadbd8;\">nance and a</span><span style=\"background-color:#d4e6f1;\">gree that s</span>tate "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 884ms/step - loss: 0.5500\n",
            "Epoch 7201/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5491\n",
            "Epoch 7202/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5481\n",
            "Epoch 7203/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5451\n",
            "Epoch 7204/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5439\n",
            "Epoch 7205/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5426\n",
            "Epoch 7206/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5432\n",
            "Epoch 7207/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5385\n",
            "Epoch 7208/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5430\n",
            "Epoch 7209/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5426\n",
            "Epoch 7210/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5408\n",
            "Epoch 7211/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5347\n",
            "Epoch 7212/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5381\n",
            "Epoch 7213/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5344\n",
            "Epoch 7214/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5305\n",
            "Epoch 7215/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5344\n",
            "Epoch 7216/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5341\n",
            "Epoch 7217/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5321\n",
            "Epoch 7218/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5308\n",
            "Epoch 7219/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5304\n",
            "Epoch 7220/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5285\n",
            "Epoch 7221/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5282\n",
            "Epoch 7222/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5286\n",
            "Epoch 7223/500000\n",
            "23/28 [=======================>......] - ETA: 0s - loss: 0.5285"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    steps_per_epoch=restricted_batches//params['batch_size']\n",
        "    if steps_per_epoch < 1:\n",
        "        steps_per_epoch = 1\n",
        "    history = model.fit(dataset, epochs=EPOCHS, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, callbacks=[service_callback]) # for TPU we need to role our own checkpointer since we need to transfer the weights\n",
        "else:\n",
        "    history = model.fit(dataset, validation_data=validation_dataset, epochs=EPOCHS, initial_epoch=initial_epoch, callbacks=[checkpoint_callback, tensorboard_callback, service_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a81LdPyY2dyo"
      },
      "outputs": [],
      "source": [
        "model_cpu.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDNYZiEQtgF"
      },
      "outputs": [],
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "def doDialog(model):\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "    print(\"Please enter some dialog.\")\n",
        "    print(\"The net will answer according to your input.\")\n",
        "    print(\"'bye' for end,\")\n",
        "    print(\"'reset' to reset the conversation context,\")\n",
        "    print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "    print(\"    to change character of the dialog.\")\n",
        "    print(\"    Current temperature={}.\".format(temperature))\n",
        "    print()\n",
        "    xso = None\n",
        "    bye = False\n",
        "    doini = True\n",
        "    bye = False\n",
        "    while not bye:\n",
        "        print(\"> \", end=\"\")\n",
        "        prompt = input()\n",
        "        if prompt == 'bye':\n",
        "            bye = True\n",
        "            print(\"Good bye!\")\n",
        "            continue\n",
        "        if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "            t = float(prompt[len(\"temperature=\"):])\n",
        "            if t > 0.05 and t < 1.4:\n",
        "                temperature = t\n",
        "                print(\"(generator temperature now {})\".format(t))\n",
        "                print()\n",
        "                continue\n",
        "            print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "            continue\n",
        "        reply=mhsa_generate(model, prompt, gen_len=256, temperature=temperature, verbose=True)\n",
        "        td.source_highlight(reply, min_quote_size=13, dark_mode=use_dark_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JEPK2WIQtgI"
      },
      "outputs": [],
      "source": [
        "# Talk to the net!\n",
        "doDialog(model_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMCWf5AZn1-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "VmWbteSFQtfq",
        "yWE_ZZMKEARV"
      ],
      "machine_shape": "hm",
      "name": "transformer_poet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}