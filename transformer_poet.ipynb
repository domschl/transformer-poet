{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/transformer-poet/blob/main/transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Transformer-Poet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabS0VZ-1Zp0"
      },
      "source": [
        "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jtpy59Yq-Qfz",
        "outputId": "aab85854-0f48-4a64-ecce-a1d487d23c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml-indie-tools\n",
            "  Downloading ml_indie_tools-0.1.2-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: ml-indie-tools\n",
            "Successfully installed ml-indie-tools-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U5T4m6earb1e"
      },
      "outputs": [],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
        "\n",
        "from ml_indie_tools.keras_custom_layers import MultiHeadSelfAttention, PositionalEncoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A tensorflow deep multi-head attention model for text generation\n",
        "\n",
        "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
        "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "llPw84PkEAP2",
        "outputId": "52ae8204-91a2-46f9-8ff8-8c9d254f44a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OS: Linux, Python: 3.7.12, Colab Jupyter Notebook Tensorflow: 2.8.0, GPU: Tesla T4 (3MiB / 15109MiB)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oZ6t9b6ZwSxi"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    tpu_strategy = ml_env.tpu_strategy\n",
        "    tpu_is_init=True\n",
        "    use_eager=False\n",
        "else:\n",
        "    use_eager=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t-TP3Pnsrb1f",
        "outputId": "2d46ecc4-24a0-4de6-8a54-5677f20f388c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Root path (all projects) : /content/drive/My Drive (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
            "Project path             : /content/drive/My Drive/Colab Notebooks/women_writers (Changes to the file system happen only below this project path\n",
            "Model path (snapshots)   : /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf (Model weights and snapshots are stored here)\n",
            "Data path (training data): /content/drive/My Drive/Colab Notebooks/women_writers/data (Training data will be downloaded here)\n",
            "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
          ]
        }
      ],
      "source": [
        "project_name='women_writers'\n",
        "model_name='mhsa_v1_tf'\n",
        "\n",
        "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
        "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
        "\n",
        "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
        "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
        "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
        "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
        "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
        "encoding, batch generation, and formatted source display. It read some \n",
        "books from Project Gutenberg and supports creation of training batches. \n",
        "The output functions support highlighting to allow to compare generated \n",
        "texts with the actual sources to help to identify identical (memorized) \n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "use_dark_mode=True # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C66X7ynnrb1h",
        "outputId": "e2fc71e5-8854-4326-9674-3ebb30aa4634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
          ]
        }
      ],
      "source": [
        "# sample searches\n",
        "search_spec= {\"author\": [\"Emily Brontë\",\"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
        "\n",
        "book_list=gd.search(search_spec)\n",
        "book_cnt = len(book_list)\n",
        "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "if book_cnt<40:\n",
        "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
        "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
        "else:\n",
        "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MH6_7IU3upOd",
        "outputId": "4480b901-d1f5-45f1-f5d7-c29bbb6e19ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: The Common Reader - Virginia Woolf, 64457\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
            "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
            "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
            "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
            "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
            "6: Pride and Prejudice - Jane Austen, 42671\n",
            "7: The Letters of Jane Austen - Jane Austen, 42078\n",
            "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
            "9: Jacob's Room - Virginia Woolf, 5670\n",
            "10: Pride and Prejudice - Jane Austen, 1342\n",
            "11: Night and Day - Virginia Woolf, 1245\n",
            "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
            "13: Lady Susan - Jane Austen, 946\n",
            "14: Wuthering Heights - Emily Brontë, 768\n",
            "15: Sense and Sensibility - Jane Austen, 161\n",
            "16: Emma - Jane Austen, 158\n",
            "17: The Voyage Out - Virginia Woolf, 144\n",
            "18: Mansfield Park - Jane Austen, 141\n",
            "19: Northanger Abbey - Jane Austen, 121\n",
            "20: Persuasion - Jane Austen, 105\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(book_list)):\n",
        "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2jBH3Z15rb1h",
        "outputId": "38e9ccc9-1b90-4305-9d93-d3c1e07f81f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Datasets:Loaded 6 texts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using:\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
            "2: Pride and Prejudice - Jane Austen\n",
            "3: Lady Susan - Jane Austen\n",
            "4: Wuthering Heights - Emily Brontë\n",
            "5: Emma - Jane Austen\n",
            "6: The Voyage Out - Virginia Woolf\n"
          ]
        }
      ],
      "source": [
        "select = (14,1,6,13,16,17)\n",
        "sub_book_list = [book_list[i] for i in range(len(book_list)) if i in select]\n",
        "\n",
        "print(\"Using:\")\n",
        "for i in range(len(sub_book_list)):\n",
        "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
        "\n",
        "td = Text_Dataset(sub_book_list)\n",
        "td.init_tokenizer(tokenizer='char')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f7_tc2Lirb1i",
        "outputId": "5d53d0f7-586c-4ac3-a065-7859fca16561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3153130 records\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LEN = 96\n",
        "SUB_PROBABILITY = 0.15  # like BERT\n",
        "\n",
        "td.init_getitem(sample_type='chargen_single_encoded', sample_length=SEQUENCE_LEN, content_stepping=1)\n",
        "\n",
        "num_records = len(td)\n",
        "\n",
        "print(f\"{num_records} records\")\n",
        "\n",
        "def get_sample_batch(td, batch_size, length, random_index=True, SUB_probability=0.0):\n",
        "    for i in range(batch_size):\n",
        "        if random_index is True:\n",
        "            ind = random.randint(0, num_records-1)\n",
        "        else:\n",
        "            ind = i * td.getitem_content_stepping\n",
        "        Xi = td[ind]\n",
        "        yi = [Xi[-1]]\n",
        "        if SUB_probability==0.0:\n",
        "            Xi[-1]=td.c2i['␚']  # use 'SUB'-stitut glyph to mark last char of input\n",
        "        else:\n",
        "            l=int(len(Xi)*SUB_probability)\n",
        "            for li in range(l):\n",
        "                pos=random.randint(0,len(Xi)-1)\n",
        "                Xi[pos]=td.c2i['␚']\n",
        "        if i==0:\n",
        "            smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)\n",
        "\n",
        "def get_random_onehot_sample_batch(td, batch_size, length):\n",
        "    X, y = get_random_sample_batch(td, batch_size, length)\n",
        "    xoh = tf.keras.backend.one_hot(X, len(td.i2c))\n",
        "    yk = tf.keras.backend.constant(y)\n",
        "    return xoh, yk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TI3Fx6bNuR9A",
        "outputId": "0f17a917-3353-4547-84e9-ca782bf7fc39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]: X=>␚efore,␚or an\n",
            "␚our b␚␚ore, its␚inte␚est soon␚increa␚ed; ␚nd before their fi␚st␚conv␚rsation was <, y=> <\n",
            "[1]: X=>ed by a␚s␚rt of fa␚c␚n␚tion, it would be ri␚ic␚lous in␚me to repeat\n",
            "th␚ instances ␚f gre␚t ␚␚s␚o<, y=>o<\n"
          ]
        }
      ],
      "source": [
        "test_x, test_y = get_sample_batch(td, 2, 40, random_index=True, SUB_probability=SUB_PROBABILITY)\n",
        "for i in range(len(test_x)):\n",
        "    print(f\"[{i}]: X=>{td.decode(test_x[i])}<, y=>{td.decode(test_y[i])}<\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jn_LcJ6g9Mzy"
      },
      "outputs": [],
      "source": [
        "def expand_name_template(template, params):\n",
        "    exp=copy.copy(template)\n",
        "    for key in params:\n",
        "        src=\"{\"+key+\"}\"\n",
        "        dst=f\"{params[key]}\"\n",
        "        exp=exp.replace(src,dst)\n",
        "    return exp\n",
        "\n",
        "def save_model_metadata(epoch, suffix='std'):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    params['current_epoch'] = epoch\n",
        "    try:\n",
        "        with open(meta_file, 'w') as f:\n",
        "            f.write(json.dumps(params))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def read_model_metadata(suffix=\"std\"):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    try:\n",
        "        with open(meta_file, 'r') as f:\n",
        "            meta = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
        "        return None\n",
        "    return meta\n",
        "\n",
        "def is_metadata_compatible(params, meta):\n",
        "    is_valid=True\n",
        "    keys=set(list(params.keys())+list(meta.keys()))\n",
        "    for key in keys:\n",
        "        if key in updatable_keys:\n",
        "            continue\n",
        "        if key not in meta:\n",
        "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif key not in params:\n",
        "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif meta[key]!=params[key]:\n",
        "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "    if is_valid is False:\n",
        "        print(\"Aborting import.\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "znpIUA3ig3gO",
        "outputId": "a5a2ae26-841f-4db4-e660-72b982d6579b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot access project meta-data at /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/model_meta_6x4x192x132.json: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/model_meta_6x4x192x132.json', starting anew.\n",
            "Starting new model\n",
            "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 6, 'heads': 4, 'units': 192, 'norm': 'softmax', 'mh_normalize': True, 'l2_regularizer': 1e-09, 'dropout': 0.9, 'join_heads_by_add': False, 'vocab_size': 132, 'sequence_len': 96, 'batch_size': 128, 'learning_rate': 0.001, 'clipvalue': None, 'sample_every_n_epochs': 5}\n"
          ]
        }
      ],
      "source": [
        "params = { # Multi-head self-attention\n",
        "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
        "\n",
        "    'mhsa_layers': 6,\n",
        "    'heads': 4,\n",
        "    'units': 192, # len(td.i2c),\n",
        "    'norm': 'softmax', # this is for within each head\n",
        "    'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
        "    'l2_regularizer': 1e-9,\n",
        "    'dropout': 0.9,       # no dropout: 0.0\n",
        "    'join_heads_by_add': False,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads\n",
        "    'vocab_size': len(td.i2c),\n",
        "    'sequence_len': SEQUENCE_LEN,\n",
        "\n",
        "    'batch_size': 128,\n",
        "    'learning_rate': 0.001,\n",
        "    'clipvalue': None,\n",
        "    'sample_every_n_epochs': 5,\n",
        "}\n",
        "\n",
        "if ml_env.is_tpu is True:\n",
        "    lr = params['learning_rate']*1.0\n",
        "else:\n",
        "    lr = params['learning_rate']\n",
        "\n",
        "model_suffix = expand_name_template(params['name'], params)\n",
        "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
        "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
        "\n",
        "# When comparing if training-data is compatible with new params set, \n",
        "# the following keys are updatable, they can be changed while continuing\n",
        "# to use existing checkpoints and continue training with those values\n",
        "# changed:\n",
        "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
        "             'sample_every_n_epochs']\n",
        "\n",
        "# These values are taking from saved checkpoint:\n",
        "keep_keys=['current_epoch']\n",
        "\n",
        "continue_last = True\n",
        "\n",
        "meta = read_model_metadata(suffix=model_suffix)\n",
        "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
        "    for key in keep_keys:\n",
        "        if key in meta:\n",
        "            params[key]=meta[key]\n",
        "    if params is not None:\n",
        "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
        "    else:\n",
        "        print(f\"No previous data, starting new model\")\n",
        "else:\n",
        "    print(\"Starting new model\")\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jY3hUuhQYzdT",
        "outputId": "0b42fe44-a0de-4362-bb08-209bed63b4f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches = 24633\n"
          ]
        }
      ],
      "source": [
        "num_batches = num_records // params['batch_size']\n",
        "print(f\"num_batches = {num_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EeB7jugCV4lI"
      },
      "outputs": [],
      "source": [
        "# @tf.function\n",
        "def make_tf_dataset(num, random_index=False, SUB_probability=0.0):\n",
        "    dx=[]\n",
        "    dy=[]\n",
        "    num_batches_active = num\n",
        "    for i in range(num_batches_active):\n",
        "        x,y=get_sample_batch(td, params['batch_size'], params['sequence_len'], random_index=random_index, SUB_probability=SUB_probability)\n",
        "        if i<1:\n",
        "            print(f\"[{num} x]: {x.shape} -> {y.shape}\")\n",
        "        dx.append(x)\n",
        "        dy.append(y)\n",
        "    dx=np.array(dx)\n",
        "    dy=np.array(dy)\n",
        "    data_xy = (dx, dy)\n",
        "    tf_dataset=tf.data.Dataset.from_tensor_slices(data_xy)\n",
        "    return tf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DCy7WmQyS9T-",
        "outputId": "ddc9c9b3-cd31-4997-c1b5-74d83276faa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24633 x]: (128, 96) -> (128, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e1d5cd25053c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrestricted_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtextlib_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestricted_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSUB_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSUB_PROBABILITY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{restricted_batches} records, {dt} sec, {dt/restricted_batches} sec/record\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dt' is not defined"
          ]
        }
      ],
      "source": [
        "MAX_NUM_BATCHES = 40000\n",
        "\n",
        "if num_batches>MAX_NUM_BATCHES:\n",
        "    restricted_batches=MAX_NUM_BATCHES\n",
        "else:\n",
        "    restricted_batches=num_batches\n",
        "textlib_dataset = make_tf_dataset(restricted_batches, random_index=True, SUB_probability=SUB_PROBABILITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "boow8wR7sLwi",
        "outputId": "26481e51-281d-44a5-b7d6-9d9889e64a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(128, 96), dtype=tf.float32, name=None), TensorSpec(shape=(128, 1), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "shuffle_buffer=10000\n",
        "if ml_env.is_tpu is True:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer).repeat()  # Otherwise TPU may run dry\n",
        "else:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
        "dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B-G5HLMqqbeT",
        "outputId": "a9d5fb43-ccf8-4a51-ccd5-d9d595d34fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 x]: (128, 96) -> (128, 1)\n"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    validation_dataset = make_tf_dataset(10, random_index=True, SUB_probability=SUB_PROBABILITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZAzFlCVBiL0Q"
      },
      "outputs": [],
      "source": [
        "def model_mhsa(inputs, params):\n",
        "    dense = layers.Dense(params['vocab_size'], kernel_regularizer=regularizers.l2(params['l2_regularizer']))  # using softmax here prevents temperature adjust, affects 'from_logits' param in sparse_categorical loss \n",
        "    fl = layers.Flatten()\n",
        "    dr = layers.Dropout(params['dropout'])\n",
        "    pe = PositionalEncoding(amplitude=0.3)\n",
        "    mhsa=[]\n",
        "    for i in range(params['mhsa_layers']):\n",
        "        mhsa.append(MultiHeadSelfAttention(params['heads'], units=params['units'], norm=params['norm'], mh_normalize=params['mh_normalize'], join_heads_by_add=params['join_heads_by_add']))\n",
        "    x = tf.one_hot(tf.cast(inputs,dtype=tf.int32), params['vocab_size'], axis=-1)\n",
        "    x = pe(x)\n",
        "    for i in range(params['mhsa_layers']):\n",
        "        x = mhsa[i](x)\n",
        "        # x = mhsa[i](x,x)\n",
        "    if params['dropout']>0.0:\n",
        "        x = dr(x)\n",
        "    x = dense(fl(x))\n",
        "    return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J13Gp_hjqqn"
      },
      "outputs": [],
      "source": [
        "def mhsa_generate(text, gen_len=64, temperature=0.9, verbose=False):\n",
        "    if verbose is True:\n",
        "        full=text[:-1]\n",
        "    gen_text=\"\"\n",
        "    lf=0\n",
        "    if verbose is True:\n",
        "        print(f\"[{full}]\", end='')\n",
        "    tex=copy.copy(text)\n",
        "    while len(tex) < params['sequence_len']:\n",
        "        tex=' '+tex\n",
        "    tex=tex[-params['sequence_len']+1:]\n",
        "    tex=tex+'␚'\n",
        "    for i in range(gen_len):\n",
        "        input = np.array([td.encode(tex)])\n",
        "        pred = model.predict(input, batch_size=1)\n",
        "        pred /= temperature\n",
        "        pred = tf.keras.layers.Softmax()(pred)\n",
        "        if use_eager is True:\n",
        "            pred=pred.numpy()\n",
        "        else:\n",
        "            pred=tf.keras.backend.eval(pred)\n",
        "        ci=np.random.choice(list(range(len(pred[0]))), p=pred[0]) # np.argmax(pred[0])\n",
        "        c=td.i2c[ci]\n",
        "        if verbose is True:\n",
        "            print(c, end='')\n",
        "            if c=='\\n':\n",
        "                lf=0\n",
        "            else:\n",
        "                lf += 1\n",
        "                if (lf>80 and c==' ') or lf>120:\n",
        "                    print()\n",
        "                    lf=0\n",
        "            full+=c\n",
        "        gen_text+=c\n",
        "        tex=tex[:-1]+c+'␚'\n",
        "        tex=tex[-params['sequence_len']:]\n",
        "    if verbose is True:\n",
        "        print()\n",
        "    return gen_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf-NHZ326NqJ"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "outputs = model_mhsa(inputs, params)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXx-nBe5-jyJ"
      },
      "outputs": [],
      "source": [
        "def import_previous_compatible_checkpoint(force_import=False):\n",
        "    meta = read_model_metadata(suffix=model_suffix)\n",
        "    if meta is None:\n",
        "        print(\"No previous checkpoint found\")\n",
        "        return False\n",
        "    if is_metadata_compatible(params, meta) is not True and force_import is False:\n",
        "        print(\"No useable import found.\")\n",
        "        return False\n",
        "    try:\n",
        "        last_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot determine last checkpoint in {checkpoint_dir}, cannot import due to: {e}\")\n",
        "        return False\n",
        "    print(f\"Last checkpoint: {last_checkpoint}\")\n",
        "    try:\n",
        "        model.load_weights(last_checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to import model {last_checkpoint}: {e}\")\n",
        "        return False\n",
        "    if 'current_epoch' in meta:\n",
        "        params['current_epoch'] = meta['current_epoch']\n",
        "    print(f\"Successful import of epoch {params['current_epoch']} from {last_checkpoint}, continuing from there...\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkCGZtS4-jyK"
      },
      "outputs": [],
      "source": [
        "import_checkpoint = False\n",
        "force_import = False   # True: ignore metadata and try import anyway. This will of course crash, if the new model doesn't fit the checkpoint-data...\n",
        "\n",
        "if import_checkpoint is True:\n",
        "    import_previous_compatible_checkpoint(force_import=force_import)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB-Q8YXvndE"
      },
      "source": [
        "### Loss function, optimizer, tensorboard output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t5JWEdYZNGz"
      },
      "outputs": [],
      "source": [
        "kscc = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss(labels, logits):\n",
        "  vl=kscc(labels, logits)\n",
        "  return vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc2kbGoAZXHi"
      },
      "outputs": [],
      "source": [
        "if params['clipvalue'] is not None:\n",
        "    opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "else:\n",
        "    opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "if ml_env.is_tpu is True:\n",
        "    model.compile(optimizer=opti, loss=loss, metrics=[])\n",
        "else:\n",
        "    model.compile(optimizer=opti, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vxZF0wOEAQr"
      },
      "outputs": [],
      "source": [
        " model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHZurM5ei95K"
      },
      "outputs": [],
      "source": [
        "TPU_GENERATE_ON_CPU = False  # The thing is: both options are slow on TPU :-/\n",
        "\n",
        "class GeneratorCallback(keras.callbacks.Callback):\n",
        "#    def on_test_end(self, logs=None):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_model_metadata(epoch, suffix=model_suffix)\n",
        "        if (epoch+1) % params['sample_every_n_epochs'] == 0:\n",
        "            idx=random.randint(0,len(td)-1)\n",
        "            text=td.decode(td[idx])\n",
        "            print()\n",
        "            if ml_env.is_tpu is True:\n",
        "                temp_list=[0.7]\n",
        "                gen_len=64\n",
        "            else:\n",
        "                temp_list=[0.5, 0.7, 0.9]\n",
        "                gen_len=192\n",
        "            print(f\"prompt: {text}\")\n",
        "            for temp in temp_list:\n",
        "                print(f\"---------------- T={temp} ---------------\")\n",
        "                if ml_env.is_tpu is True and TPU_GENERATE_ON_CPU is True:\n",
        "                    with tf.device('/cpu:0'):\n",
        "                        reply=mhsa_generate(text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                else:\n",
        "                    reply=mhsa_generate(text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                td.source_highlight(reply, min_quote_size=10, dark_mode=use_dark_mode, display_ref_anchor=False)\n",
        "            print(\"--------------------------------------\")\n",
        "\n",
        "generator_callback=GeneratorCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SKvObcsEAQ5"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch', histogram_freq=1) # update_freq='epoch', "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Ew6pgWzeFj"
      },
      "outputs": [],
      "source": [
        "# This sometimes completely freezes?! Simply skip this cells, if it seems\n",
        "# to block. Bug.\n",
        "# if ml_env.is_colab:\n",
        "#     try:\n",
        "#         # use the python variable log_path:\n",
        "#         get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
        "#     except:\n",
        "#         pass\n",
        "\n",
        "# The following throws errors on non-colab, but the guarding above is too bug-ridden.\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFbZcN0vxOB"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh2yUKBoEAQ8"
      },
      "outputs": [],
      "source": [
        "EPOCHS=1000\n",
        "if 'current_epoch' in params:\n",
        "    initial_epoch=params['current_epoch']\n",
        "else:\n",
        "    initial_epoch=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    steps_per_epoch=restricted_batches//params['batch_size']\n",
        "    history = model.fit(dataset, epochs=EPOCHS, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback, tensorboard_callback, generator_callback])\n",
        "else:\n",
        "    history = model.fit(dataset, validation_data=validation_dataset, epochs=EPOCHS, initial_epoch=initial_epoch, callbacks=[checkpoint_callback, tensorboard_callback, generator_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDNYZiEQtgF"
      },
      "outputs": [],
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "def doDialog():\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "    print(\"Please enter some dialog.\")\n",
        "    print(\"The net will answer according to your input.\")\n",
        "    print(\"'bye' for end,\")\n",
        "    print(\"'reset' to reset the conversation context,\")\n",
        "    print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "    print(\"    to change character of the dialog.\")\n",
        "    print(\"    Current temperature={}.\".format(temperature))\n",
        "    print()\n",
        "    xso = None\n",
        "    bye = False\n",
        "    doini = True\n",
        "    bye = False\n",
        "    while not bye:\n",
        "        print(\"> \", end=\"\")\n",
        "        prompt = input()\n",
        "        if prompt == 'bye':\n",
        "            bye = True\n",
        "            print(\"Good bye!\")\n",
        "            continue\n",
        "        if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "            t = float(prompt[len(\"temperature=\"):])\n",
        "            if t > 0.05 and t < 1.4:\n",
        "                temperature = t\n",
        "                print(\"(generator temperature now {})\".format(t))\n",
        "                print()\n",
        "                continue\n",
        "            print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "            continue\n",
        "        reply=mhsa_generate(prompt, gen_len=256, temperature=temperature, verbose=False)\n",
        "        td.source_highlight(reply, min_quote_size=13, dark_mode=use_dark_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JEPK2WIQtgI"
      },
      "outputs": [],
      "source": [
        "# Talk to the net!\n",
        "doDialog()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50m_8R9qrb1n"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VmWbteSFQtfq",
        "yWE_ZZMKEARV"
      ],
      "machine_shape": "hm",
      "name": "transformer_poet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}