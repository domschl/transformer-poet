{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/transformer-poet/blob/main/transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Transformer-Poet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabS0VZ-1Zp0"
      },
      "source": [
        "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jtpy59Yq-Qfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71921409-8405-4e39-a4db-3e0b857a7b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ml-indie-tools\n",
            "  Downloading ml_indie_tools-0.3.8-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: ml-indie-tools\n",
            "Successfully installed ml-indie-tools-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U5T4m6earb1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3c45df-3b49-4d38-ecae-45d9e3dc1104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TF-Keras version: 2.9.0\n"
          ]
        }
      ],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
        "\n",
        "from ml_indie_tools.keras_custom_layers import MultiHeadSelfAttention, PositionalEncoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A tensorflow deep multi-head attention model for text generation\n",
        "\n",
        "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
        "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6c66c30c-56ce-4aa9-edf6-8c84a221c727"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OS: Linux, Python: 3.7.15, Colab Jupyter Notebook Tensorflow: 2.9.2, TPU: TPU, 8 nodes v2 (8GB)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
        "\n",
        "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oZ6t9b6ZwSxi"
      },
      "outputs": [],
      "source": [
        "use_eager=tf.executing_eagerly()\n",
        "if ml_env.is_tpu is True:\n",
        "    tpu_strategy = ml_env.tpu_strategy\n",
        "    tpu_is_init=True\n",
        "    if use_eager is True:\n",
        "        tf.config.run_functions_eagerly(False)\n",
        "    use_eager=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t-TP3Pnsrb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77acc2c0-31ba-4c0f-9e07-15d2694ebe30"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Root path (all projects) : /content/drive/My Drive (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
            "Project path             : /content/drive/My Drive/Colab Notebooks/women_writers (Changes to the file system happen only below this project path\n",
            "Model path (snapshots)   : /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf (Model weights and snapshots are stored here)\n",
            "Data path (training data): /content/drive/My Drive/Colab Notebooks/women_writers/data (Training data will be downloaded here)\n",
            "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
          ]
        }
      ],
      "source": [
        "project_name='women_writers'\n",
        "model_name='mhsa_v1_tf'\n",
        "\n",
        "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
        "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
        "#\n",
        "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
        "\n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
        "\n",
        "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
        "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
        "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
        "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
        "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
        "encoding, batch generation, and formatted source display. It read some \n",
        "books from Project Gutenberg and supports creation of training batches. \n",
        "The output functions support highlighting to allow to compare generated \n",
        "texts with the actual sources to help to identify identical (memorized) \n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C66X7ynnrb1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d4fbac-43eb-4a2d-9153-0131e8fdf9fc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
          ]
        }
      ],
      "source": [
        "# sample searches\n",
        "search_spec= {\"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
        "\n",
        "book_list=gd.search(search_spec)\n",
        "book_cnt = len(book_list)\n",
        "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "if book_cnt<40:\n",
        "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
        "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
        "else:\n",
        "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MH6_7IU3upOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11743c65-59fc-4835-9385-036594b52b9a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: The Common Reader - Virginia Woolf, 64457\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
            "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
            "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
            "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
            "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
            "6: Pride and Prejudice - Jane Austen, 42671\n",
            "7: The Letters of Jane Austen - Jane Austen, 42078\n",
            "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
            "9: Jacob's Room - Virginia Woolf, 5670\n",
            "10: Pride and Prejudice - Jane Austen, 1342\n",
            "11: Night and Day - Virginia Woolf, 1245\n",
            "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
            "13: Lady Susan - Jane Austen, 946\n",
            "14: Wuthering Heights - Emily Brontë, 768\n",
            "15: Sense and Sensibility - Jane Austen, 161\n",
            "16: Emma - Jane Austen, 158\n",
            "17: The Voyage Out - Virginia Woolf, 144\n",
            "18: Mansfield Park - Jane Austen, 141\n",
            "19: Northanger Abbey - Jane Austen, 121\n",
            "20: Persuasion - Jane Austen, 105\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(book_list)):\n",
        "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2jBH3Z15rb1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d685025e-4030-453f-c873-66f3f0f58071"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using:\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
            "2: Jacob's Room - Virginia Woolf\n",
            "3: Pride and Prejudice - Jane Austen\n",
            "4: Night and Day - Virginia Woolf\n",
            "5: Lady Susan - Jane Austen\n",
            "6: Wuthering Heights - Emily Brontë\n",
            "7: Sense and Sensibility - Jane Austen\n",
            "8: Emma - Jane Austen\n",
            "9: The Voyage Out - Virginia Woolf\n",
            "10: Mansfield Park - Jane Austen\n",
            "11: Northanger Abbey - Jane Austen\n"
          ]
        }
      ],
      "source": [
        "select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
        "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
        "\n",
        "print(\"Using:\")\n",
        "for i in range(len(sub_book_list)):\n",
        "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
        "\n",
        "textlib_dataset = None  # Forces re-caching\n",
        "td = Text_Dataset(sub_book_list)\n",
        "td.init_tokenizer(tokenizer='ngram', max_ngrams=6, max_tokens=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f7_tc2Lirb1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5d1d51-70e4-4c06-863f-3498099984d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1854539 records\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LEN = 80\n",
        "SUB_PROBABILITY = 0.15  # like BERT\n",
        "\n",
        "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN, content_stepping=1)\n",
        "\n",
        "num_records = len(td)\n",
        "\n",
        "print(f\"{num_records} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zZbbsNm0cOeW"
      },
      "outputs": [],
      "source": [
        "def get_sample_batch(td, batch_size, length, SUB_probability=0.15):\n",
        "    for i in range(batch_size):\n",
        "        Xi = td.get_random_item()\n",
        "        yi = Xi.copy()\n",
        "        l=int(len(Xi)*SUB_probability)\n",
        "        for li in range(l):\n",
        "            pos=random.randint(0,len(Xi)-1)\n",
        "            if td.tokenizer_type=='char':\n",
        "                Xi[pos]=td.c2i['␚']\n",
        "            elif td.tokenizer_type=='word':\n",
        "                Xi[pos]=td.w2i['<subst>']\n",
        "            elif td.tokenizer_type=='ngram':\n",
        "                Xi[pos]=td.t2i['<subst>']\n",
        "            else:\n",
        "                print(f\"Unexpected tokenizer_type {td.tokenizer_type}\")\n",
        "        if i==0:\n",
        "            # smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpX=np.array(Xi, dtype=np.int32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TI3Fx6bNuR9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47fc754d-069d-467b-c109-6cbd8d9e8aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0](l=80): X=>both\n",
            "lighthouse and bird; he was steadfast and brilli<subst><subst>at the same\n",
            "time he was whirled, with all other things, senseless <subst>st t<subst>gl<subst><subst>got up, left his <subst><subst>e of <subst>lver, and pressed on, with the\n",
            "wind against him. The <subst>age of the lighthouse and t<subst>orm full of\n",
            "birds <, y=>both\n",
            "lighthouse and bird; he was steadfast and brilliant; and at the same\n",
            "time he was whirled, with all other things, senseless against the\n",
            "glass. He got up, left his tribute of silver, and pressed on, with the\n",
            "wind against him. The image of the lighthouse and the storm full of\n",
            "birds <\n",
            "[1](l=80): X=>uncle and aunt<subst>  <subst>he was in town; and why not to me<subst>If he fears me, why come\n",
            "      hi<subst>? If he<subst>longer cares for <subst>why silent? T<subst>ing,\n",
            "     <subst>as<subst>man! I will think no more about him.”\n",
            "\n",
            "  <subst>Her r<subst>olution was for a short time involuntarily <subst>pt by th<, y=>uncle and aunt,\n",
            "      when he was in town; and why not to me? If he fears me, why come\n",
            "      hither? If he no longer cares for me, why silent? Teasing,\n",
            "      teasing, man! I will think no more about him.”\n",
            "\n",
            "      Her resolution was for a short time involuntarily kept by th<\n"
          ]
        }
      ],
      "source": [
        "test_x, test_y = get_sample_batch(td, 2, 40, SUB_probability=SUB_PROBABILITY)\n",
        "for i in range(len(test_x)):\n",
        "    xi=[int(x) for x in test_x[i]]\n",
        "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<, y=>{td.decode(test_y[i])}<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qnMxRkkmcOeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a578ded7-64ba-4f5b-9a5f-c55c5182ad00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 80), (2, 80))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "test_x.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jn_LcJ6g9Mzy"
      },
      "outputs": [],
      "source": [
        "def expand_name_template(template, params):\n",
        "    exp=copy.copy(template)\n",
        "    for key in params:\n",
        "        src=\"{\"+key+\"}\"\n",
        "        dst=f\"{params[key]}\"\n",
        "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
        "    return exp\n",
        "\n",
        "def save_model_metadata(epoch, suffix='std'):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    params['current_epoch'] = epoch\n",
        "    try:\n",
        "        with open(meta_file, 'w') as f:\n",
        "            f.write(json.dumps(params))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def read_model_metadata(suffix=\"std\"):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    try:\n",
        "        with open(meta_file, 'r') as f:\n",
        "            meta = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
        "        return None\n",
        "    return meta\n",
        "\n",
        "def is_metadata_compatible(params, meta):\n",
        "    is_valid=True\n",
        "    keys=set(list(params.keys())+list(meta.keys()))\n",
        "    for key in keys:\n",
        "        if key in updatable_keys:\n",
        "            continue\n",
        "        if key not in meta:\n",
        "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif key not in params:\n",
        "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif meta[key]!=params[key]:\n",
        "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "    if is_valid is False:\n",
        "        print(\"Aborting import.\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "znpIUA3ig3gO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87aa508e-073c-4e92-aac0-7489491ad1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuing last session from epoch 6873\n",
            "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 6, 'heads': [6, 6, 6, 6, 6, 6], 'units': [512, 512, 512, 512, 512, 512], 'norm': 'softmax', 'mh_normalize': True, 'l2_regularizer': 1e-09, 'dropout': 0.0, 'join_heads_by_add': True, 'vocab_size': 5000, 'sequence_len': 80, 'embedding_size': 128, 'batch_size': 256, 'learning_rate': 0.0002, 'clipvalue': None, 'sample_every_n_epochs': 100, 'current_epoch': 6873}\n"
          ]
        }
      ],
      "source": [
        "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
        "\n",
        "lyrs = 6;\n",
        "\n",
        "params = { # Multi-head self-attention\n",
        "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
        "\n",
        "    'mhsa_layers': lyrs, \n",
        "    'heads': [6]*lyrs,\n",
        "    'units': [512]*lyrs,  # 0 inserts an LSTM for memory-states :-)\n",
        "    'norm': 'softmax', # this is for within each head\n",
        "    'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
        "    'l2_regularizer': 1e-9,\n",
        "    'dropout': 0.0,       # no dropout: 0.0\n",
        "    'join_heads_by_add': True,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads\n",
        "    'vocab_size': vocabulary_size,\n",
        "    'sequence_len': SEQUENCE_LEN,\n",
        "    'embedding_size': 128,\n",
        "\n",
        "    'batch_size': 256,\n",
        "    'learning_rate': 0.0002,\n",
        "    'clipvalue': None,\n",
        "    'sample_every_n_epochs': 100,\n",
        "}\n",
        "\n",
        "if len(params['heads'])!=params['mhsa_layers'] or len(params['units'])!=params['mhsa_layers']:\n",
        "    print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
        "    \n",
        "if ml_env.is_tpu is True:\n",
        "    lr = params['learning_rate']*1.0\n",
        "else:\n",
        "    lr = params['learning_rate']\n",
        "\n",
        "model_suffix = expand_name_template(params['name'], params)\n",
        "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
        "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
        "if os.path.exists(checkpoint_dir) is False:\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# When comparing if training-data is compatible with new params set, \n",
        "# the following keys are updatable, they can be changed while continuing\n",
        "# to use existing checkpoints and continue training with those values\n",
        "# changed:\n",
        "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
        "             'sample_every_n_epochs']\n",
        "\n",
        "# These values are taking from saved checkpoint:\n",
        "keep_keys=['current_epoch']\n",
        "\n",
        "continue_last = True\n",
        "if continue_last is False:\n",
        "    print(\"NOT continuing based on existing training! New start.\")\n",
        "\n",
        "meta = read_model_metadata(suffix=model_suffix)\n",
        "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
        "    for key in keep_keys:\n",
        "        if key in meta:\n",
        "            params[key]=meta[key]\n",
        "    if params is not None:\n",
        "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
        "    else:\n",
        "        print(f\"No previous data, starting new model\")\n",
        "else:\n",
        "    print(\"Starting new model\")\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jY3hUuhQYzdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0874160a-6d13-494e-94f0-3cc819ad2c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches = 7244\n"
          ]
        }
      ],
      "source": [
        "num_batches = num_records // params['batch_size']\n",
        "print(f\"num_batches = {num_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EeB7jugCV4lI"
      },
      "outputs": [],
      "source": [
        "# @tf.function   (only slows things down [considerably!])\n",
        "def make_tf_dataset(num, random_index=False, SUB_probability=0.0):\n",
        "    dx=[]\n",
        "    dy=[]\n",
        "    num_batches_active = num\n",
        "    for i in range(num_batches_active):\n",
        "        x,y=get_sample_batch(td, params['batch_size'], params['sequence_len'], SUB_probability=SUB_probability)\n",
        "        if i<1:\n",
        "            print(f\"[{num} x]: {x.shape} -> {y.shape}\")\n",
        "        dx.append(x)\n",
        "        dy.append(y)\n",
        "    dx=np.array(dx)\n",
        "    dy=np.array(dy)\n",
        "    print(f\"dx.shape={dx.shape}, dy.shape={dy.shape}\")\n",
        "    data_xy = (dx, dy)\n",
        "    tf_dataset=tf.data.Dataset.from_tensor_slices(data_xy)\n",
        "    return tf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DCy7WmQyS9T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f8efb4-62db-4589-e584-7258653356d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7244 batches\n",
            "Creating dataset, this is slow. Be patient...\n",
            "[7244 x]: (256, 80) -> (256, 80)\n",
            "dx.shape=(7244, 256, 80), dy.shape=(7244, 256, 80)\n",
            "Dataset done and cached.\n"
          ]
        }
      ],
      "source": [
        "MAX_NUM_BATCHES = 50000\n",
        "\n",
        "if num_batches>MAX_NUM_BATCHES:\n",
        "    restricted_batches=MAX_NUM_BATCHES\n",
        "    print(f\"Restrictinig {num_batches} to max of {restricted_batches}\")\n",
        "else:\n",
        "    restricted_batches=num_batches\n",
        "    print(f\"{restricted_batches} batches\")\n",
        "if cached_batch_data == restricted_batches and textlib_dataset is not None:\n",
        "    print(\"Reusing cached training-data\")\n",
        "else:\n",
        "    print(\"Creating dataset, this is slow. Be patient...\")\n",
        "    textlib_dataset = make_tf_dataset(restricted_batches, SUB_probability=SUB_PROBABILITY)\n",
        "    cached_batch_data = restricted_batches\n",
        "    print(\"Dataset done and cached.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "boow8wR7sLwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2040205d-8280-4a3d-a057-bcb3d706e74d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256, 80), dtype=tf.int32, name=None), TensorSpec(shape=(256, 80), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "shuffle_buffer=10000\n",
        "if ml_env.is_tpu is True:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer).repeat()  # Otherwise TPU may run dry\n",
        "else:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
        "dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "B-G5HLMqqbeT"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    validation_dataset = make_tf_dataset(10, random_index=True, SUB_probability=SUB_PROBABILITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZAzFlCVBiL0Q"
      },
      "outputs": [],
      "source": [
        "def model_mhsa(inputs, params):\n",
        "    dense = layers.Dense(params['vocab_size'], kernel_regularizer=regularizers.l2(params['l2_regularizer']))  # using softmax here prevents temperature adjust, affects 'from_logits' param in sparse_categorical loss \n",
        "    fl = layers.Flatten()\n",
        "    dr = layers.Dropout(params['dropout'])\n",
        "    pe = PositionalEncoding(amplitude=0.3)\n",
        "    rs_up = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    if 0 in params['units']:\n",
        "        lstm1 = layers.LSTM(units=vocabulary_size, return_sequences=True)\n",
        "    if vocabulary_size>=300:\n",
        "        emb=layers.Embedding(vocabulary_size,params['embedding_size'],input_length=params['sequence_len'])\n",
        "    rs_down = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    mhsa=[]\n",
        "    residuals=[]\n",
        "\n",
        "    for i in range(params['mhsa_layers']):\n",
        "        if params['units'][i]==0:\n",
        "            mhsa.append(None)\n",
        "            residuals.append(i)\n",
        "        else:\n",
        "            mhsa.append(MultiHeadSelfAttention(params['heads'][i], units=params['units'][i], norm=params['norm'], mh_normalize=params['mh_normalize'], join_heads_by_add=params['join_heads_by_add']))\n",
        "    xint = tf.cast(inputs,dtype=tf.int32)\n",
        "    if vocabulary_size<300:\n",
        "        x = tf.one_hot(xint, params['vocab_size'], axis=-1)\n",
        "    else:\n",
        "        x = emb(xint)\n",
        "    x = pe(x)\n",
        "    for i in range(len(mhsa)):\n",
        "        if i in residuals:\n",
        "            x = rs_down(lstm1(rs_up(x)))+x\n",
        "            print(f\"Residual at layer {i} added.\")\n",
        "        else:\n",
        "            x = mhsa[i](x)\n",
        "        # x = mhsa[i](x,x)\n",
        "    if params['dropout']>0.0:\n",
        "        x = dr(x)\n",
        "    # x = dense(fl(x))\n",
        "    x = dense(x)\n",
        "    return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4J13Gp_hjqqn"
      },
      "outputs": [],
      "source": [
        "def mhsa_generate(model, text, gen_len=64, temperature=0.9, argmax=False, verbose=False):\n",
        "    if verbose is True:\n",
        "        full=text[:-1]\n",
        "    gen_text=\"\"\n",
        "    lf=0\n",
        "    input = np.array(td.encode(text))\n",
        "    while len(input) < params['sequence_len']:\n",
        "        input = np.concatenate([td.encode('<pad>'),input])\n",
        "    for i in range(gen_len):\n",
        "        input = np.concatenate([input[1:],td.encode('<subst>')])\n",
        "        if len(input)!=params['sequence_len']:\n",
        "            print('assertion failure')\n",
        "            return None\n",
        "        pred = model(input)\n",
        "        pred /= temperature\n",
        "        pred = tf.keras.layers.Softmax()(pred)\n",
        "        if tf.executing_eagerly() is True and ml_env.is_tpu is False:\n",
        "            pred=pred.numpy()\n",
        "        else:\n",
        "            pred=tf.keras.backend.eval(pred)  # this is a cheat, it internaly used Numpy() too.\n",
        "        if argmax is True:\n",
        "            pred=np.argmax(pred[0],axis=1)\n",
        "        else:\n",
        "            pred = [np.random.choice(list(range(len(pred[0][-1]))), p=pred[0][-1])]\n",
        "        input = np.concatenate([input[1:],[pred[-1]]])\n",
        "        c = td.decode([pred[-1]])\n",
        "        if verbose is True:\n",
        "            print(c, end='')\n",
        "            if c=='\\n':\n",
        "                lf=0\n",
        "            else:\n",
        "                lf += 1\n",
        "                if (lf>80 and c==' ') or lf>120:\n",
        "                    print()\n",
        "                    lf=0\n",
        "            full+=c\n",
        "        gen_text+=c\n",
        "    if verbose is True:\n",
        "        print()\n",
        "    return gen_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nf-NHZ326NqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2459f99-8f6f-4cb3-92b6-71cf4f50ba78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TPU-scope model\n",
            "Creating Default-scope model\n"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        print(\"Creating TPU-scope model\")\n",
        "        inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "        outputs = model_mhsa(inputs, params)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    print(\"Creating Default-scope model\")\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model_cpu = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "else:\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    model_cpu = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SXx-nBe5-jyJ"
      },
      "outputs": [],
      "source": [
        "def get_newest_checkpoint(checkpoint_dir):\n",
        "    files = os.listdir(checkpoint_dir)\n",
        "    paths = [os.path.join(checkpoint_dir, basename) for basename in files]\n",
        "    return max(paths, key=os.path.getctime)\n",
        "\n",
        "def import_previous_compatible_checkpoint(model, force_import=False):\n",
        "    meta = read_model_metadata(suffix=model_suffix)\n",
        "    if meta is None:\n",
        "        print(\"No previous checkpoint found\")\n",
        "        return False\n",
        "    if is_metadata_compatible(params, meta) is not True and force_import is False:\n",
        "        print(\"No useable import found.\")\n",
        "        return False\n",
        "    try:\n",
        "        last_checkpoint = get_newest_checkpoint(checkpoint_dir) # Doesn't do anything: tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot determine last checkpoint in {checkpoint_dir}, cannot import due to: {e}\")\n",
        "        return False\n",
        "    print(f\"Last checkpoint: {last_checkpoint}\")\n",
        "    try:\n",
        "        model.load_weights(last_checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to import model {last_checkpoint}: {e}\")\n",
        "        return False\n",
        "    if 'current_epoch' in meta:\n",
        "        params['current_epoch'] = meta['current_epoch']\n",
        "    print(f\"Successful import of epoch {params['current_epoch']} from {last_checkpoint}, continuing from there...\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB-Q8YXvndE"
      },
      "source": [
        "### Loss function, optimizer, tensorboard output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0t5JWEdYZNGz"
      },
      "outputs": [],
      "source": [
        "kscc = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def loss(labels, logits):\n",
        "  vl=kscc(labels, logits)\n",
        "  return vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "jc2kbGoAZXHi"
      },
      "outputs": [],
      "source": [
        "if params['clipvalue'] is not None:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "else:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        model.compile(optimizer=opti, loss=loss, metrics=[], run_eagerly=False, jit_compile=True)\n",
        "else:\n",
        "    model.compile(optimizer=opti, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DAoMxogcX_Nq"
      },
      "outputs": [],
      "source": [
        "import_checkpoint = False\n",
        "force_import = False   # True: ignore metadata and try import anyway. This will of course crash, if the new model doesn't fit the checkpoint-data...\n",
        "\n",
        "if import_checkpoint is True:\n",
        "    import_previous_compatible_checkpoint(model, force_import=force_import)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb651ec1-2d36-426c-e309-e9c938cc3e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"mhsa_v1_tf\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 80)]              0         \n",
            "                                                                 \n",
            " tf.cast (TFOpLambda)        (None, 80)                0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 80, 128)           640000    \n",
            "                                                                 \n",
            " positional_encoding (Positi  (None, 80, 128)          0         \n",
            " onalEncoding)                                                   \n",
            "                                                                 \n",
            " multi_head_self_attention (  (None, 80, 128)          1606144   \n",
            " MultiHeadSelfAttention)                                         \n",
            "                                                                 \n",
            " multi_head_self_attention_1  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_2  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_3  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_4  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_5  (None, 80, 128)          1606144   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 80, 5000)          645000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,921,864\n",
            "Trainable params: 10,921,864\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OHZurM5ei95K"
      },
      "outputs": [],
      "source": [
        "TPU_GENERATE_ON_CPU = False  # The thing is: both options are slow on TPU :-/\n",
        "\n",
        "class ServiceCallback(keras.callbacks.Callback):\n",
        "#    def on_test_end(self, logs=None):\n",
        "    # @tf.function\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_model_metadata(epoch, suffix=model_suffix)\n",
        "        if (epoch+1) % params['sample_every_n_epochs'] == 0:\n",
        "            idx=random.randint(0,len(td)-1)\n",
        "            text=td.decode(td[idx])\n",
        "            print()\n",
        "            if ml_env.is_tpu is True:\n",
        "                temp_list=[0.7] # [0.6,0.7,0.8]\n",
        "                gen_len=50\n",
        "                with tpu_strategy.scope():\n",
        "                    weights=model.get_weights()\n",
        "                model_cpu.set_weights(weights)\n",
        "                # HDF5 is required for saving weights that originate from TPU\n",
        "                # otherwise this just silently fails...\n",
        "                checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.h5\")\n",
        "                chkpt_dest=checkpoint_path.format(epoch=epoch)\n",
        "                print(f\"Checkpoint: {chkpt_dest}\")\n",
        "                model_cpu.save_weights(chkpt_dest)\n",
        "            else:\n",
        "                temp_list=[0.6, 0.7, 0.8]\n",
        "                gen_len=192\n",
        "            print(f\"prompt: {text}\")\n",
        "            for temp in temp_list:\n",
        "                print(f\"---------------- T={temp} ---------------\")\n",
        "                if ml_env.is_tpu is True and TPU_GENERATE_ON_CPU is True:\n",
        "                    with tf.device('/cpu:0'):\n",
        "                        if temp==0.0:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                        else:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                else:\n",
        "                    if temp==0.0:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                    else:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                td.source_highlight(reply, min_quote_size=10, dark_mode=use_dark_mode, display_ref_anchor=False)\n",
        "            print(\"--------------------------------------\")\n",
        "\n",
        "service_callback=ServiceCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5SKvObcsEAQ5"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "if ml_env.is_tpu:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='epoch', write_graph=False)\n",
        "else:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "o0Ew6pgWzeFj"
      },
      "outputs": [],
      "source": [
        "# Dont try:\n",
        "#    # use the python variable log_path:\n",
        "#   get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
        "#except:\n",
        "#   pass\n",
        "\n",
        "# The following throws errors on non-colab, but the guarding above is too bug-ridden.\n",
        "# if ml_env.is_tpu is False:\n",
        "#    %tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFbZcN0vxOB"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kh2yUKBoEAQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098906d2-e82d-4bb1-e10a-7dfce5e2eef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING override of sample_every_n_epochs sample-generation to: 200\n"
          ]
        }
      ],
      "source": [
        "EPOCHS=500000\n",
        "if 'current_epoch' in params:\n",
        "    initial_epoch=params['current_epoch']\n",
        "else:\n",
        "    initial_epoch=0\n",
        "\n",
        "override=200\n",
        "print(f\"WARNING override of sample_every_n_epochs sample-generation to: {override}\")\n",
        "params['sample_every_n_epochs']=override"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d00a408a-29f3-419c-b58f-7c5ecd9416c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6874/500000\n",
            " 5/28 [====>.........................] - ETA: 0s - loss: 5.8706"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 3.8225s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28/28 [==============================] - 49s 43ms/step - loss: 2.6447\n",
            "Epoch 6875/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1974\n",
            "Epoch 6876/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1604\n",
            "Epoch 6877/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1498\n",
            "Epoch 6878/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1466\n",
            "Epoch 6879/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1383\n",
            "Epoch 6880/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1377\n",
            "Epoch 6881/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1324\n",
            "Epoch 6882/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1317\n",
            "Epoch 6883/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1301\n",
            "Epoch 6884/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1293\n",
            "Epoch 6885/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1240\n",
            "Epoch 6886/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1219\n",
            "Epoch 6887/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1220\n",
            "Epoch 6888/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1206\n",
            "Epoch 6889/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1188\n",
            "Epoch 6890/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1185\n",
            "Epoch 6891/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1169\n",
            "Epoch 6892/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1156\n",
            "Epoch 6893/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1147\n",
            "Epoch 6894/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1141\n",
            "Epoch 6895/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1133\n",
            "Epoch 6896/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1136\n",
            "Epoch 6897/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1131\n",
            "Epoch 6898/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1117\n",
            "Epoch 6899/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1110\n",
            "Epoch 6900/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1092\n",
            "Epoch 6901/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1099\n",
            "Epoch 6902/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1084\n",
            "Epoch 6903/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1114\n",
            "Epoch 6904/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1080\n",
            "Epoch 6905/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1089\n",
            "Epoch 6906/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1057\n",
            "Epoch 6907/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1055\n",
            "Epoch 6908/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.1077\n",
            "Epoch 6909/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1061\n",
            "Epoch 6910/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1060\n",
            "Epoch 6911/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1048\n",
            "Epoch 6912/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1039\n",
            "Epoch 6913/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1036\n",
            "Epoch 6914/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1061\n",
            "Epoch 6915/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1039\n",
            "Epoch 6916/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1018\n",
            "Epoch 6917/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1042\n",
            "Epoch 6918/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1038\n",
            "Epoch 6919/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1026\n",
            "Epoch 6920/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1043\n",
            "Epoch 6921/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1054\n",
            "Epoch 6922/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 1.1013\n",
            "Epoch 6923/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 1.1034\n",
            "Epoch 6924/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1003\n",
            "Epoch 6925/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 1.1014\n",
            "Epoch 6926/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1012\n",
            "Epoch 6927/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1005\n",
            "Epoch 6928/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1009\n",
            "Epoch 6929/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0978\n",
            "Epoch 6930/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1017\n",
            "Epoch 6931/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0998\n",
            "Epoch 6932/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1008\n",
            "Epoch 6933/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0993\n",
            "Epoch 6934/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1008\n",
            "Epoch 6935/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1009\n",
            "Epoch 6936/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0979\n",
            "Epoch 6937/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0996\n",
            "Epoch 6938/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.1020\n",
            "Epoch 6939/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0989\n",
            "Epoch 6940/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0990\n",
            "Epoch 6941/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0967\n",
            "Epoch 6942/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0965\n",
            "Epoch 6943/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0979\n",
            "Epoch 6944/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0985\n",
            "Epoch 6945/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0983\n",
            "Epoch 6946/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0985\n",
            "Epoch 6947/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0975\n",
            "Epoch 6948/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0991\n",
            "Epoch 6949/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0957\n",
            "Epoch 6950/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0949\n",
            "Epoch 6951/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0961\n",
            "Epoch 6952/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0969\n",
            "Epoch 6953/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0964\n",
            "Epoch 6954/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0967\n",
            "Epoch 6955/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0944\n",
            "Epoch 6956/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0936\n",
            "Epoch 6957/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0919\n",
            "Epoch 6958/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0924\n",
            "Epoch 6959/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0928\n",
            "Epoch 6960/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0923\n",
            "Epoch 6961/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0912\n",
            "Epoch 6962/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0901\n",
            "Epoch 6963/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0919\n",
            "Epoch 6964/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0922\n",
            "Epoch 6965/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 1.0917\n",
            "Epoch 6966/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0894\n",
            "Epoch 6967/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0881\n",
            "Epoch 6968/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0895\n",
            "Epoch 6969/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0897\n",
            "Epoch 6970/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0904\n",
            "Epoch 6971/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0885\n",
            "Epoch 6972/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0869\n",
            "Epoch 6973/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0868\n",
            "Epoch 6974/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0882\n",
            "Epoch 6975/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0884\n",
            "Epoch 6976/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0865\n",
            "Epoch 6977/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0852\n",
            "Epoch 6978/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0846\n",
            "Epoch 6979/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 1.0856\n",
            "Epoch 6980/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0853\n",
            "Epoch 6981/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0842\n",
            "Epoch 6982/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0846\n",
            "Epoch 6983/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0826\n",
            "Epoch 6984/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0836\n",
            "Epoch 6985/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0821\n",
            "Epoch 6986/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0842\n",
            "Epoch 6987/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0816\n",
            "Epoch 6988/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0813\n",
            "Epoch 6989/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0819\n",
            "Epoch 6990/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0817\n",
            "Epoch 6991/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0791\n",
            "Epoch 6992/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0787\n",
            "Epoch 6993/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0797\n",
            "Epoch 6994/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0771\n",
            "Epoch 6995/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0806\n",
            "Epoch 6996/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0767\n",
            "Epoch 6997/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0755\n",
            "Epoch 6998/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0773\n",
            "Epoch 6999/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0746\n",
            "Epoch 7000/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 1.0725\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-6999.h5\n",
            "prompt: and in old age\n",
            "the art is practised mostly for its uses, and friendships and other\n",
            "adventures and experiments in the art of reading character are seldom\n",
            "made. But novelists differ from the rest of the world because they do\n",
            "not cease to be interested in character when they have learnt enough\n",
            "a\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ", she of on ob anyalll hoabfor ar ing hiying  dome, who ink. “rs, dld atmarrhe g pro, to imrelsshould <br>WhD.<br>rs. !’<span style=\"background-color:#eadbd8;\">rother’s i</span>st!elllen’ton!,” saiBine go"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 903ms/step - loss: 1.0724\n",
            "Epoch 7001/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0705\n",
            "Epoch 7002/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0708\n",
            "Epoch 7003/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0697\n",
            "Epoch 7004/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0694\n",
            "Epoch 7005/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0708\n",
            "Epoch 7006/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0663\n",
            "Epoch 7007/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0672\n",
            "Epoch 7008/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0679\n",
            "Epoch 7009/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0661\n",
            "Epoch 7010/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0640\n",
            "Epoch 7011/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0635\n",
            "Epoch 7012/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0613\n",
            "Epoch 7013/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0596\n",
            "Epoch 7014/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0601\n",
            "Epoch 7015/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0581\n",
            "Epoch 7016/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0571\n",
            "Epoch 7017/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0573\n",
            "Epoch 7018/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0568\n",
            "Epoch 7019/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0547\n",
            "Epoch 7020/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0526\n",
            "Epoch 7021/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0510\n",
            "Epoch 7022/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0497\n",
            "Epoch 7023/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0465\n",
            "Epoch 7024/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0465\n",
            "Epoch 7025/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0457\n",
            "Epoch 7026/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0450\n",
            "Epoch 7027/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0447\n",
            "Epoch 7028/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0415\n",
            "Epoch 7029/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0371\n",
            "Epoch 7030/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0346\n",
            "Epoch 7031/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0327\n",
            "Epoch 7032/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 1.0335\n",
            "Epoch 7033/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0325\n",
            "Epoch 7034/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0285\n",
            "Epoch 7035/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0267\n",
            "Epoch 7036/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0258\n",
            "Epoch 7037/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0216\n",
            "Epoch 7038/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0183\n",
            "Epoch 7039/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0134\n",
            "Epoch 7040/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0146\n",
            "Epoch 7041/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0108\n",
            "Epoch 7042/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0055\n",
            "Epoch 7043/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0041\n",
            "Epoch 7044/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 1.0012\n",
            "Epoch 7045/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9993\n",
            "Epoch 7046/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9923\n",
            "Epoch 7047/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9921\n",
            "Epoch 7048/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9865\n",
            "Epoch 7049/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9814\n",
            "Epoch 7050/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9815\n",
            "Epoch 7051/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9744\n",
            "Epoch 7052/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9730\n",
            "Epoch 7053/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9680\n",
            "Epoch 7054/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9686\n",
            "Epoch 7055/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9633\n",
            "Epoch 7056/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9623\n",
            "Epoch 7057/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9546\n",
            "Epoch 7058/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9548\n",
            "Epoch 7059/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9515\n",
            "Epoch 7060/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9507\n",
            "Epoch 7061/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9447\n",
            "Epoch 7062/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9438\n",
            "Epoch 7063/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9402\n",
            "Epoch 7064/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9376\n",
            "Epoch 7065/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9337\n",
            "Epoch 7066/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9344\n",
            "Epoch 7067/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9270\n",
            "Epoch 7068/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9278\n",
            "Epoch 7069/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9217\n",
            "Epoch 7070/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9177\n",
            "Epoch 7071/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9166\n",
            "Epoch 7072/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9163\n",
            "Epoch 7073/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9127\n",
            "Epoch 7074/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9117\n",
            "Epoch 7075/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9065\n",
            "Epoch 7076/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.9067\n",
            "Epoch 7077/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.9022\n",
            "Epoch 7078/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8987\n",
            "Epoch 7079/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8966\n",
            "Epoch 7080/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.8936\n",
            "Epoch 7081/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8925\n",
            "Epoch 7082/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8895\n",
            "Epoch 7083/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.8855\n",
            "Epoch 7084/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8808\n",
            "Epoch 7085/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8800\n",
            "Epoch 7086/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8803\n",
            "Epoch 7087/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.8737\n",
            "Epoch 7088/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8727\n",
            "Epoch 7089/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8680\n",
            "Epoch 7090/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8671\n",
            "Epoch 7091/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.8643\n",
            "Epoch 7092/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8568\n",
            "Epoch 7093/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8565\n",
            "Epoch 7094/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8549\n",
            "Epoch 7095/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8543\n",
            "Epoch 7096/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.8488\n",
            "Epoch 7097/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8440\n",
            "Epoch 7098/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8419\n",
            "Epoch 7099/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8406\n",
            "Epoch 7100/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8353\n",
            "Epoch 7101/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8314\n",
            "Epoch 7102/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8290\n",
            "Epoch 7103/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8272\n",
            "Epoch 7104/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8248\n",
            "Epoch 7105/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8177\n",
            "Epoch 7106/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8168\n",
            "Epoch 7107/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8132\n",
            "Epoch 7108/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8086\n",
            "Epoch 7109/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.8050\n",
            "Epoch 7110/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7980\n",
            "Epoch 7111/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7929\n",
            "Epoch 7112/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7910\n",
            "Epoch 7113/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7834\n",
            "Epoch 7114/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7767\n",
            "Epoch 7115/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7750\n",
            "Epoch 7116/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7685\n",
            "Epoch 7117/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7657\n",
            "Epoch 7118/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7604\n",
            "Epoch 7119/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.7570\n",
            "Epoch 7120/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7484\n",
            "Epoch 7121/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7469\n",
            "Epoch 7122/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7381\n",
            "Epoch 7123/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7364\n",
            "Epoch 7124/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.7314\n",
            "Epoch 7125/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7298\n",
            "Epoch 7126/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.7243\n",
            "Epoch 7127/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.7183\n",
            "Epoch 7128/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.7148\n",
            "Epoch 7129/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7105\n",
            "Epoch 7130/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7073\n",
            "Epoch 7131/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.7035\n",
            "Epoch 7132/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6989\n",
            "Epoch 7133/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6938\n",
            "Epoch 7134/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6886\n",
            "Epoch 7135/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6885\n",
            "Epoch 7136/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6864\n",
            "Epoch 7137/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6800\n",
            "Epoch 7138/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6785\n",
            "Epoch 7139/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6756\n",
            "Epoch 7140/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6686\n",
            "Epoch 7141/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6677\n",
            "Epoch 7142/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6660\n",
            "Epoch 7143/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6629\n",
            "Epoch 7144/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6587\n",
            "Epoch 7145/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6556\n",
            "Epoch 7146/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6555\n",
            "Epoch 7147/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6507\n",
            "Epoch 7148/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6492\n",
            "Epoch 7149/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6439\n",
            "Epoch 7150/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6405\n",
            "Epoch 7151/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6390\n",
            "Epoch 7152/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6376\n",
            "Epoch 7153/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6365\n",
            "Epoch 7154/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6279\n",
            "Epoch 7155/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6290\n",
            "Epoch 7156/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6256\n",
            "Epoch 7157/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6253\n",
            "Epoch 7158/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6255\n",
            "Epoch 7159/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6184\n",
            "Epoch 7160/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6197\n",
            "Epoch 7161/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6150\n",
            "Epoch 7162/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6150\n",
            "Epoch 7163/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6137\n",
            "Epoch 7164/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6122\n",
            "Epoch 7165/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6050\n",
            "Epoch 7166/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6042\n",
            "Epoch 7167/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.6044\n",
            "Epoch 7168/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.6022\n",
            "Epoch 7169/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.5982\n",
            "Epoch 7170/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5991\n",
            "Epoch 7171/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5963\n",
            "Epoch 7172/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5957\n",
            "Epoch 7173/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5924\n",
            "Epoch 7174/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5906\n",
            "Epoch 7175/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5886\n",
            "Epoch 7176/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5915\n",
            "Epoch 7177/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.5851\n",
            "Epoch 7178/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5863\n",
            "Epoch 7179/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5829\n",
            "Epoch 7180/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5799\n",
            "Epoch 7181/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5818\n",
            "Epoch 7182/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5790\n",
            "Epoch 7183/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5750\n",
            "Epoch 7184/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5743\n",
            "Epoch 7185/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5707\n",
            "Epoch 7186/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5695\n",
            "Epoch 7187/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5686\n",
            "Epoch 7188/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5679\n",
            "Epoch 7189/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5654\n",
            "Epoch 7190/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5657\n",
            "Epoch 7191/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5588\n",
            "Epoch 7192/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5609\n",
            "Epoch 7193/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5632\n",
            "Epoch 7194/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5594\n",
            "Epoch 7195/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5593\n",
            "Epoch 7196/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5584\n",
            "Epoch 7197/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5561\n",
            "Epoch 7198/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5551\n",
            "Epoch 7199/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5526\n",
            "Epoch 7200/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.5502\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-7199.h5\n",
            "prompt: ting that as he found himself obliged\n",
            "to go to London on the morrow for a few days, he could not help trying\n",
            "to procure a companion; and therefore hoped that if William could make\n",
            "up his mind to leave Mansfield half a day earlier than had been\n",
            "proposed, he would accept a place in hi\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:#eadbd8;\">t anything a</span>dded<br><span style=\"background-color:#d4efdf;\">he always be </span>farth<span style=\"background-color:#d6dbdf;\">inion. Her c</span>huld,<br>were welenrly fins no defieldmixiouss of exionne<span style=\"background-color:#eadbd8;\">e—but she </span>know,<span style=\"background-color:#d0ece7;\"> means any</span><span style=\"background-color:#e2d7d5;\">thing so di</span><span style=\"background-color:#eadbd8;\">nance and a</span><span style=\"background-color:#d4e6f1;\">gree that s</span>tate "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 884ms/step - loss: 0.5500\n",
            "Epoch 7201/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5491\n",
            "Epoch 7202/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5481\n",
            "Epoch 7203/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5451\n",
            "Epoch 7204/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5439\n",
            "Epoch 7205/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5426\n",
            "Epoch 7206/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5432\n",
            "Epoch 7207/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5385\n",
            "Epoch 7208/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5430\n",
            "Epoch 7209/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5426\n",
            "Epoch 7210/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5408\n",
            "Epoch 7211/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5347\n",
            "Epoch 7212/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5381\n",
            "Epoch 7213/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5344\n",
            "Epoch 7214/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5305\n",
            "Epoch 7215/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5344\n",
            "Epoch 7216/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5341\n",
            "Epoch 7217/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5321\n",
            "Epoch 7218/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5308\n",
            "Epoch 7219/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5304\n",
            "Epoch 7220/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5285\n",
            "Epoch 7221/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5282\n",
            "Epoch 7222/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5286\n",
            "Epoch 7223/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5281\n",
            "Epoch 7224/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5235\n",
            "Epoch 7225/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5255\n",
            "Epoch 7226/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5231\n",
            "Epoch 7227/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5226\n",
            "Epoch 7228/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5234\n",
            "Epoch 7229/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5203\n",
            "Epoch 7230/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5202\n",
            "Epoch 7231/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5169\n",
            "Epoch 7232/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5198\n",
            "Epoch 7233/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5173\n",
            "Epoch 7234/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5172\n",
            "Epoch 7235/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5156\n",
            "Epoch 7236/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5128\n",
            "Epoch 7237/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5128\n",
            "Epoch 7238/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5113\n",
            "Epoch 7239/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5103\n",
            "Epoch 7240/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5136\n",
            "Epoch 7241/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5089\n",
            "Epoch 7242/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5095\n",
            "Epoch 7243/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5079\n",
            "Epoch 7244/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5105\n",
            "Epoch 7245/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5074\n",
            "Epoch 7246/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5065\n",
            "Epoch 7247/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5035\n",
            "Epoch 7248/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5068\n",
            "Epoch 7249/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5053\n",
            "Epoch 7250/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5032\n",
            "Epoch 7251/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5036\n",
            "Epoch 7252/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.5005\n",
            "Epoch 7253/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4983\n",
            "Epoch 7254/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.5002\n",
            "Epoch 7255/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4998\n",
            "Epoch 7256/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4959\n",
            "Epoch 7257/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4960\n",
            "Epoch 7258/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4999\n",
            "Epoch 7259/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4951\n",
            "Epoch 7260/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4958\n",
            "Epoch 7261/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4935\n",
            "Epoch 7262/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4973\n",
            "Epoch 7263/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4921\n",
            "Epoch 7264/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4918\n",
            "Epoch 7265/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4927\n",
            "Epoch 7266/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4878\n",
            "Epoch 7267/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4882\n",
            "Epoch 7268/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4926\n",
            "Epoch 7269/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4888\n",
            "Epoch 7270/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4868\n",
            "Epoch 7271/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4880\n",
            "Epoch 7272/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.4898\n",
            "Epoch 7273/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4854\n",
            "Epoch 7274/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4861\n",
            "Epoch 7275/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4856\n",
            "Epoch 7276/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4831\n",
            "Epoch 7277/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4847\n",
            "Epoch 7278/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4829\n",
            "Epoch 7279/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4830\n",
            "Epoch 7280/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4806\n",
            "Epoch 7281/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4833\n",
            "Epoch 7282/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4813\n",
            "Epoch 7283/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4785\n",
            "Epoch 7284/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4809\n",
            "Epoch 7285/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4767\n",
            "Epoch 7286/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4801\n",
            "Epoch 7287/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4804\n",
            "Epoch 7288/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4793\n",
            "Epoch 7289/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4777\n",
            "Epoch 7290/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4751\n",
            "Epoch 7291/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4734\n",
            "Epoch 7292/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4734\n",
            "Epoch 7293/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4746\n",
            "Epoch 7294/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4747\n",
            "Epoch 7295/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4769\n",
            "Epoch 7296/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4723\n",
            "Epoch 7297/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4734\n",
            "Epoch 7298/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4681\n",
            "Epoch 7299/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4711\n",
            "Epoch 7300/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4696\n",
            "Epoch 7301/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4732\n",
            "Epoch 7302/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4714\n",
            "Epoch 7303/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4676\n",
            "Epoch 7304/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4727\n",
            "Epoch 7305/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4675\n",
            "Epoch 7306/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4705\n",
            "Epoch 7307/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4683\n",
            "Epoch 7308/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4637\n",
            "Epoch 7309/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4656\n",
            "Epoch 7310/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4634\n",
            "Epoch 7311/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4626\n",
            "Epoch 7312/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4644\n",
            "Epoch 7313/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4662\n",
            "Epoch 7314/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4628\n",
            "Epoch 7315/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4619\n",
            "Epoch 7316/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4616\n",
            "Epoch 7317/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4645\n",
            "Epoch 7318/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4662\n",
            "Epoch 7319/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4622\n",
            "Epoch 7320/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4568\n",
            "Epoch 7321/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4597\n",
            "Epoch 7322/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4596\n",
            "Epoch 7323/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4580\n",
            "Epoch 7324/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4572\n",
            "Epoch 7325/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4601\n",
            "Epoch 7326/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4560\n",
            "Epoch 7327/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4567\n",
            "Epoch 7328/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4579\n",
            "Epoch 7329/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4570\n",
            "Epoch 7330/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4550\n",
            "Epoch 7331/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4573\n",
            "Epoch 7332/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4546\n",
            "Epoch 7333/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4581\n",
            "Epoch 7334/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4526\n",
            "Epoch 7335/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4504\n",
            "Epoch 7336/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4532\n",
            "Epoch 7337/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4508\n",
            "Epoch 7338/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4508\n",
            "Epoch 7339/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4544\n",
            "Epoch 7340/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4498\n",
            "Epoch 7341/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4522\n",
            "Epoch 7342/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4493\n",
            "Epoch 7343/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4505\n",
            "Epoch 7344/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4528\n",
            "Epoch 7345/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4540\n",
            "Epoch 7346/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4507\n",
            "Epoch 7347/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4518\n",
            "Epoch 7348/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4509\n",
            "Epoch 7349/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4480\n",
            "Epoch 7350/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4489\n",
            "Epoch 7351/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4470\n",
            "Epoch 7352/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4490\n",
            "Epoch 7353/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4486\n",
            "Epoch 7354/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4478\n",
            "Epoch 7355/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4436\n",
            "Epoch 7356/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4452\n",
            "Epoch 7357/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4422\n",
            "Epoch 7358/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4453\n",
            "Epoch 7359/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4429\n",
            "Epoch 7360/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4428\n",
            "Epoch 7361/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4432\n",
            "Epoch 7362/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4426\n",
            "Epoch 7363/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4417\n",
            "Epoch 7364/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4427\n",
            "Epoch 7365/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4423\n",
            "Epoch 7366/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4445\n",
            "Epoch 7367/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4441\n",
            "Epoch 7368/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4390\n",
            "Epoch 7369/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4399\n",
            "Epoch 7370/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4429\n",
            "Epoch 7371/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4376\n",
            "Epoch 7372/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4379\n",
            "Epoch 7373/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4422\n",
            "Epoch 7374/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4385\n",
            "Epoch 7375/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4400\n",
            "Epoch 7376/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4371\n",
            "Epoch 7377/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4389\n",
            "Epoch 7378/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4390\n",
            "Epoch 7379/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4369\n",
            "Epoch 7380/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4342\n",
            "Epoch 7381/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4359\n",
            "Epoch 7382/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4350\n",
            "Epoch 7383/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4340\n",
            "Epoch 7384/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4377\n",
            "Epoch 7385/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4345\n",
            "Epoch 7386/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4342\n",
            "Epoch 7387/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4334\n",
            "Epoch 7388/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4330\n",
            "Epoch 7389/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4330\n",
            "Epoch 7390/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4345\n",
            "Epoch 7391/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4322\n",
            "Epoch 7392/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4327\n",
            "Epoch 7393/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4299\n",
            "Epoch 7394/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4307\n",
            "Epoch 7395/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4286\n",
            "Epoch 7396/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4299\n",
            "Epoch 7397/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4265\n",
            "Epoch 7398/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4289\n",
            "Epoch 7399/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4258\n",
            "Epoch 7400/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.4289\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-7399.h5\n",
            "prompt: an to walk slowly home.\n",
            "\n",
            "\"Who is that?\" said Mrs. Flanders, shading her eyes.\n",
            "\n",
            "\"That old man in the road?\" said Archer, looking below.\n",
            "\n",
            "\"He's not an old man,\" said Mrs. Flanders. \"He's--no, he's not--I\n",
            "thought it was the Captain, but \n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "out<span style=\"background-color:#e2d7d5;\"> more in the </span>bluiae what<span style=\"background-color:#d4e6f1;\">’s Easter </span>of so with, bfica<span style=\"background-color:#e2d7d5;\">ntion seemed to </span>her se<span style=\"background-color:#edebd0;\">at everything </span>and accury, for<span style=\"background-color:#e2d7d5;\"> if she se</span><span style=\"background-color:#d4efdf;\">eing at the </span>frocan<br>bele orsor! Well----I wish "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#edebd0;\">Jane Austen: Lady Susan</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 883ms/step - loss: 0.4286\n",
            "Epoch 7401/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4262\n",
            "Epoch 7402/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4279\n",
            "Epoch 7403/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4268\n",
            "Epoch 7404/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4288\n",
            "Epoch 7405/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4262\n",
            "Epoch 7406/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4268\n",
            "Epoch 7407/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4256\n",
            "Epoch 7408/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4259\n",
            "Epoch 7409/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4265\n",
            "Epoch 7410/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4266\n",
            "Epoch 7411/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4298\n",
            "Epoch 7412/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4268\n",
            "Epoch 7413/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4270\n",
            "Epoch 7414/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4233\n",
            "Epoch 7415/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4251\n",
            "Epoch 7416/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4248\n",
            "Epoch 7417/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4269\n",
            "Epoch 7418/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4235\n",
            "Epoch 7419/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4253\n",
            "Epoch 7420/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4228\n",
            "Epoch 7421/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4223\n",
            "Epoch 7422/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4253\n",
            "Epoch 7423/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4203\n",
            "Epoch 7424/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4221\n",
            "Epoch 7425/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4224\n",
            "Epoch 7426/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4199\n",
            "Epoch 7427/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4222\n",
            "Epoch 7428/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4204\n",
            "Epoch 7429/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4174\n",
            "Epoch 7430/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4205\n",
            "Epoch 7431/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4180\n",
            "Epoch 7432/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4209\n",
            "Epoch 7433/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4199\n",
            "Epoch 7434/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4192\n",
            "Epoch 7435/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4182\n",
            "Epoch 7436/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4200\n",
            "Epoch 7437/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4166\n",
            "Epoch 7438/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4162\n",
            "Epoch 7439/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4169\n",
            "Epoch 7440/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4181\n",
            "Epoch 7441/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4164\n",
            "Epoch 7442/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4159\n",
            "Epoch 7443/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4162\n",
            "Epoch 7444/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4175\n",
            "Epoch 7445/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4137\n",
            "Epoch 7446/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4169\n",
            "Epoch 7447/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4154\n",
            "Epoch 7448/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4160\n",
            "Epoch 7449/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4131\n",
            "Epoch 7450/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4165\n",
            "Epoch 7451/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4165\n",
            "Epoch 7452/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4118\n",
            "Epoch 7453/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4127\n",
            "Epoch 7454/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4112\n",
            "Epoch 7455/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4124\n",
            "Epoch 7456/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4147\n",
            "Epoch 7457/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4131\n",
            "Epoch 7458/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4140\n",
            "Epoch 7459/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4133\n",
            "Epoch 7460/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4128\n",
            "Epoch 7461/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4131\n",
            "Epoch 7462/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4130\n",
            "Epoch 7463/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4148\n",
            "Epoch 7464/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4119\n",
            "Epoch 7465/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4105\n",
            "Epoch 7466/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4120\n",
            "Epoch 7467/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4111\n",
            "Epoch 7468/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4132\n",
            "Epoch 7469/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4126\n",
            "Epoch 7470/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4098\n",
            "Epoch 7471/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4132\n",
            "Epoch 7472/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.4115\n",
            "Epoch 7473/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4121\n",
            "Epoch 7474/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4082\n",
            "Epoch 7475/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4095\n",
            "Epoch 7476/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4100\n",
            "Epoch 7477/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4083\n",
            "Epoch 7478/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4098\n",
            "Epoch 7479/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.4102\n",
            "Epoch 7480/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4085\n",
            "Epoch 7481/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4056\n",
            "Epoch 7482/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4064\n",
            "Epoch 7483/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4105\n",
            "Epoch 7484/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4074\n",
            "Epoch 7485/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4093\n",
            "Epoch 7486/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4084\n",
            "Epoch 7487/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4058\n",
            "Epoch 7488/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4056\n",
            "Epoch 7489/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4052\n",
            "Epoch 7490/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4045\n",
            "Epoch 7491/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4058\n",
            "Epoch 7492/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4055\n",
            "Epoch 7493/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4040\n",
            "Epoch 7494/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4035\n",
            "Epoch 7495/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4052\n",
            "Epoch 7496/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4056\n",
            "Epoch 7497/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4031\n",
            "Epoch 7498/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4074\n",
            "Epoch 7499/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4060\n",
            "Epoch 7500/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4046\n",
            "Epoch 7501/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4027\n",
            "Epoch 7502/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4034\n",
            "Epoch 7503/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4039\n",
            "Epoch 7504/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4011\n",
            "Epoch 7505/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4051\n",
            "Epoch 7506/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4041\n",
            "Epoch 7507/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4037\n",
            "Epoch 7508/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4013\n",
            "Epoch 7509/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4037\n",
            "Epoch 7510/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4035\n",
            "Epoch 7511/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4017\n",
            "Epoch 7512/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3990\n",
            "Epoch 7513/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4023\n",
            "Epoch 7514/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4017\n",
            "Epoch 7515/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4019\n",
            "Epoch 7516/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4022\n",
            "Epoch 7517/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4023\n",
            "Epoch 7518/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4012\n",
            "Epoch 7519/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3984\n",
            "Epoch 7520/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3988\n",
            "Epoch 7521/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4027\n",
            "Epoch 7522/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3999\n",
            "Epoch 7523/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.4005\n",
            "Epoch 7524/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4028\n",
            "Epoch 7525/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3960\n",
            "Epoch 7526/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4003\n",
            "Epoch 7527/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3972\n",
            "Epoch 7528/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3990\n",
            "Epoch 7529/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3989\n",
            "Epoch 7530/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3990\n",
            "Epoch 7531/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3982\n",
            "Epoch 7532/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3969\n",
            "Epoch 7533/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3973\n",
            "Epoch 7534/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3963\n",
            "Epoch 7535/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3954\n",
            "Epoch 7536/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3978\n",
            "Epoch 7537/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3972\n",
            "Epoch 7538/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3952\n",
            "Epoch 7539/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3964\n",
            "Epoch 7540/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3956\n",
            "Epoch 7541/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3965\n",
            "Epoch 7542/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3955\n",
            "Epoch 7543/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3946\n",
            "Epoch 7544/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3953\n",
            "Epoch 7545/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3949\n",
            "Epoch 7546/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3951\n",
            "Epoch 7547/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3931\n",
            "Epoch 7548/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3941\n",
            "Epoch 7549/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3953\n",
            "Epoch 7550/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3941\n",
            "Epoch 7551/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3947\n",
            "Epoch 7552/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3898\n",
            "Epoch 7553/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3938\n",
            "Epoch 7554/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3943\n",
            "Epoch 7555/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3943\n",
            "Epoch 7556/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3934\n",
            "Epoch 7557/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3926\n",
            "Epoch 7558/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3939\n",
            "Epoch 7559/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3912\n",
            "Epoch 7560/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3942\n",
            "Epoch 7561/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3885\n",
            "Epoch 7562/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3937\n",
            "Epoch 7563/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3942\n",
            "Epoch 7564/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3912\n",
            "Epoch 7565/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3942\n",
            "Epoch 7566/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3927\n",
            "Epoch 7567/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3929\n",
            "Epoch 7568/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3927\n",
            "Epoch 7569/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3924\n",
            "Epoch 7570/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3883\n",
            "Epoch 7571/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3887\n",
            "Epoch 7572/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3896\n",
            "Epoch 7573/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3920\n",
            "Epoch 7574/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3914\n",
            "Epoch 7575/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3917\n",
            "Epoch 7576/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3908\n",
            "Epoch 7577/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3918\n",
            "Epoch 7578/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3897\n",
            "Epoch 7579/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3877\n",
            "Epoch 7580/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3896\n",
            "Epoch 7581/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3894\n",
            "Epoch 7582/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3916\n",
            "Epoch 7583/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3874\n",
            "Epoch 7584/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3894\n",
            "Epoch 7585/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3885\n",
            "Epoch 7586/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3879\n",
            "Epoch 7587/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3856\n",
            "Epoch 7588/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3860\n",
            "Epoch 7589/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3865\n",
            "Epoch 7590/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3865\n",
            "Epoch 7591/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3901\n",
            "Epoch 7592/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3871\n",
            "Epoch 7593/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3848\n",
            "Epoch 7594/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3859\n",
            "Epoch 7595/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3871\n",
            "Epoch 7596/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3881\n",
            "Epoch 7597/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3870\n",
            "Epoch 7598/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3821\n",
            "Epoch 7599/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3867\n",
            "Epoch 7600/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.3834\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-7599.h5\n",
            "prompt:  had passed,\n",
            "they were discovering that they knew some of the same people, as indeed\n",
            "had been obvious from their appearance directly they saw each other.\n",
            "\n",
            "“Ah yes, old Truefit,” said Mr. Elliot. “He has a son at Oxford. I’ve\n",
            "often stayed with them. It’s a lovely old Jacobean house\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:#ebdef0;\">; and the<br></span><span style=\"background-color:#ebdef0;\">hands were </span><span style=\"background-color:#d4efdf;\">half the rest</span><span style=\"background-color:#ebdef0;\">, some with </span><span style=\"background-color:#d6eaf8;\">every position</span>,<br>and few men<span style=\"background-color:#d4e6f1;\"> burst, and </span>powerful<br>in<span style=\"background-color:#e2d7d5;\">taking the pre</span>sent, spoliving<span style=\"background-color:#d4efdf;\"> no sentime</span>s<span style=\"background-color:#eadbd8;\"> very house</span>. He trial<span style=\"background-color:#e2d7d5;\"> time in comp</span>aniba<span style=\"background-color:#eadbd8;\">d talking to</span><span style=\"background-color:#e2d7d5;\"><br>attention t</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 873ms/step - loss: 0.3836\n",
            "Epoch 7601/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 0.3860\n",
            "Epoch 7602/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3856\n",
            "Epoch 7603/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3856\n",
            "Epoch 7604/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3848\n",
            "Epoch 7605/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3860\n",
            "Epoch 7606/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3861\n",
            "Epoch 7607/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3886\n",
            "Epoch 7608/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3850\n",
            "Epoch 7609/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3867\n",
            "Epoch 7610/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3810\n",
            "Epoch 7611/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3823\n",
            "Epoch 7612/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3849\n",
            "Epoch 7613/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3859\n",
            "Epoch 7614/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3831\n",
            "Epoch 7615/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3867\n",
            "Epoch 7616/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3862\n",
            "Epoch 7617/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3814\n",
            "Epoch 7618/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3830\n",
            "Epoch 7619/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3854\n",
            "Epoch 7620/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3847\n",
            "Epoch 7621/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3843\n",
            "Epoch 7622/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3813\n",
            "Epoch 7623/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3800\n",
            "Epoch 7624/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3834\n",
            "Epoch 7625/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3812\n",
            "Epoch 7626/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3828\n",
            "Epoch 7627/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3808\n",
            "Epoch 7628/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3804\n",
            "Epoch 7629/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3814\n",
            "Epoch 7630/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3803\n",
            "Epoch 7631/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3848\n",
            "Epoch 7632/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3823\n",
            "Epoch 7633/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3816\n",
            "Epoch 7634/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3832\n",
            "Epoch 7635/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3791\n",
            "Epoch 7636/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3825\n",
            "Epoch 7637/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3833\n",
            "Epoch 7638/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3800\n",
            "Epoch 7639/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3817\n",
            "Epoch 7640/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3814\n",
            "Epoch 7641/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3790\n",
            "Epoch 7642/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3801\n",
            "Epoch 7643/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3797\n",
            "Epoch 7644/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3804\n",
            "Epoch 7645/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3786\n",
            "Epoch 7646/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3781\n",
            "Epoch 7647/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3833\n",
            "Epoch 7648/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3790\n",
            "Epoch 7649/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3790\n",
            "Epoch 7650/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3739\n",
            "Epoch 7651/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3739\n",
            "Epoch 7652/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3734\n",
            "Epoch 7653/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3725\n",
            "Epoch 7654/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3734\n",
            "Epoch 7655/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3742\n",
            "Epoch 7656/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3749\n",
            "Epoch 7657/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3762\n",
            "Epoch 7658/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3745\n",
            "Epoch 7659/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3733\n",
            "Epoch 7660/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3752\n",
            "Epoch 7661/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3749\n",
            "Epoch 7662/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3746\n",
            "Epoch 7663/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3720\n",
            "Epoch 7664/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3762\n",
            "Epoch 7665/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3736\n",
            "Epoch 7666/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3756\n",
            "Epoch 7667/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3761\n",
            "Epoch 7668/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3725\n",
            "Epoch 7669/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3716\n",
            "Epoch 7670/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3726\n",
            "Epoch 7671/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3696\n",
            "Epoch 7672/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3721\n",
            "Epoch 7673/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3735\n",
            "Epoch 7674/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3708\n",
            "Epoch 7675/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3718\n",
            "Epoch 7676/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3750\n",
            "Epoch 7677/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3743\n",
            "Epoch 7678/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3725\n",
            "Epoch 7679/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3708\n",
            "Epoch 7680/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3729\n",
            "Epoch 7681/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3725\n",
            "Epoch 7682/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3734\n",
            "Epoch 7683/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3717\n",
            "Epoch 7684/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3722\n",
            "Epoch 7685/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3711\n",
            "Epoch 7686/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3696\n",
            "Epoch 7687/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3755\n",
            "Epoch 7688/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3717\n",
            "Epoch 7689/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3744\n",
            "Epoch 7690/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3693\n",
            "Epoch 7691/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3734\n",
            "Epoch 7692/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3716\n",
            "Epoch 7693/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3694\n",
            "Epoch 7694/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3710\n",
            "Epoch 7695/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3696\n",
            "Epoch 7696/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3680\n",
            "Epoch 7697/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3722\n",
            "Epoch 7698/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3700\n",
            "Epoch 7699/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3711\n",
            "Epoch 7700/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3673\n",
            "Epoch 7701/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3707\n",
            "Epoch 7702/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3686\n",
            "Epoch 7703/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3681\n",
            "Epoch 7704/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3711\n",
            "Epoch 7705/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3736\n",
            "Epoch 7706/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3720\n",
            "Epoch 7707/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3681\n",
            "Epoch 7708/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3701\n",
            "Epoch 7709/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3700\n",
            "Epoch 7710/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3730\n",
            "Epoch 7711/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3722\n",
            "Epoch 7712/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3680\n",
            "Epoch 7713/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3694\n",
            "Epoch 7714/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3695\n",
            "Epoch 7715/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3674\n",
            "Epoch 7716/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3679\n",
            "Epoch 7717/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3676\n",
            "Epoch 7718/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3671\n",
            "Epoch 7719/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3680\n",
            "Epoch 7720/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3700\n",
            "Epoch 7721/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3664\n",
            "Epoch 7722/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3680\n",
            "Epoch 7723/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3674\n",
            "Epoch 7724/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3662\n",
            "Epoch 7725/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3662\n",
            "Epoch 7726/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3672\n",
            "Epoch 7727/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3701\n",
            "Epoch 7728/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3685\n",
            "Epoch 7729/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3664\n",
            "Epoch 7730/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3692\n",
            "Epoch 7731/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3662\n",
            "Epoch 7732/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3690\n",
            "Epoch 7733/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3648\n",
            "Epoch 7734/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3688\n",
            "Epoch 7735/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3668\n",
            "Epoch 7736/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3656\n",
            "Epoch 7737/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3693\n",
            "Epoch 7738/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3653\n",
            "Epoch 7739/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3641\n",
            "Epoch 7740/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3692\n",
            "Epoch 7741/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3643\n",
            "Epoch 7742/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3666\n",
            "Epoch 7743/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3647\n",
            "Epoch 7744/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3642\n",
            "Epoch 7745/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3636\n",
            "Epoch 7746/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3670\n",
            "Epoch 7747/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3631\n",
            "Epoch 7748/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3689\n",
            "Epoch 7749/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3650\n",
            "Epoch 7750/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3653\n",
            "Epoch 7751/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3630\n",
            "Epoch 7752/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3662\n",
            "Epoch 7753/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3618\n",
            "Epoch 7754/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3610\n",
            "Epoch 7755/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3650\n",
            "Epoch 7756/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3636\n",
            "Epoch 7757/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3657\n",
            "Epoch 7758/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3634\n",
            "Epoch 7759/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3664\n",
            "Epoch 7760/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3629\n",
            "Epoch 7761/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3678\n",
            "Epoch 7762/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3630\n",
            "Epoch 7763/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3639\n",
            "Epoch 7764/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3622\n",
            "Epoch 7765/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3646\n",
            "Epoch 7766/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3642\n",
            "Epoch 7767/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3665\n",
            "Epoch 7768/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3612\n",
            "Epoch 7769/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3619\n",
            "Epoch 7770/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3653\n",
            "Epoch 7771/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3607\n",
            "Epoch 7772/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3639\n",
            "Epoch 7773/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3612\n",
            "Epoch 7774/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3625\n",
            "Epoch 7775/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3614\n",
            "Epoch 7776/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3602\n",
            "Epoch 7777/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3638\n",
            "Epoch 7778/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3607\n",
            "Epoch 7779/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3639\n",
            "Epoch 7780/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3634\n",
            "Epoch 7781/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3625\n",
            "Epoch 7782/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3608\n",
            "Epoch 7783/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3593\n",
            "Epoch 7784/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3617\n",
            "Epoch 7785/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3606\n",
            "Epoch 7786/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3608\n",
            "Epoch 7787/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3615\n",
            "Epoch 7788/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3633\n",
            "Epoch 7789/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3586\n",
            "Epoch 7790/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3603\n",
            "Epoch 7791/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3594\n",
            "Epoch 7792/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3601\n",
            "Epoch 7793/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3619\n",
            "Epoch 7794/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3596\n",
            "Epoch 7795/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3602\n",
            "Epoch 7796/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3628\n",
            "Epoch 7797/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3616\n",
            "Epoch 7798/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3591\n",
            "Epoch 7799/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3597\n",
            "Epoch 7800/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.3612\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-7799.h5\n",
            "prompt: ry is the\n",
            "eldest, he was named after me, not after his father. John, the second,\n",
            "is named after his father. Some people are surprized, I believe, that\n",
            "the eldest was not, but Isabella would have him called Henry, which I\n",
            "thought very pretty of her. And he is a very clever boy, indeed. They\n",
            "a\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "lmost not ficeverte<span style=\"background-color:#eadbd8;\">nship and </span><span style=\"background-color:#e2d7d5;\">young man, n</span><span style=\"background-color:#e2d7d5;\">ot differen</span>ce, w<span style=\"background-color:#d4efdf;\">hat<br>Marianne was </span><span style=\"background-color:#e2d7d5;\">convenient p</span><span style=\"background-color:#d0ece7;\">ossession of her<br></span>really<span style=\"background-color:#ebdef0;\">, not in the </span>whol<span style=\"background-color:#d4e6f1;\">dmund, and </span><span style=\"background-color:#d6eaf8;\">they were made</span><span style=\"background-color:#eadbd8;\"> for having s</span>aw Edmur<span style=\"background-color:#e2d7d5;\">ch her mother </span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 873ms/step - loss: 0.3613\n",
            "Epoch 7801/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3565\n",
            "Epoch 7802/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3615\n",
            "Epoch 7803/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3600\n",
            "Epoch 7804/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3616\n",
            "Epoch 7805/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3589\n",
            "Epoch 7806/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3602\n",
            "Epoch 7807/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3602\n",
            "Epoch 7808/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3594\n",
            "Epoch 7809/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3605\n",
            "Epoch 7810/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3586\n",
            "Epoch 7811/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3589\n",
            "Epoch 7812/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3580\n",
            "Epoch 7813/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3566\n",
            "Epoch 7814/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3586\n",
            "Epoch 7815/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3579\n",
            "Epoch 7816/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3586\n",
            "Epoch 7817/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3570\n",
            "Epoch 7818/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3599\n",
            "Epoch 7819/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3582\n",
            "Epoch 7820/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3589\n",
            "Epoch 7821/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3602\n",
            "Epoch 7822/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3553\n",
            "Epoch 7823/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3601\n",
            "Epoch 7824/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3581\n",
            "Epoch 7825/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3545\n",
            "Epoch 7826/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3561\n",
            "Epoch 7827/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3553\n",
            "Epoch 7828/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3585\n",
            "Epoch 7829/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3595\n",
            "Epoch 7830/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3547\n",
            "Epoch 7831/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3570\n",
            "Epoch 7832/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3562\n",
            "Epoch 7833/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3593\n",
            "Epoch 7834/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3562\n",
            "Epoch 7835/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3568\n",
            "Epoch 7836/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3569\n",
            "Epoch 7837/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3546\n",
            "Epoch 7838/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3569\n",
            "Epoch 7839/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3558\n",
            "Epoch 7840/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3577\n",
            "Epoch 7841/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3553\n",
            "Epoch 7842/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3571\n",
            "Epoch 7843/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3524\n",
            "Epoch 7844/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3569\n",
            "Epoch 7845/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3544\n",
            "Epoch 7846/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3570\n",
            "Epoch 7847/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3561\n",
            "Epoch 7848/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3526\n",
            "Epoch 7849/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3560\n",
            "Epoch 7850/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3531\n",
            "Epoch 7851/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3554\n",
            "Epoch 7852/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3562\n",
            "Epoch 7853/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3543\n",
            "Epoch 7854/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3562\n",
            "Epoch 7855/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3535\n",
            "Epoch 7856/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3524\n",
            "Epoch 7857/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3538\n",
            "Epoch 7858/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3543\n",
            "Epoch 7859/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3572\n",
            "Epoch 7860/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3530\n",
            "Epoch 7861/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3538\n",
            "Epoch 7862/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3545\n",
            "Epoch 7863/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3502\n",
            "Epoch 7864/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3551\n",
            "Epoch 7865/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3521\n",
            "Epoch 7866/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3509\n",
            "Epoch 7867/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3570\n",
            "Epoch 7868/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3543\n",
            "Epoch 7869/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3524\n",
            "Epoch 7870/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3535\n",
            "Epoch 7871/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3533\n",
            "Epoch 7872/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3533\n",
            "Epoch 7873/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3523\n",
            "Epoch 7874/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3497\n",
            "Epoch 7875/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3510\n",
            "Epoch 7876/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3534\n",
            "Epoch 7877/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3514\n",
            "Epoch 7878/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3542\n",
            "Epoch 7879/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3532\n",
            "Epoch 7880/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3553\n",
            "Epoch 7881/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3557\n",
            "Epoch 7882/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3513\n",
            "Epoch 7883/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3535\n",
            "Epoch 7884/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3515\n",
            "Epoch 7885/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3545\n",
            "Epoch 7886/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3523\n",
            "Epoch 7887/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3526\n",
            "Epoch 7888/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3509\n",
            "Epoch 7889/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3540\n",
            "Epoch 7890/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3526\n",
            "Epoch 7891/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3529\n",
            "Epoch 7892/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3506\n",
            "Epoch 7893/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3522\n",
            "Epoch 7894/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3498\n",
            "Epoch 7895/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3497\n",
            "Epoch 7896/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3523\n",
            "Epoch 7897/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3491\n",
            "Epoch 7898/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3511\n",
            "Epoch 7899/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3483\n",
            "Epoch 7900/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3509\n",
            "Epoch 7901/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3505\n",
            "Epoch 7902/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3489\n",
            "Epoch 7903/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3494\n",
            "Epoch 7904/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3509\n",
            "Epoch 7905/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3495\n",
            "Epoch 7906/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3503\n",
            "Epoch 7907/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3490\n",
            "Epoch 7908/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3523\n",
            "Epoch 7909/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3456\n",
            "Epoch 7910/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3434\n",
            "Epoch 7911/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3455\n",
            "Epoch 7912/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3442\n",
            "Epoch 7913/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3442\n",
            "Epoch 7914/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3478\n",
            "Epoch 7915/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3471\n",
            "Epoch 7916/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3465\n",
            "Epoch 7917/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3462\n",
            "Epoch 7918/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3470\n",
            "Epoch 7919/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3450\n",
            "Epoch 7920/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3505\n",
            "Epoch 7921/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3462\n",
            "Epoch 7922/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3462\n",
            "Epoch 7923/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3454\n",
            "Epoch 7924/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3470\n",
            "Epoch 7925/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3450\n",
            "Epoch 7926/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3484\n",
            "Epoch 7927/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3448\n",
            "Epoch 7928/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3418\n",
            "Epoch 7929/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3452\n",
            "Epoch 7930/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3447\n",
            "Epoch 7931/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3456\n",
            "Epoch 7932/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3478\n",
            "Epoch 7933/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3472\n",
            "Epoch 7934/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3461\n",
            "Epoch 7935/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3477\n",
            "Epoch 7936/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3448\n",
            "Epoch 7937/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3435\n",
            "Epoch 7938/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3437\n",
            "Epoch 7939/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3456\n",
            "Epoch 7940/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3450\n",
            "Epoch 7941/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3427\n",
            "Epoch 7942/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3451\n",
            "Epoch 7943/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3434\n",
            "Epoch 7944/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3441\n",
            "Epoch 7945/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3461\n",
            "Epoch 7946/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3437\n",
            "Epoch 7947/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3439\n",
            "Epoch 7948/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3407\n",
            "Epoch 7949/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3427\n",
            "Epoch 7950/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3422\n",
            "Epoch 7951/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3438\n",
            "Epoch 7952/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3446\n",
            "Epoch 7953/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3469\n",
            "Epoch 7954/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3440\n",
            "Epoch 7955/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3417\n",
            "Epoch 7956/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3419\n",
            "Epoch 7957/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3454\n",
            "Epoch 7958/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3453\n",
            "Epoch 7959/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3427\n",
            "Epoch 7960/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3430\n",
            "Epoch 7961/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3437\n",
            "Epoch 7962/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3428\n",
            "Epoch 7963/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3409\n",
            "Epoch 7964/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3465\n",
            "Epoch 7965/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3421\n",
            "Epoch 7966/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3426\n",
            "Epoch 7967/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3407\n",
            "Epoch 7968/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3425\n",
            "Epoch 7969/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3436\n",
            "Epoch 7970/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3448\n",
            "Epoch 7971/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3434\n",
            "Epoch 7972/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3409\n",
            "Epoch 7973/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 0.3451\n",
            "Epoch 7974/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3421\n",
            "Epoch 7975/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3444\n",
            "Epoch 7976/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3431\n",
            "Epoch 7977/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3435\n",
            "Epoch 7978/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3443\n",
            "Epoch 7979/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3421\n",
            "Epoch 7980/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3404\n",
            "Epoch 7981/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3421\n",
            "Epoch 7982/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3434\n",
            "Epoch 7983/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3394\n",
            "Epoch 7984/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3468\n",
            "Epoch 7985/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3418\n",
            "Epoch 7986/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3424\n",
            "Epoch 7987/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3426\n",
            "Epoch 7988/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3415\n",
            "Epoch 7989/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3442\n",
            "Epoch 7990/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3415\n",
            "Epoch 7991/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3418\n",
            "Epoch 7992/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3437\n",
            "Epoch 7993/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3379\n",
            "Epoch 7994/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3427\n",
            "Epoch 7995/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3429\n",
            "Epoch 7996/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3391\n",
            "Epoch 7997/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3442\n",
            "Epoch 7998/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3422\n",
            "Epoch 7999/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3408\n",
            "Epoch 8000/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.3417\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-7999.h5\n",
            "prompt: riage after the said dinner\n",
            "visit; “they are very elegant, agreeable girls.”\n",
            "\n",
            "“So they are indeed, and I am delighted to hear you say it. But you\n",
            "like Julia best.”\n",
            "\n",
            "“Oh yes! I like Julia best.”\n",
            "\n",
            "“But do you really? for Miss Bertram is in general thought the\n",
            "handsome\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:#d0ece7;\">—we shall </span><span style=\"background-color:#e2d7d5;\">be on her w</span>ith<br>still uneasing<span style=\"background-color:#ecf3cf;\">—have been </span><span style=\"background-color:#e2d7d5;\">on the high </span>under<br>y<span style=\"background-color:#eadbd8;\">e. I am very </span><span style=\"background-color:#ecf3cf;\">little more a</span><span style=\"background-color:#d4efdf;\">nd my mother </span><span style=\"background-color:#d0ece7;\">would have been all t</span>o<br>dom who<span style=\"background-color:#d4efdf;\">se being a</span><span style=\"background-color:#d4efdf;\">nd his home</span>—Otwant-<span style=\"background-color:#d8daef;\">room is a </span>pla"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 878ms/step - loss: 0.3415\n",
            "Epoch 8001/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3394\n",
            "Epoch 8002/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3414\n",
            "Epoch 8003/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3424\n",
            "Epoch 8004/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3404\n",
            "Epoch 8005/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3399\n",
            "Epoch 8006/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3420\n",
            "Epoch 8007/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3424\n",
            "Epoch 8008/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3383\n",
            "Epoch 8009/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3407\n",
            "Epoch 8010/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3438\n",
            "Epoch 8011/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3407\n",
            "Epoch 8012/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3371\n",
            "Epoch 8013/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3398\n",
            "Epoch 8014/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3435\n",
            "Epoch 8015/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3381\n",
            "Epoch 8016/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3394\n",
            "Epoch 8017/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3418\n",
            "Epoch 8018/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3412\n",
            "Epoch 8019/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3378\n",
            "Epoch 8020/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3404\n",
            "Epoch 8021/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3393\n",
            "Epoch 8022/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 0.3399\n",
            "Epoch 8023/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3393\n",
            "Epoch 8024/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3392\n",
            "Epoch 8025/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3389\n",
            "Epoch 8026/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3412\n",
            "Epoch 8027/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3399\n",
            "Epoch 8028/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3394\n",
            "Epoch 8029/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3394\n",
            "Epoch 8030/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3419\n",
            "Epoch 8031/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3388\n",
            "Epoch 8032/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3413\n",
            "Epoch 8033/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3408\n",
            "Epoch 8034/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3380\n",
            "Epoch 8035/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3392\n",
            "Epoch 8036/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3371\n",
            "Epoch 8037/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3401\n",
            "Epoch 8038/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3394\n",
            "Epoch 8039/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3404\n",
            "Epoch 8040/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3418\n",
            "Epoch 8041/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3394\n",
            "Epoch 8042/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3394\n",
            "Epoch 8043/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3372\n",
            "Epoch 8044/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3377\n",
            "Epoch 8045/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3364\n",
            "Epoch 8046/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3377\n",
            "Epoch 8047/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3385\n",
            "Epoch 8048/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3370\n",
            "Epoch 8049/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3367\n",
            "Epoch 8050/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3370\n",
            "Epoch 8051/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3378\n",
            "Epoch 8052/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3361\n",
            "Epoch 8053/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3348\n",
            "Epoch 8054/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3378\n",
            "Epoch 8055/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3380\n",
            "Epoch 8056/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3350\n",
            "Epoch 8057/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3391\n",
            "Epoch 8058/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3366\n",
            "Epoch 8059/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3375\n",
            "Epoch 8060/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3354\n",
            "Epoch 8061/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3387\n",
            "Epoch 8062/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3368\n",
            "Epoch 8063/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3385\n",
            "Epoch 8064/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3382\n",
            "Epoch 8065/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3359\n",
            "Epoch 8066/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3356\n",
            "Epoch 8067/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3373\n",
            "Epoch 8068/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3379\n",
            "Epoch 8069/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3345\n",
            "Epoch 8070/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3378\n",
            "Epoch 8071/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3392\n",
            "Epoch 8072/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3337\n",
            "Epoch 8073/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3346\n",
            "Epoch 8074/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3359\n",
            "Epoch 8075/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3358\n",
            "Epoch 8076/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3377\n",
            "Epoch 8077/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3359\n",
            "Epoch 8078/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3353\n",
            "Epoch 8079/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3353\n",
            "Epoch 8080/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3344\n",
            "Epoch 8081/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3374\n",
            "Epoch 8082/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3363\n",
            "Epoch 8083/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3376\n",
            "Epoch 8084/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3341\n",
            "Epoch 8085/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3326\n",
            "Epoch 8086/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3357\n",
            "Epoch 8087/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3364\n",
            "Epoch 8088/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3362\n",
            "Epoch 8089/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3369\n",
            "Epoch 8090/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3356\n",
            "Epoch 8091/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3334\n",
            "Epoch 8092/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3356\n",
            "Epoch 8093/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3358\n",
            "Epoch 8094/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3367\n",
            "Epoch 8095/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3384\n",
            "Epoch 8096/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3356\n",
            "Epoch 8097/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3353\n",
            "Epoch 8098/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3336\n",
            "Epoch 8099/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3347\n",
            "Epoch 8100/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3358\n",
            "Epoch 8101/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3326\n",
            "Epoch 8102/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3364\n",
            "Epoch 8103/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3349\n",
            "Epoch 8104/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3338\n",
            "Epoch 8105/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3348\n",
            "Epoch 8106/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3355\n",
            "Epoch 8107/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3354\n",
            "Epoch 8108/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3341\n",
            "Epoch 8109/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3335\n",
            "Epoch 8110/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3362\n",
            "Epoch 8111/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3328\n",
            "Epoch 8112/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3330\n",
            "Epoch 8113/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3337\n",
            "Epoch 8114/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3330\n",
            "Epoch 8115/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3327\n",
            "Epoch 8116/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3389\n",
            "Epoch 8117/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3332\n",
            "Epoch 8118/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3332\n",
            "Epoch 8119/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3360\n",
            "Epoch 8120/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3338\n",
            "Epoch 8121/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3314\n",
            "Epoch 8122/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3333\n",
            "Epoch 8123/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3360\n",
            "Epoch 8124/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3322\n",
            "Epoch 8125/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3320\n",
            "Epoch 8126/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3329\n",
            "Epoch 8127/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3350\n",
            "Epoch 8128/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3344\n",
            "Epoch 8129/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3347\n",
            "Epoch 8130/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3347\n",
            "Epoch 8131/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3337\n",
            "Epoch 8132/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3320\n",
            "Epoch 8133/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3317\n",
            "Epoch 8134/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3331\n",
            "Epoch 8135/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3333\n",
            "Epoch 8136/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3328\n",
            "Epoch 8137/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3351\n",
            "Epoch 8138/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3303\n",
            "Epoch 8139/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3314\n",
            "Epoch 8140/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3288\n",
            "Epoch 8141/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3305\n",
            "Epoch 8142/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3331\n",
            "Epoch 8143/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3308\n",
            "Epoch 8144/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3327\n",
            "Epoch 8145/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3325\n",
            "Epoch 8146/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3297\n",
            "Epoch 8147/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3329\n",
            "Epoch 8148/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3329\n",
            "Epoch 8149/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3327\n",
            "Epoch 8150/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3325\n",
            "Epoch 8151/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3329\n",
            "Epoch 8152/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3317\n",
            "Epoch 8153/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3314\n",
            "Epoch 8154/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3318\n",
            "Epoch 8155/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3316\n",
            "Epoch 8156/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3322\n",
            "Epoch 8157/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3313\n",
            "Epoch 8158/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3307\n",
            "Epoch 8159/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3307\n",
            "Epoch 8160/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3312\n",
            "Epoch 8161/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3323\n",
            "Epoch 8162/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3310\n",
            "Epoch 8163/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3323\n",
            "Epoch 8164/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3341\n",
            "Epoch 8165/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3329\n",
            "Epoch 8166/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3292\n",
            "Epoch 8167/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3251\n",
            "Epoch 8168/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3248\n",
            "Epoch 8169/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3237\n",
            "Epoch 8170/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3234\n",
            "Epoch 8171/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3248\n",
            "Epoch 8172/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3270\n",
            "Epoch 8173/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3250\n",
            "Epoch 8174/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3267\n",
            "Epoch 8175/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3295\n",
            "Epoch 8176/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3275\n",
            "Epoch 8177/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3275\n",
            "Epoch 8178/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3290\n",
            "Epoch 8179/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3253\n",
            "Epoch 8180/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3264\n",
            "Epoch 8181/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3280\n",
            "Epoch 8182/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3274\n",
            "Epoch 8183/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3269\n",
            "Epoch 8184/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3250\n",
            "Epoch 8185/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3278\n",
            "Epoch 8186/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3280\n",
            "Epoch 8187/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3262\n",
            "Epoch 8188/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3280\n",
            "Epoch 8189/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3285\n",
            "Epoch 8190/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3273\n",
            "Epoch 8191/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3250\n",
            "Epoch 8192/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3273\n",
            "Epoch 8193/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3228\n",
            "Epoch 8194/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3267\n",
            "Epoch 8195/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3259\n",
            "Epoch 8196/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3272\n",
            "Epoch 8197/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3250\n",
            "Epoch 8198/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3261\n",
            "Epoch 8199/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3260\n",
            "Epoch 8200/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.3282\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-8199.h5\n",
            "prompt: n excellent wife all the time you were at Hartfield. You might\n",
            "not give Emma such a complete education as your powers would seem to\n",
            "promise; but you were receiving a very good education from _her_, on\n",
            "the very material matrimonial point of submitting your own will, and\n",
            "\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:#d0ece7;\">what you to</span> leading <span style=\"background-color:#d0ece7;\">us affection</span><span style=\"background-color:#eadbd8;\">s of them </span><span style=\"background-color:#ebdef0;\">of those w</span>as t<span style=\"background-color:#d6dbdf;\">he<br>has been </span><span style=\"background-color:#e2d7d5;\">such an impo</span>r<span style=\"background-color:#d6eaf8;\">.”<br><br>Helen </span>d<span style=\"background-color:#eadbd8;\">id too much </span><span style=\"background-color:#d6eaf8;\">which they re</span><span style=\"background-color:#e2d7d5;\">mained, and</span><span style=\"background-color:#d4e6f1;\"><br>when they had </span>mad<span style=\"background-color:#ecf3cf;\">e him what </span>is y<span style=\"background-color:#e2d7d5;\">ears upon </span>it<span style=\"background-color:#d4e6f1;\">, and<br>Fanny</span><span style=\"background-color:#d4e6f1;\">, Fanny’s </span>eye. O"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d6dbdf;\">Jane Austen: Northanger Abbey</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d6eaf8;\">Virginia Woolf: The Voyage Out</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 884ms/step - loss: 0.3280\n",
            "Epoch 8201/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3264\n",
            "Epoch 8202/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3290\n",
            "Epoch 8203/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3251\n",
            "Epoch 8204/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3249\n",
            "Epoch 8205/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3264\n",
            "Epoch 8206/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3285\n",
            "Epoch 8207/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3267\n",
            "Epoch 8208/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3271\n",
            "Epoch 8209/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3258\n",
            "Epoch 8210/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3271\n",
            "Epoch 8211/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3244\n",
            "Epoch 8212/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3266\n",
            "Epoch 8213/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3232\n",
            "Epoch 8214/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3262\n",
            "Epoch 8215/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3249\n",
            "Epoch 8216/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3297\n",
            "Epoch 8217/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3255\n",
            "Epoch 8218/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3273\n",
            "Epoch 8219/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3252\n",
            "Epoch 8220/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3286\n",
            "Epoch 8221/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3265\n",
            "Epoch 8222/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3269\n",
            "Epoch 8223/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3275\n",
            "Epoch 8224/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3274\n",
            "Epoch 8225/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3263\n",
            "Epoch 8226/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3272\n",
            "Epoch 8227/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3225\n",
            "Epoch 8228/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3254\n",
            "Epoch 8229/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3260\n",
            "Epoch 8230/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3250\n",
            "Epoch 8231/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3263\n",
            "Epoch 8232/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3236\n",
            "Epoch 8233/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3257\n",
            "Epoch 8234/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3280\n",
            "Epoch 8235/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3231\n",
            "Epoch 8236/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3242\n",
            "Epoch 8237/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3267\n",
            "Epoch 8238/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3250\n",
            "Epoch 8239/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3276\n",
            "Epoch 8240/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3215\n",
            "Epoch 8241/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3263\n",
            "Epoch 8242/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3224\n",
            "Epoch 8243/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3245\n",
            "Epoch 8244/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3229\n",
            "Epoch 8245/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3270\n",
            "Epoch 8246/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3223\n",
            "Epoch 8247/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3245\n",
            "Epoch 8248/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3246\n",
            "Epoch 8249/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3245\n",
            "Epoch 8250/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3244\n",
            "Epoch 8251/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3254\n",
            "Epoch 8252/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3250\n",
            "Epoch 8253/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3240\n",
            "Epoch 8254/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3270\n",
            "Epoch 8255/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3223\n",
            "Epoch 8256/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3249\n",
            "Epoch 8257/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3238\n",
            "Epoch 8258/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3242\n",
            "Epoch 8259/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3238\n",
            "Epoch 8260/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3246\n",
            "Epoch 8261/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3246\n",
            "Epoch 8262/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3249\n",
            "Epoch 8263/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3257\n",
            "Epoch 8264/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3261\n",
            "Epoch 8265/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3243\n",
            "Epoch 8266/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3235\n",
            "Epoch 8267/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3249\n",
            "Epoch 8268/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3269\n",
            "Epoch 8269/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3225\n",
            "Epoch 8270/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3219\n",
            "Epoch 8271/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3262\n",
            "Epoch 8272/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3270\n",
            "Epoch 8273/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3228\n",
            "Epoch 8274/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3238\n",
            "Epoch 8275/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3214\n",
            "Epoch 8276/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3229\n",
            "Epoch 8277/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3226\n",
            "Epoch 8278/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3246\n",
            "Epoch 8279/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3217\n",
            "Epoch 8280/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3238\n",
            "Epoch 8281/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3227\n",
            "Epoch 8282/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3220\n",
            "Epoch 8283/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3222\n",
            "Epoch 8284/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3217\n",
            "Epoch 8285/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3231\n",
            "Epoch 8286/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3233\n",
            "Epoch 8287/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3241\n",
            "Epoch 8288/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3226\n",
            "Epoch 8289/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3229\n",
            "Epoch 8290/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3250\n",
            "Epoch 8291/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3236\n",
            "Epoch 8292/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3236\n",
            "Epoch 8293/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3241\n",
            "Epoch 8294/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3246\n",
            "Epoch 8295/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3240\n",
            "Epoch 8296/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3240\n",
            "Epoch 8297/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3214\n",
            "Epoch 8298/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3217\n",
            "Epoch 8299/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3241\n",
            "Epoch 8300/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3233\n",
            "Epoch 8301/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3226\n",
            "Epoch 8302/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3207\n",
            "Epoch 8303/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3207\n",
            "Epoch 8304/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3228\n",
            "Epoch 8305/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3248\n",
            "Epoch 8306/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3211\n",
            "Epoch 8307/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3217\n",
            "Epoch 8308/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3236\n",
            "Epoch 8309/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3221\n",
            "Epoch 8310/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3232\n",
            "Epoch 8311/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3220\n",
            "Epoch 8312/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3207\n",
            "Epoch 8313/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3216\n",
            "Epoch 8314/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3224\n",
            "Epoch 8315/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3219\n",
            "Epoch 8316/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3215\n",
            "Epoch 8317/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3230\n",
            "Epoch 8318/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3225\n",
            "Epoch 8319/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3213\n",
            "Epoch 8320/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3215\n",
            "Epoch 8321/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3217\n",
            "Epoch 8322/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3206\n",
            "Epoch 8323/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3218\n",
            "Epoch 8324/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3211\n",
            "Epoch 8325/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3200\n",
            "Epoch 8326/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3224\n",
            "Epoch 8327/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3203\n",
            "Epoch 8328/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3229\n",
            "Epoch 8329/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3197\n",
            "Epoch 8330/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3207\n",
            "Epoch 8331/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3204\n",
            "Epoch 8332/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3225\n",
            "Epoch 8333/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3205\n",
            "Epoch 8334/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3211\n",
            "Epoch 8335/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3195\n",
            "Epoch 8336/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3193\n",
            "Epoch 8337/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3213\n",
            "Epoch 8338/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3196\n",
            "Epoch 8339/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3201\n",
            "Epoch 8340/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3165\n",
            "Epoch 8341/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3194\n",
            "Epoch 8342/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3199\n",
            "Epoch 8343/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3222\n",
            "Epoch 8344/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3213\n",
            "Epoch 8345/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3216\n",
            "Epoch 8346/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3200\n",
            "Epoch 8347/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3190\n",
            "Epoch 8348/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3210\n",
            "Epoch 8349/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3222\n",
            "Epoch 8350/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3222\n",
            "Epoch 8351/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3204\n",
            "Epoch 8352/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3201\n",
            "Epoch 8353/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3198\n",
            "Epoch 8354/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3213\n",
            "Epoch 8355/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3179\n",
            "Epoch 8356/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3204\n",
            "Epoch 8357/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3227\n",
            "Epoch 8358/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3190\n",
            "Epoch 8359/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3207\n",
            "Epoch 8360/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3195\n",
            "Epoch 8361/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3194\n",
            "Epoch 8362/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3210\n",
            "Epoch 8363/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3214\n",
            "Epoch 8364/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3181\n",
            "Epoch 8365/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3186\n",
            "Epoch 8366/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3198\n",
            "Epoch 8367/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3192\n",
            "Epoch 8368/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3187\n",
            "Epoch 8369/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3191\n",
            "Epoch 8370/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3175\n",
            "Epoch 8371/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3216\n",
            "Epoch 8372/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3209\n",
            "Epoch 8373/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3176\n",
            "Epoch 8374/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3208\n",
            "Epoch 8375/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3194\n",
            "Epoch 8376/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3192\n",
            "Epoch 8377/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3187\n",
            "Epoch 8378/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3196\n",
            "Epoch 8379/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3205\n",
            "Epoch 8380/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3189\n",
            "Epoch 8381/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3176\n",
            "Epoch 8382/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3186\n",
            "Epoch 8383/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3212\n",
            "Epoch 8384/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3207\n",
            "Epoch 8385/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3198\n",
            "Epoch 8386/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3181\n",
            "Epoch 8387/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3173\n",
            "Epoch 8388/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3201\n",
            "Epoch 8389/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3182\n",
            "Epoch 8390/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3201\n",
            "Epoch 8391/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3182\n",
            "Epoch 8392/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3200\n",
            "Epoch 8393/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3170\n",
            "Epoch 8394/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3184\n",
            "Epoch 8395/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3190\n",
            "Epoch 8396/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3169\n",
            "Epoch 8397/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3184\n",
            "Epoch 8398/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3191\n",
            "Epoch 8399/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3184\n",
            "Epoch 8400/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.3175\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-8399.h5\n",
            "prompt: on’t be named a dog in\n",
            "the manger again. You _shall_ stay: now then! Heathcliff, why don’t you\n",
            "evince satisfaction at my pleasant news? Isabella swears that the love\n",
            "Edgar has for me is nothing to that she entertains for you. I’m sure\n",
            "she made some speech of \n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "my descia<span style=\"background-color:#e2d7d5;\">ted to<br>feel</span><span style=\"background-color:#eadbd8;\">ing for me, </span>she was I know<span style=\"background-color:#d0ece7;\">s soon comp</span>any<br>means. She<span style=\"background-color:#e2d7d5;\">’m certain</span><span style=\"background-color:#eadbd8;\">ly known, </span><span style=\"background-color:#e2d7d5;\">silence for </span>you.<span style=\"background-color:#d8daef;\"><br>But if I ha</span>ve a words I seated. Hen—<span style=\"background-color:#d4e6f1;\">you<br>but to </span>sle"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 891ms/step - loss: 0.3173\n",
            "Epoch 8401/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3206\n",
            "Epoch 8402/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3180\n",
            "Epoch 8403/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3164\n",
            "Epoch 8404/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3166\n",
            "Epoch 8405/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3190\n",
            "Epoch 8406/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3197\n",
            "Epoch 8407/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3169\n",
            "Epoch 8408/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3189\n",
            "Epoch 8409/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3170\n",
            "Epoch 8410/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3152\n",
            "Epoch 8411/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3164\n",
            "Epoch 8412/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3170\n",
            "Epoch 8413/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3190\n",
            "Epoch 8414/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3173\n",
            "Epoch 8415/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3172\n",
            "Epoch 8416/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3189\n",
            "Epoch 8417/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3203\n",
            "Epoch 8418/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3168\n",
            "Epoch 8419/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3193\n",
            "Epoch 8420/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3177\n",
            "Epoch 8421/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3172\n",
            "Epoch 8422/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3153\n",
            "Epoch 8423/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3149\n",
            "Epoch 8424/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3146\n",
            "Epoch 8425/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3172\n",
            "Epoch 8426/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3117\n",
            "Epoch 8427/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3117\n",
            "Epoch 8428/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3119\n",
            "Epoch 8429/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3102\n",
            "Epoch 8430/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3118\n",
            "Epoch 8431/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3134\n",
            "Epoch 8432/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3134\n",
            "Epoch 8433/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3128\n",
            "Epoch 8434/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3097\n",
            "Epoch 8435/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3118\n",
            "Epoch 8436/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3143\n",
            "Epoch 8437/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3139\n",
            "Epoch 8438/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3153\n",
            "Epoch 8439/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3122\n",
            "Epoch 8440/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3110\n",
            "Epoch 8441/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3117\n",
            "Epoch 8442/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3155\n",
            "Epoch 8443/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3149\n",
            "Epoch 8444/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3128\n",
            "Epoch 8445/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3127\n",
            "Epoch 8446/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3122\n",
            "Epoch 8447/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3140\n",
            "Epoch 8448/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3120\n",
            "Epoch 8449/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3149\n",
            "Epoch 8450/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3148\n",
            "Epoch 8451/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3117\n",
            "Epoch 8452/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3137\n",
            "Epoch 8453/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3153\n",
            "Epoch 8454/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3118\n",
            "Epoch 8455/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3146\n",
            "Epoch 8456/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3147\n",
            "Epoch 8457/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3091\n",
            "Epoch 8458/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3142\n",
            "Epoch 8459/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3116\n",
            "Epoch 8460/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3152\n",
            "Epoch 8461/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3157\n",
            "Epoch 8462/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3150\n",
            "Epoch 8463/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3144\n",
            "Epoch 8464/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3142\n",
            "Epoch 8465/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3110\n",
            "Epoch 8466/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3128\n",
            "Epoch 8467/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3132\n",
            "Epoch 8468/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3159\n",
            "Epoch 8469/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3129\n",
            "Epoch 8470/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 0.3128\n",
            "Epoch 8471/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3110\n",
            "Epoch 8472/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3106\n",
            "Epoch 8473/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3124\n",
            "Epoch 8474/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3136\n",
            "Epoch 8475/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3116\n",
            "Epoch 8476/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3132\n",
            "Epoch 8477/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3115\n",
            "Epoch 8478/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3152\n",
            "Epoch 8479/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3143\n",
            "Epoch 8480/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3112\n",
            "Epoch 8481/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3141\n",
            "Epoch 8482/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3113\n",
            "Epoch 8483/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3135\n",
            "Epoch 8484/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3152\n",
            "Epoch 8485/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3100\n",
            "Epoch 8486/500000\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 0.3154\n",
            "Epoch 8487/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3137\n",
            "Epoch 8488/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3114\n",
            "Epoch 8489/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3127\n",
            "Epoch 8490/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3116\n",
            "Epoch 8491/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3132\n",
            "Epoch 8492/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3143\n",
            "Epoch 8493/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3123\n",
            "Epoch 8494/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3116\n",
            "Epoch 8495/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3096\n",
            "Epoch 8496/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3097\n",
            "Epoch 8497/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3127\n",
            "Epoch 8498/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3131\n",
            "Epoch 8499/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3142\n",
            "Epoch 8500/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3097\n",
            "Epoch 8501/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3127\n",
            "Epoch 8502/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3149\n",
            "Epoch 8503/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3104\n",
            "Epoch 8504/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3098\n",
            "Epoch 8505/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3129\n",
            "Epoch 8506/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3135\n",
            "Epoch 8507/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3135\n",
            "Epoch 8508/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3107\n",
            "Epoch 8509/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3110\n",
            "Epoch 8510/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3108\n",
            "Epoch 8511/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3116\n",
            "Epoch 8512/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3099\n",
            "Epoch 8513/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3121\n",
            "Epoch 8514/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3128\n",
            "Epoch 8515/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3103\n",
            "Epoch 8516/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3117\n",
            "Epoch 8517/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3126\n",
            "Epoch 8518/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3103\n",
            "Epoch 8519/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3127\n",
            "Epoch 8520/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3117\n",
            "Epoch 8521/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3115\n",
            "Epoch 8522/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3146\n",
            "Epoch 8523/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3128\n",
            "Epoch 8524/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3099\n",
            "Epoch 8525/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3105\n",
            "Epoch 8526/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3096\n",
            "Epoch 8527/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3122\n",
            "Epoch 8528/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3130\n",
            "Epoch 8529/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3089\n",
            "Epoch 8530/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3116\n",
            "Epoch 8531/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3112\n",
            "Epoch 8532/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3110\n",
            "Epoch 8533/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3089\n",
            "Epoch 8534/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3129\n",
            "Epoch 8535/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3122\n",
            "Epoch 8536/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3110\n",
            "Epoch 8537/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3109\n",
            "Epoch 8538/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3110\n",
            "Epoch 8539/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3096\n",
            "Epoch 8540/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3096\n",
            "Epoch 8541/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3121\n",
            "Epoch 8542/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3078\n",
            "Epoch 8543/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3128\n",
            "Epoch 8544/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3114\n",
            "Epoch 8545/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3088\n",
            "Epoch 8546/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3095\n",
            "Epoch 8547/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3109\n",
            "Epoch 8548/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3118\n",
            "Epoch 8549/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3116\n",
            "Epoch 8550/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3143\n",
            "Epoch 8551/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3113\n",
            "Epoch 8552/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3123\n",
            "Epoch 8553/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3095\n",
            "Epoch 8554/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3099\n",
            "Epoch 8555/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3104\n",
            "Epoch 8556/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3116\n",
            "Epoch 8557/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3084\n",
            "Epoch 8558/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3102\n",
            "Epoch 8559/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3081\n",
            "Epoch 8560/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3132\n",
            "Epoch 8561/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3113\n",
            "Epoch 8562/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3090\n",
            "Epoch 8563/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3107\n",
            "Epoch 8564/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3110\n",
            "Epoch 8565/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3087\n",
            "Epoch 8566/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3114\n",
            "Epoch 8567/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3113\n",
            "Epoch 8568/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3081\n",
            "Epoch 8569/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3114\n",
            "Epoch 8570/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3128\n",
            "Epoch 8571/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3100\n",
            "Epoch 8572/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3116\n",
            "Epoch 8573/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3106\n",
            "Epoch 8574/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3112\n",
            "Epoch 8575/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3114\n",
            "Epoch 8576/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3099\n",
            "Epoch 8577/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3104\n",
            "Epoch 8578/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3094\n",
            "Epoch 8579/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3069\n",
            "Epoch 8580/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3107\n",
            "Epoch 8581/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3100\n",
            "Epoch 8582/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3092\n",
            "Epoch 8583/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3068\n",
            "Epoch 8584/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3093\n",
            "Epoch 8585/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3100\n",
            "Epoch 8586/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3119\n",
            "Epoch 8587/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3103\n",
            "Epoch 8588/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3097\n",
            "Epoch 8589/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3091\n",
            "Epoch 8590/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3109\n",
            "Epoch 8591/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3102\n",
            "Epoch 8592/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3083\n",
            "Epoch 8593/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3072\n",
            "Epoch 8594/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3088\n",
            "Epoch 8595/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3104\n",
            "Epoch 8596/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3062\n",
            "Epoch 8597/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3095\n",
            "Epoch 8598/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3091\n",
            "Epoch 8599/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3107\n",
            "Epoch 8600/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.3111\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-8599.h5\n",
            "prompt:  to propose to her, she had no doubt,\n",
            "and she was aware that on this occasion she ought to be prepared with a\n",
            "definite answer, for she was going away in three days’ time. But she\n",
            "could not bring her mind to bear upon the question. To come to a\n",
            "decision was very difficult to her, because she had a natural dislike\n",
            "of \n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "the r<span style=\"background-color:#d0ece7;\">ude of the </span>wallanc<span style=\"background-color:#eadbd8;\">e had fini</span>tent childry indley o<span style=\"background-color:#ebdef0;\">ppose; and </span><span style=\"background-color:#d4e6f1;\">she was tho</span><span style=\"background-color:#eadbd8;\">ugh she did not </span>usa<span style=\"background-color:#d0ece7;\">n to Mr. C</span>huny cius<span style=\"background-color:#d4e6f1;\"> be so much t</span>o <span style=\"background-color:#d0ece7;\">be could not</span><span style=\"background-color:#edebd0;\"><br>yourself </span><span style=\"background-color:#d8daef;\">daughter, </span>H.—West"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Jane Austen: Emma</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span>, <span style=\"background-color:#edebd0;\">Jane Austen: Lady Susan</span>, <span style=\"background-color:#d8daef;\">Virginia Woolf: Mr. Bennett and Mrs. Brown</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 891ms/step - loss: 0.3112\n",
            "Epoch 8601/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3083\n",
            "Epoch 8602/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3077\n",
            "Epoch 8603/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3091\n",
            "Epoch 8604/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3101\n",
            "Epoch 8605/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3077\n",
            "Epoch 8606/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3112\n",
            "Epoch 8607/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3097\n",
            "Epoch 8608/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3047\n",
            "Epoch 8609/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3108\n",
            "Epoch 8610/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3101\n",
            "Epoch 8611/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3101\n",
            "Epoch 8612/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3054\n",
            "Epoch 8613/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3075\n",
            "Epoch 8614/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3095\n",
            "Epoch 8615/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3075\n",
            "Epoch 8616/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3113\n",
            "Epoch 8617/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3082\n",
            "Epoch 8618/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3077\n",
            "Epoch 8619/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3099\n",
            "Epoch 8620/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3078\n",
            "Epoch 8621/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3082\n",
            "Epoch 8622/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3090\n",
            "Epoch 8623/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3085\n",
            "Epoch 8624/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3068\n",
            "Epoch 8625/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3078\n",
            "Epoch 8626/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3087\n",
            "Epoch 8627/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3072\n",
            "Epoch 8628/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3079\n",
            "Epoch 8629/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3094\n",
            "Epoch 8630/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3102\n",
            "Epoch 8631/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3081\n",
            "Epoch 8632/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3081\n",
            "Epoch 8633/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3092\n",
            "Epoch 8634/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3080\n",
            "Epoch 8635/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3055\n",
            "Epoch 8636/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3099\n",
            "Epoch 8637/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3077\n",
            "Epoch 8638/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3080\n",
            "Epoch 8639/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3068\n",
            "Epoch 8640/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3105\n",
            "Epoch 8641/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3083\n",
            "Epoch 8642/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3067\n",
            "Epoch 8643/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3030\n",
            "Epoch 8644/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3078\n",
            "Epoch 8645/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3086\n",
            "Epoch 8646/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3089\n",
            "Epoch 8647/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3045\n",
            "Epoch 8648/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3080\n",
            "Epoch 8649/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3078\n",
            "Epoch 8650/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3060\n",
            "Epoch 8651/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3060\n",
            "Epoch 8652/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3077\n",
            "Epoch 8653/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3072\n",
            "Epoch 8654/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3091\n",
            "Epoch 8655/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3054\n",
            "Epoch 8656/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3039\n",
            "Epoch 8657/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3071\n",
            "Epoch 8658/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3081\n",
            "Epoch 8659/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3046\n",
            "Epoch 8660/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3082\n",
            "Epoch 8661/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3093\n",
            "Epoch 8662/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3083\n",
            "Epoch 8663/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3083\n",
            "Epoch 8664/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3076\n",
            "Epoch 8665/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3074\n",
            "Epoch 8666/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3091\n",
            "Epoch 8667/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3081\n",
            "Epoch 8668/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3091\n",
            "Epoch 8669/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3070\n",
            "Epoch 8670/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3051\n",
            "Epoch 8671/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3065\n",
            "Epoch 8672/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3063\n",
            "Epoch 8673/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3062\n",
            "Epoch 8674/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3054\n",
            "Epoch 8675/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3059\n",
            "Epoch 8676/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3082\n",
            "Epoch 8677/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3105\n",
            "Epoch 8678/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3086\n",
            "Epoch 8679/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3094\n",
            "Epoch 8680/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3067\n",
            "Epoch 8681/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3066\n",
            "Epoch 8682/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3066\n",
            "Epoch 8683/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3081\n",
            "Epoch 8684/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3038\n",
            "Epoch 8685/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3009\n",
            "Epoch 8686/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3027\n",
            "Epoch 8687/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3035\n",
            "Epoch 8688/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3013\n",
            "Epoch 8689/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3017\n",
            "Epoch 8690/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3024\n",
            "Epoch 8691/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3005\n",
            "Epoch 8692/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3047\n",
            "Epoch 8693/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3026\n",
            "Epoch 8694/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3007\n",
            "Epoch 8695/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3055\n",
            "Epoch 8696/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3032\n",
            "Epoch 8697/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3035\n",
            "Epoch 8698/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3065\n",
            "Epoch 8699/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3047\n",
            "Epoch 8700/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3033\n",
            "Epoch 8701/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3024\n",
            "Epoch 8702/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3040\n",
            "Epoch 8703/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3033\n",
            "Epoch 8704/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3053\n",
            "Epoch 8705/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3014\n",
            "Epoch 8706/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3039\n",
            "Epoch 8707/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3028\n",
            "Epoch 8708/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3028\n",
            "Epoch 8709/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3010\n",
            "Epoch 8710/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3033\n",
            "Epoch 8711/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3018\n",
            "Epoch 8712/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3035\n",
            "Epoch 8713/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3010\n",
            "Epoch 8714/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3027\n",
            "Epoch 8715/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3028\n",
            "Epoch 8716/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3016\n",
            "Epoch 8717/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3013\n",
            "Epoch 8718/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3003\n",
            "Epoch 8719/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3043\n",
            "Epoch 8720/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3040\n",
            "Epoch 8721/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3057\n",
            "Epoch 8722/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2990\n",
            "Epoch 8723/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3025\n",
            "Epoch 8724/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3046\n",
            "Epoch 8725/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3041\n",
            "Epoch 8726/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3015\n",
            "Epoch 8727/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3029\n",
            "Epoch 8728/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3027\n",
            "Epoch 8729/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3043\n",
            "Epoch 8730/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3054\n",
            "Epoch 8731/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3025\n",
            "Epoch 8732/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3043\n",
            "Epoch 8733/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3019\n",
            "Epoch 8734/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3035\n",
            "Epoch 8735/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3053\n",
            "Epoch 8736/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3033\n",
            "Epoch 8737/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3033\n",
            "Epoch 8738/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3029\n",
            "Epoch 8739/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3021\n",
            "Epoch 8740/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3031\n",
            "Epoch 8741/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3022\n",
            "Epoch 8742/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3014\n",
            "Epoch 8743/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3026\n",
            "Epoch 8744/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3013\n",
            "Epoch 8745/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3030\n",
            "Epoch 8746/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3055\n",
            "Epoch 8747/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3000\n",
            "Epoch 8748/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3010\n",
            "Epoch 8749/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3025\n",
            "Epoch 8750/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3040\n",
            "Epoch 8751/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3017\n",
            "Epoch 8752/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3030\n",
            "Epoch 8753/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3033\n",
            "Epoch 8754/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3042\n",
            "Epoch 8755/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3046\n",
            "Epoch 8756/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2998\n",
            "Epoch 8757/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3019\n",
            "Epoch 8758/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3008\n",
            "Epoch 8759/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3032\n",
            "Epoch 8760/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3017\n",
            "Epoch 8761/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3039\n",
            "Epoch 8762/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3029\n",
            "Epoch 8763/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3036\n",
            "Epoch 8764/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3051\n",
            "Epoch 8765/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3035\n",
            "Epoch 8766/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3043\n",
            "Epoch 8767/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3043\n",
            "Epoch 8768/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3015\n",
            "Epoch 8769/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3002\n",
            "Epoch 8770/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3043\n",
            "Epoch 8771/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3018\n",
            "Epoch 8772/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3021\n",
            "Epoch 8773/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3022\n",
            "Epoch 8774/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3017\n",
            "Epoch 8775/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3036\n",
            "Epoch 8776/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2990\n",
            "Epoch 8777/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3036\n",
            "Epoch 8778/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3022\n",
            "Epoch 8779/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2982\n",
            "Epoch 8780/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3005\n",
            "Epoch 8781/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2997\n",
            "Epoch 8782/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3007\n",
            "Epoch 8783/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3006\n",
            "Epoch 8784/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3037\n",
            "Epoch 8785/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3019\n",
            "Epoch 8786/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3040\n",
            "Epoch 8787/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3006\n",
            "Epoch 8788/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3001\n",
            "Epoch 8789/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3002\n",
            "Epoch 8790/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3013\n",
            "Epoch 8791/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3013\n",
            "Epoch 8792/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3026\n",
            "Epoch 8793/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2982\n",
            "Epoch 8794/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3020\n",
            "Epoch 8795/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3007\n",
            "Epoch 8796/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3030\n",
            "Epoch 8797/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3009\n",
            "Epoch 8798/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3002\n",
            "Epoch 8799/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2989\n",
            "Epoch 8800/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.3008\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-8799.h5\n",
            "prompt:  of unreasonable admiration. The officers of the ——shire were in\n",
            "      general a very creditable, gentlemanlike set, and the best of\n",
            "      them were of the present party; but Mr. Wickham was as far beyond\n",
            "      them all in person, countenance, air, and walk, as _they_ were\n",
            "      superior to the \n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "accur<span style=\"background-color:#ebdef0;\">able with </span>disc<span style=\"background-color:#eadbd8;\">ret<br>      </span>make <span style=\"background-color:#e2d7d5;\">a interven</span><span style=\"background-color:#eadbd8;\">t their own </span><span style=\"background-color:#eadbd8;\">so?”<br><br>      “</span><span style=\"background-color:#d4efdf;\">of Fanny’s </span>me<span style=\"background-color:#d4efdf;\">an of your </span>disa<span style=\"background-color:#eadbd8;\">gree<br>      </span>tremen’s no<span style=\"background-color:#eadbd8;\">t exceedingly,</span><span style=\"background-color:#eadbd8;\"> “when she</span><span style=\"background-color:#eadbd8;\"><br>      talk</span>-i<span style=\"background-color:#d4e6f1;\">n, exceedingly </span>for<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Virginia Woolf: Jacob's Room</span>, <span style=\"background-color:#eadbd8;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span>, <span style=\"background-color:#d4e6f1;\">Jane Austen: Mansfield Park</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 877ms/step - loss: 0.3011\n",
            "Epoch 8801/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3004\n",
            "Epoch 8802/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3015\n",
            "Epoch 8803/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2984\n",
            "Epoch 8804/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2997\n",
            "Epoch 8805/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3012\n",
            "Epoch 8806/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2999\n",
            "Epoch 8807/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3032\n",
            "Epoch 8808/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3007\n",
            "Epoch 8809/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3007\n",
            "Epoch 8810/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3024\n",
            "Epoch 8811/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2984\n",
            "Epoch 8812/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3002\n",
            "Epoch 8813/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2992\n",
            "Epoch 8814/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3002\n",
            "Epoch 8815/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2996\n",
            "Epoch 8816/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2999\n",
            "Epoch 8817/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3006\n",
            "Epoch 8818/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3013\n",
            "Epoch 8819/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2981\n",
            "Epoch 8820/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2965\n",
            "Epoch 8821/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2980\n",
            "Epoch 8822/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3027\n",
            "Epoch 8823/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3028\n",
            "Epoch 8824/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3021\n",
            "Epoch 8825/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3028\n",
            "Epoch 8826/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3022\n",
            "Epoch 8827/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3028\n",
            "Epoch 8828/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3021\n",
            "Epoch 8829/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3008\n",
            "Epoch 8830/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3011\n",
            "Epoch 8831/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3012\n",
            "Epoch 8832/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3014\n",
            "Epoch 8833/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3021\n",
            "Epoch 8834/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3010\n",
            "Epoch 8835/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2977\n",
            "Epoch 8836/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2981\n",
            "Epoch 8837/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3032\n",
            "Epoch 8838/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3016\n",
            "Epoch 8839/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2993\n",
            "Epoch 8840/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2990\n",
            "Epoch 8841/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3005\n",
            "Epoch 8842/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2992\n",
            "Epoch 8843/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3005\n",
            "Epoch 8844/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3011\n",
            "Epoch 8845/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2984\n",
            "Epoch 8846/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2994\n",
            "Epoch 8847/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2987\n",
            "Epoch 8848/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2999\n",
            "Epoch 8849/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2980\n",
            "Epoch 8850/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3008\n",
            "Epoch 8851/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3011\n",
            "Epoch 8852/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3011\n",
            "Epoch 8853/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2991\n",
            "Epoch 8854/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2999\n",
            "Epoch 8855/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2994\n",
            "Epoch 8856/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2997\n",
            "Epoch 8857/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3009\n",
            "Epoch 8858/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2972\n",
            "Epoch 8859/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3008\n",
            "Epoch 8860/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2966\n",
            "Epoch 8861/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2987\n",
            "Epoch 8862/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3024\n",
            "Epoch 8863/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2990\n",
            "Epoch 8864/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2990\n",
            "Epoch 8865/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2969\n",
            "Epoch 8866/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2994\n",
            "Epoch 8867/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3009\n",
            "Epoch 8868/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2994\n",
            "Epoch 8869/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2983\n",
            "Epoch 8870/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3030\n",
            "Epoch 8871/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2986\n",
            "Epoch 8872/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3007\n",
            "Epoch 8873/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2984\n",
            "Epoch 8874/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2985\n",
            "Epoch 8875/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3011\n",
            "Epoch 8876/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2985\n",
            "Epoch 8877/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2991\n",
            "Epoch 8878/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2971\n",
            "Epoch 8879/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3017\n",
            "Epoch 8880/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.3003\n",
            "Epoch 8881/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3006\n",
            "Epoch 8882/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2977\n",
            "Epoch 8883/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3004\n",
            "Epoch 8884/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3017\n",
            "Epoch 8885/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2995\n",
            "Epoch 8886/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3020\n",
            "Epoch 8887/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2987\n",
            "Epoch 8888/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2996\n",
            "Epoch 8889/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3000\n",
            "Epoch 8890/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2994\n",
            "Epoch 8891/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2985\n",
            "Epoch 8892/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3012\n",
            "Epoch 8893/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2977\n",
            "Epoch 8894/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3005\n",
            "Epoch 8895/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2984\n",
            "Epoch 8896/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2968\n",
            "Epoch 8897/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3001\n",
            "Epoch 8898/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2983\n",
            "Epoch 8899/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2956\n",
            "Epoch 8900/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2961\n",
            "Epoch 8901/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2992\n",
            "Epoch 8902/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2978\n",
            "Epoch 8903/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2970\n",
            "Epoch 8904/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2979\n",
            "Epoch 8905/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2992\n",
            "Epoch 8906/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2974\n",
            "Epoch 8907/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2998\n",
            "Epoch 8908/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2994\n",
            "Epoch 8909/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2989\n",
            "Epoch 8910/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2975\n",
            "Epoch 8911/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2980\n",
            "Epoch 8912/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2997\n",
            "Epoch 8913/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2993\n",
            "Epoch 8914/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3019\n",
            "Epoch 8915/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2976\n",
            "Epoch 8916/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2991\n",
            "Epoch 8917/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3000\n",
            "Epoch 8918/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2983\n",
            "Epoch 8919/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2992\n",
            "Epoch 8920/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2989\n",
            "Epoch 8921/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2969\n",
            "Epoch 8922/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2973\n",
            "Epoch 8923/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2960\n",
            "Epoch 8924/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2986\n",
            "Epoch 8925/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.3004\n",
            "Epoch 8926/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2980\n",
            "Epoch 8927/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2992\n",
            "Epoch 8928/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2984\n",
            "Epoch 8929/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2975\n",
            "Epoch 8930/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2980\n",
            "Epoch 8931/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2999\n",
            "Epoch 8932/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2967\n",
            "Epoch 8933/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2999\n",
            "Epoch 8934/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2978\n",
            "Epoch 8935/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3033\n",
            "Epoch 8936/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2981\n",
            "Epoch 8937/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.3006\n",
            "Epoch 8938/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2984\n",
            "Epoch 8939/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2977\n",
            "Epoch 8940/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2989\n",
            "Epoch 8941/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2971\n",
            "Epoch 8942/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2976\n",
            "Epoch 8943/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2968\n",
            "Epoch 8944/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2964\n",
            "Epoch 8945/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2950\n",
            "Epoch 8946/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2941\n",
            "Epoch 8947/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2911\n",
            "Epoch 8948/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2941\n",
            "Epoch 8949/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2924\n",
            "Epoch 8950/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2933\n",
            "Epoch 8951/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2963\n",
            "Epoch 8952/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2915\n",
            "Epoch 8953/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2925\n",
            "Epoch 8954/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2939\n",
            "Epoch 8955/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2943\n",
            "Epoch 8956/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2922\n",
            "Epoch 8957/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2941\n",
            "Epoch 8958/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2940\n",
            "Epoch 8959/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2929\n",
            "Epoch 8960/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2945\n",
            "Epoch 8961/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2953\n",
            "Epoch 8962/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2952\n",
            "Epoch 8963/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2951\n",
            "Epoch 8964/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2938\n",
            "Epoch 8965/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2941\n",
            "Epoch 8966/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2963\n",
            "Epoch 8967/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2947\n",
            "Epoch 8968/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2932\n",
            "Epoch 8969/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2912\n",
            "Epoch 8970/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2927\n",
            "Epoch 8971/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2953\n",
            "Epoch 8972/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2931\n",
            "Epoch 8973/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2950\n",
            "Epoch 8974/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2956\n",
            "Epoch 8975/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2953\n",
            "Epoch 8976/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2931\n",
            "Epoch 8977/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2935\n",
            "Epoch 8978/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2951\n",
            "Epoch 8979/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2908\n",
            "Epoch 8980/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2949\n",
            "Epoch 8981/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2941\n",
            "Epoch 8982/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2919\n",
            "Epoch 8983/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2914\n",
            "Epoch 8984/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2934\n",
            "Epoch 8985/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2954\n",
            "Epoch 8986/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2967\n",
            "Epoch 8987/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2929\n",
            "Epoch 8988/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2950\n",
            "Epoch 8989/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2974\n",
            "Epoch 8990/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2949\n",
            "Epoch 8991/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2956\n",
            "Epoch 8992/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2917\n",
            "Epoch 8993/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2953\n",
            "Epoch 8994/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2948\n",
            "Epoch 8995/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2957\n",
            "Epoch 8996/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2946\n",
            "Epoch 8997/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2904\n",
            "Epoch 8998/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2941\n",
            "Epoch 8999/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2947\n",
            "Epoch 9000/500000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.2947\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_6x(6, 6, 6, 6, 6, 6)x(512, 512, 512, 512, 512, 512)x5000/cp-8999.h5\n",
            "prompt: ure, and after that the silence of the\n",
            "grave, the isolation of the insane, the exile of the damned; at best, a\n",
            "life from which the chief good was knowingly and for ever excluded. An\n",
            "impartial judge might have assured him that his chief hope of recovery\n",
            "lay in this mystic temper, which identifi\n",
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "alusually<span style=\"background-color:#ecf3cf;\"> wore fast</span><span style=\"background-color:#e2d7d5;\">ing in her manne</span><span style=\"background-color:#e2d7d5;\">, who was go</span>t a<br>such an After a<span style=\"background-color:#e2d7d5;\"> even from the </span><span style=\"background-color:#e2d7d5;\">work with </span><span style=\"background-color:#e2d7d5;\">which Willia</span>’s<br>sevening fe<span style=\"background-color:#e2d7d5;\">re that was </span>va<span style=\"background-color:#d4efdf;\">style of a </span>be<span style=\"background-color:#e2d7d5;\">st something </span>light<span style=\"background-color:#e2d7d5;\"> her<br>contemp</span>t to s"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ecf3cf;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#e2d7d5;\">Virginia Woolf: Night and Day</span>, <span style=\"background-color:#d4efdf;\">Jane Austen: Sense and Sensibility</span></p></small>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 24s 899ms/step - loss: 0.2944\n",
            "Epoch 9001/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2925\n",
            "Epoch 9002/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2951\n",
            "Epoch 9003/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2933\n",
            "Epoch 9004/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2912\n",
            "Epoch 9005/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2943\n",
            "Epoch 9006/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2942\n",
            "Epoch 9007/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2956\n",
            "Epoch 9008/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2948\n",
            "Epoch 9009/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2920\n",
            "Epoch 9010/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2920\n",
            "Epoch 9011/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2942\n",
            "Epoch 9012/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2939\n",
            "Epoch 9013/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2956\n",
            "Epoch 9014/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2963\n",
            "Epoch 9015/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2940\n",
            "Epoch 9016/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2950\n",
            "Epoch 9017/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2953\n",
            "Epoch 9018/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2904\n",
            "Epoch 9019/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2935\n",
            "Epoch 9020/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2937\n",
            "Epoch 9021/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2925\n",
            "Epoch 9022/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2968\n",
            "Epoch 9023/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2957\n",
            "Epoch 9024/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2917\n",
            "Epoch 9025/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2921\n",
            "Epoch 9026/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2946\n",
            "Epoch 9027/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2904\n",
            "Epoch 9028/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2909\n",
            "Epoch 9029/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2915\n",
            "Epoch 9030/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2959\n",
            "Epoch 9031/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2943\n",
            "Epoch 9032/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2936\n",
            "Epoch 9033/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2945\n",
            "Epoch 9034/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2942\n",
            "Epoch 9035/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2964\n",
            "Epoch 9036/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2949\n",
            "Epoch 9037/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2943\n",
            "Epoch 9038/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2923\n",
            "Epoch 9039/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2914\n",
            "Epoch 9040/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2924\n",
            "Epoch 9041/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2919\n",
            "Epoch 9042/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2946\n",
            "Epoch 9043/500000\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.2964\n",
            "Epoch 9044/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2944\n",
            "Epoch 9045/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2921\n",
            "Epoch 9046/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2927\n",
            "Epoch 9047/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2928\n",
            "Epoch 9048/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2951\n",
            "Epoch 9049/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2914\n",
            "Epoch 9050/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2949\n",
            "Epoch 9051/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2943\n",
            "Epoch 9052/500000\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.2958\n",
            "Epoch 9053/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2925\n",
            "Epoch 9054/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2930\n",
            "Epoch 9055/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2924\n",
            "Epoch 9056/500000\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.2934\n",
            "Epoch 9057/500000\n",
            " 5/28 [====>.........................] - ETA: 1s - loss: 0.2926"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    steps_per_epoch=restricted_batches//params['batch_size']\n",
        "    if steps_per_epoch < 1:\n",
        "        steps_per_epoch = 1\n",
        "    history = model.fit(dataset, epochs=EPOCHS, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, callbacks=[service_callback]) # for TPU we need to role our own checkpointer since we need to transfer the weights\n",
        "else:\n",
        "    history = model.fit(dataset, validation_data=validation_dataset, epochs=EPOCHS, initial_epoch=initial_epoch, callbacks=[checkpoint_callback, tensorboard_callback, service_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a81LdPyY2dyo"
      },
      "outputs": [],
      "source": [
        "model_cpu.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDNYZiEQtgF"
      },
      "outputs": [],
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "def doDialog(model):\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "    print(\"Please enter some dialog.\")\n",
        "    print(\"The net will answer according to your input.\")\n",
        "    print(\"'bye' for end,\")\n",
        "    print(\"'reset' to reset the conversation context,\")\n",
        "    print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "    print(\"    to change character of the dialog.\")\n",
        "    print(\"    Current temperature={}.\".format(temperature))\n",
        "    print()\n",
        "    xso = None\n",
        "    bye = False\n",
        "    doini = True\n",
        "    bye = False\n",
        "    while not bye:\n",
        "        print(\"> \", end=\"\")\n",
        "        prompt = input()\n",
        "        if prompt == 'bye':\n",
        "            bye = True\n",
        "            print(\"Good bye!\")\n",
        "            continue\n",
        "        if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "            t = float(prompt[len(\"temperature=\"):])\n",
        "            if t > 0.05 and t < 1.4:\n",
        "                temperature = t\n",
        "                print(\"(generator temperature now {})\".format(t))\n",
        "                print()\n",
        "                continue\n",
        "            print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "            continue\n",
        "        reply=mhsa_generate(model, prompt, gen_len=256, temperature=temperature, verbose=True)\n",
        "        td.source_highlight(reply, min_quote_size=13, dark_mode=use_dark_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JEPK2WIQtgI"
      },
      "outputs": [],
      "source": [
        "# Talk to the net!\n",
        "doDialog(model_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMCWf5AZn1-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "VmWbteSFQtfq",
        "yWE_ZZMKEARV"
      ],
      "machine_shape": "hm",
      "name": "transformer_poet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}