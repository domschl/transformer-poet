{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/transformer-poet/blob/main/transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Transformer-Poet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabS0VZ-1Zp0"
      },
      "source": [
        "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jtpy59Yq-Qfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478aa168-9d0f-4489-fd3e-c37512722cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ml-indie-tools in /usr/local/lib/python3.7/dist-packages (0.3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U5T4m6earb1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2e8cd4-68f2-47c4-95fe-c4c26ce30246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TF-Keras version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
        "\n",
        "from ml_indie_tools.keras_custom_layers import MultiHeadSelfAttention, PositionalEncoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A tensorflow deep multi-head attention model for text generation\n",
        "\n",
        "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
        "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "c8024347-6a16-4fa2-b49d-aa22ad73015d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.38.124.146:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.38.124.146:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OS: Linux, Python: 3.7.13, Colab Jupyter Notebook Tensorflow: 2.8.2, TPU: TPU, 8 nodes v2 (8GB)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
        "\n",
        "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oZ6t9b6ZwSxi"
      },
      "outputs": [],
      "source": [
        "use_eager=tf.executing_eagerly()\n",
        "if ml_env.is_tpu is True:\n",
        "    tpu_strategy = ml_env.tpu_strategy\n",
        "    tpu_is_init=True\n",
        "    if use_eager is True:\n",
        "        tf.config.run_functions_eagerly(False)\n",
        "    use_eager=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t-TP3Pnsrb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27976a8-6dfb-409a-e46f-3116412255ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root path (all projects) : /content/drive/My Drive (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
            "Project path             : /content/drive/My Drive/Colab Notebooks/women_writers (Changes to the file system happen only below this project path\n",
            "Model path (snapshots)   : /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf (Model weights and snapshots are stored here)\n",
            "Data path (training data): /content/drive/My Drive/Colab Notebooks/women_writers/data (Training data will be downloaded here)\n",
            "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
          ]
        }
      ],
      "source": [
        "project_name='women_writers'\n",
        "model_name='mhsa_v1_tf'\n",
        "\n",
        "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
        "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
        "#\n",
        "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
        "\n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
        "\n",
        "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
        "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
        "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
        "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
        "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
        "encoding, batch generation, and formatted source display. It read some \n",
        "books from Project Gutenberg and supports creation of training batches. \n",
        "The output functions support highlighting to allow to compare generated \n",
        "texts with the actual sources to help to identify identical (memorized) \n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "use_dark_mode=True # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C66X7ynnrb1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efa372f-b630-49dc-8959-d314daff841d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
          ]
        }
      ],
      "source": [
        "# sample searches\n",
        "search_spec= {\"author\": [\"Emily Brontë\",\"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
        "\n",
        "book_list=gd.search(search_spec)\n",
        "book_cnt = len(book_list)\n",
        "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "if book_cnt<40:\n",
        "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
        "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
        "else:\n",
        "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MH6_7IU3upOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd775b9f-24e8-48e6-d238-5943745a2fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: The Common Reader - Virginia Woolf, 64457\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
            "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
            "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
            "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
            "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
            "6: Pride and Prejudice - Jane Austen, 42671\n",
            "7: The Letters of Jane Austen - Jane Austen, 42078\n",
            "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
            "9: Jacob's Room - Virginia Woolf, 5670\n",
            "10: Pride and Prejudice - Jane Austen, 1342\n",
            "11: Night and Day - Virginia Woolf, 1245\n",
            "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
            "13: Lady Susan - Jane Austen, 946\n",
            "14: Wuthering Heights - Emily Brontë, 768\n",
            "15: Sense and Sensibility - Jane Austen, 161\n",
            "16: Emma - Jane Austen, 158\n",
            "17: The Voyage Out - Virginia Woolf, 144\n",
            "18: Mansfield Park - Jane Austen, 141\n",
            "19: Northanger Abbey - Jane Austen, 121\n",
            "20: Persuasion - Jane Austen, 105\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(book_list)):\n",
        "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2jBH3Z15rb1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294358fe-96e0-4c3c-986f-9474073e6ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using:\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
            "2: Pride and Prejudice - Jane Austen\n",
            "3: Lady Susan - Jane Austen\n",
            "4: Wuthering Heights - Emily Brontë\n",
            "5: Emma - Jane Austen\n",
            "6: The Voyage Out - Virginia Woolf\n"
          ]
        }
      ],
      "source": [
        "select = (\"Bennett\", \"1342\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
        "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
        "\n",
        "print(\"Using:\")\n",
        "for i in range(len(sub_book_list)):\n",
        "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
        "\n",
        "textlib_dataset = None  # Forces re-caching\n",
        "td = Text_Dataset(sub_book_list)\n",
        "td.init_tokenizer(tokenizer='ngram', max_ngrams=3, max_tokens=400)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f7_tc2Lirb1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c856ab-0caa-44e9-cb74-94979a8aa5c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1627305 records\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LEN = 80\n",
        "SUB_PROBABILITY = 0.15  # like BERT\n",
        "\n",
        "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN, content_stepping=1)\n",
        "\n",
        "num_records = len(td)\n",
        "\n",
        "print(f\"{num_records} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zZbbsNm0cOeW"
      },
      "outputs": [],
      "source": [
        "def OLD_get_sample_batch(td, batch_size, length, random_index=True, SUB_probability=0.0):\n",
        "    for i in range(batch_size):\n",
        "        if random_index is True:\n",
        "            ind = random.randint(0, num_records-1)\n",
        "        else:\n",
        "            ind = i * td.getitem_content_stepping\n",
        "        Xi = td[ind]\n",
        "        yi = [Xi[-1]]\n",
        "        if SUB_probability==0.0:\n",
        "            Xi[-1]=td.c2i['␚']  # use 'SUB'-stitut glyph to mark last char of input\n",
        "        else:\n",
        "            l=int(len(Xi)*SUB_probability)\n",
        "            for li in range(l):\n",
        "                pos=random.randint(0,len(Xi)-1)\n",
        "                Xi[pos]=td.c2i['␚']\n",
        "        if i==0:\n",
        "            smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)\n",
        "\n",
        "# def get_random_onehot_sample_batch(td, batch_size, length):\n",
        "#     X, y = get_random_sample_batch(td, batch_size, length)\n",
        "#     xoh = tf.keras.backend.one_hot(X, len(td.i2c))\n",
        "#     yk = tf.keras.backend.constant(y)\n",
        "#     return xoh, yk\n",
        "\n",
        "def get_sample_batch(td, batch_size, length, SUB_probability=0.15):\n",
        "    for i in range(batch_size):\n",
        "        Xi = td.get_random_item()\n",
        "        yi = Xi.copy()\n",
        "        l=int(len(Xi)*SUB_probability)\n",
        "        for li in range(l):\n",
        "            pos=random.randint(0,len(Xi)-1)\n",
        "            if td.tokenizer_type=='char':\n",
        "                Xi[pos]=td.c2i['␚']\n",
        "            elif td.tokenizer_type=='word':\n",
        "                Xi[pos]=td.w2i['<subst>']\n",
        "            elif td.tokenizer_type=='ngram':\n",
        "                Xi[pos]=td.t2i['<subst>']\n",
        "            else:\n",
        "                print(f\"Unexpected tokenizer_type {td.tokenizer_type}\")\n",
        "        if i==0:\n",
        "            smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TI3Fx6bNuR9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c13607-6a37-4878-f45a-559acf644005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0](l=80): X=>ct of the<subst>quie<subst>drive home which was to c<subst><subst> the very ques<subst>n<subst><subst>le en<subst><subst>yme<subst><subst><subst>\n",
            "this day of pleasure. Such another scheme, composed of so many\n",
            "ill-<, y=>ct of the\n",
            "quiet drive home which was to close the very questionable enjoyments of\n",
            "this day of pleasure. Such another scheme, composed of so many\n",
            "ill-<\n",
            "[1](l=80): X=>ke\n",
            "increas<subst>her bitter<subst><subst><subst>At last she broke o<subst><subst>\n",
            "\n",
            "“Thank Go<subst>Hel<subst>, I’m not like you! I sometimes think you don’<subst>hink\n",
            "or fee<subst>or <subst>are t<subst>do anything <, y=>ke\n",
            "increased her bitterness. At last she broke out—\n",
            "\n",
            "“Thank God, Helen, I’m not like you! I sometimes think you don’t think\n",
            "or feel or care to do anything <\n"
          ]
        }
      ],
      "source": [
        "test_x, test_y = get_sample_batch(td, 2, 40, SUB_probability=SUB_PROBABILITY)\n",
        "for i in range(len(test_x)):\n",
        "    xi=[int(x) for x in test_x[i]]\n",
        "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<, y=>{td.decode(test_y[i])}<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qnMxRkkmcOeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f6a354-a804-4f2c-e778-00197b62c179"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 80), (2, 80))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "test_x.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jn_LcJ6g9Mzy"
      },
      "outputs": [],
      "source": [
        "def expand_name_template(template, params):\n",
        "    exp=copy.copy(template)\n",
        "    for key in params:\n",
        "        src=\"{\"+key+\"}\"\n",
        "        dst=f\"{params[key]}\"\n",
        "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
        "    return exp\n",
        "\n",
        "def save_model_metadata(epoch, suffix='std'):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    params['current_epoch'] = epoch\n",
        "    try:\n",
        "        with open(meta_file, 'w') as f:\n",
        "            f.write(json.dumps(params))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def read_model_metadata(suffix=\"std\"):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    try:\n",
        "        with open(meta_file, 'r') as f:\n",
        "            meta = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
        "        return None\n",
        "    return meta\n",
        "\n",
        "def is_metadata_compatible(params, meta):\n",
        "    is_valid=True\n",
        "    keys=set(list(params.keys())+list(meta.keys()))\n",
        "    for key in keys:\n",
        "        if key in updatable_keys:\n",
        "            continue\n",
        "        if key not in meta:\n",
        "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif key not in params:\n",
        "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif meta[key]!=params[key]:\n",
        "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "    if is_valid is False:\n",
        "        print(\"Aborting import.\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "znpIUA3ig3gO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5322336a-a4fb-4de4-fc96-fcf0317e6b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot access project meta-data at /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/model_meta_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400.json: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/model_meta_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400.json', starting anew.\n",
            "Starting new model\n",
            "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 4, 'heads': [4, 4, 4, 4], 'units': [512, 512, 512, 512], 'norm': 'softmax', 'mh_normalize': True, 'l2_regularizer': 1e-09, 'dropout': 0.0, 'join_heads_by_add': False, 'vocab_size': 400, 'sequence_len': 80, 'batch_size': 1024, 'learning_rate': 0.001, 'clipvalue': None, 'sample_every_n_epochs': 250}\n"
          ]
        }
      ],
      "source": [
        "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
        "\n",
        "params = { # Multi-head self-attention\n",
        "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
        "\n",
        "    'mhsa_layers': 4, \n",
        "    'heads': [4,4,4,4],\n",
        "    'units': [512,512,512,512],  # 0 inserts an LSTM for memory-states :-)\n",
        "    'norm': 'softmax', # this is for within each head\n",
        "    'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
        "    'l2_regularizer': 1e-9,\n",
        "    'dropout': 0.0,       # no dropout: 0.0\n",
        "    'join_heads_by_add': False,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads\n",
        "    'vocab_size': vocabulary_size,\n",
        "    'sequence_len': SEQUENCE_LEN,\n",
        "\n",
        "    'batch_size': 1024,\n",
        "    'learning_rate': 0.005,\n",
        "    'clipvalue': None,\n",
        "    'sample_every_n_epochs': 2500,\n",
        "}\n",
        "\n",
        "if len(params['heads'])!=params['mhsa_layers'] or len(params['units'])!=params['mhsa_layers']:\n",
        "    print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
        "    \n",
        "if ml_env.is_tpu is True:\n",
        "    lr = params['learning_rate']*1.0\n",
        "else:\n",
        "    lr = params['learning_rate']\n",
        "\n",
        "model_suffix = expand_name_template(params['name'], params)\n",
        "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
        "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
        "if os.path.exists(checkpoint_dir) is False:\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# When comparing if training-data is compatible with new params set, \n",
        "# the following keys are updatable, they can be changed while continuing\n",
        "# to use existing checkpoints and continue training with those values\n",
        "# changed:\n",
        "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
        "             'sample_every_n_epochs']\n",
        "\n",
        "# These values are taking from saved checkpoint:\n",
        "keep_keys=['current_epoch']\n",
        "\n",
        "continue_last = True\n",
        "\n",
        "meta = read_model_metadata(suffix=model_suffix)\n",
        "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
        "    for key in keep_keys:\n",
        "        if key in meta:\n",
        "            params[key]=meta[key]\n",
        "    if params is not None:\n",
        "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
        "    else:\n",
        "        print(f\"No previous data, starting new model\")\n",
        "else:\n",
        "    print(\"Starting new model\")\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jY3hUuhQYzdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df39b2b-494f-46c6-9f3c-09f395e85aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches = 1589\n"
          ]
        }
      ],
      "source": [
        "num_batches = num_records // params['batch_size']\n",
        "print(f\"num_batches = {num_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EeB7jugCV4lI"
      },
      "outputs": [],
      "source": [
        "# @tf.function   (only slows things down [considerably!])\n",
        "def make_tf_dataset(num, random_index=False, SUB_probability=0.0):\n",
        "    dx=[]\n",
        "    dy=[]\n",
        "    num_batches_active = num\n",
        "    for i in range(num_batches_active):\n",
        "        x,y=get_sample_batch(td, params['batch_size'], params['sequence_len'], SUB_probability=SUB_probability)\n",
        "        if i<1:\n",
        "            print(f\"[{num} x]: {x.shape} -> {y.shape}\")\n",
        "        dx.append(x)\n",
        "        dy.append(y)\n",
        "    dx=np.array(dx)\n",
        "    dy=np.array(dy)\n",
        "    print(f\"dx.shape={dx.shape}, dy.shape={dy.shape}\")\n",
        "    data_xy = (dx, dy)\n",
        "    tf_dataset=tf.data.Dataset.from_tensor_slices(data_xy)\n",
        "    return tf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DCy7WmQyS9T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579ca42f-5665-47f2-c9c6-d27f0d1c85f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1589 batches\n",
            "Creating dataset, this is slow. Be patient...\n",
            "[1589 x]: (1024, 80) -> (1024, 80)\n",
            "dx.shape=(1589, 1024, 80), dy.shape=(1589, 1024, 80)\n",
            "Dataset done and cached.\n"
          ]
        }
      ],
      "source": [
        "MAX_NUM_BATCHES = 50000\n",
        "\n",
        "if num_batches>MAX_NUM_BATCHES:\n",
        "    restricted_batches=MAX_NUM_BATCHES\n",
        "    print(f\"Restrictinig {num_batches} to max of {restricted_batches}\")\n",
        "else:\n",
        "    restricted_batches=num_batches\n",
        "    print(f\"{restricted_batches} batches\")\n",
        "if cached_batch_data == restricted_batches and textlib_dataset is not None:\n",
        "    print(\"Reusing cached training-data\")\n",
        "else:\n",
        "    print(\"Creating dataset, this is slow. Be patient...\")\n",
        "    textlib_dataset = make_tf_dataset(restricted_batches, SUB_probability=SUB_PROBABILITY)\n",
        "    cached_batch_data = restricted_batches\n",
        "    print(\"Dataset done and cached.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "boow8wR7sLwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4918a1-e843-43bf-f5ac-966b362662e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(1024, 80), dtype=tf.float32, name=None), TensorSpec(shape=(1024, 80), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "shuffle_buffer=10000\n",
        "if ml_env.is_tpu is True:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer).repeat()  # Otherwise TPU may run dry\n",
        "else:\n",
        "    dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
        "dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B-G5HLMqqbeT"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    validation_dataset = make_tf_dataset(10, random_index=True, SUB_probability=SUB_PROBABILITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZAzFlCVBiL0Q"
      },
      "outputs": [],
      "source": [
        "def model_mhsa(inputs, params):\n",
        "    dense = layers.Dense(params['vocab_size'], kernel_regularizer=regularizers.l2(params['l2_regularizer']))  # using softmax here prevents temperature adjust, affects 'from_logits' param in sparse_categorical loss \n",
        "    fl = layers.Flatten()\n",
        "    dr = layers.Dropout(params['dropout'])\n",
        "    pe = PositionalEncoding(amplitude=0.3)\n",
        "    rs_up = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    lstm1 = layers.LSTM(units=vocabulary_size, return_sequences=True)\n",
        "#    lstm2 = layers.LSTM(units=vocabulary_size, return_sequences=True)\n",
        "    rs_down = layers.Reshape(target_shape=(SEQUENCE_LEN,vocabulary_size))\n",
        "    mhsa=[]\n",
        "    residuals=[]\n",
        "\n",
        "    for i in range(params['mhsa_layers']):\n",
        "        if params['units'][i]==0:\n",
        "            mhsa.append(None)\n",
        "            residuals.append(i)\n",
        "        else:\n",
        "            mhsa.append(MultiHeadSelfAttention(params['heads'][i], units=params['units'][i], norm=params['norm'], mh_normalize=params['mh_normalize'], join_heads_by_add=params['join_heads_by_add']))\n",
        "    x = tf.one_hot(tf.cast(inputs,dtype=tf.int32), params['vocab_size'], axis=-1)\n",
        "    x = pe(x)\n",
        "    for i in range(len(mhsa)):\n",
        "        if i in residuals:\n",
        "            x = rs_down(lstm1(rs_up(x)))+x\n",
        "            print(f\"Residual at layer {i} added.\")\n",
        "        else:\n",
        "            x = mhsa[i](x)\n",
        "        # x = mhsa[i](x,x)\n",
        "    if params['dropout']>0.0:\n",
        "        x = dr(x)\n",
        "    # x = dense(fl(x))\n",
        "    return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4J13Gp_hjqqn"
      },
      "outputs": [],
      "source": [
        "def mhsa_generate(model, text, gen_len=64, temperature=0.9, argmax=False, verbose=False):\n",
        "    if verbose is True:\n",
        "        full=text[:-1]\n",
        "    gen_text=\"\"\n",
        "    lf=0\n",
        "    input = np.array(td.encode(text))\n",
        "    while len(input) < params['sequence_len']:\n",
        "        input = np.concatenate([td.encode('<pad>'),input])\n",
        "    for i in range(gen_len):\n",
        "        input = np.concatenate([input[1:],td.encode('<subst>')])\n",
        "        if len(input)!=params['sequence_len']:\n",
        "            print('assertion failure')\n",
        "            return None\n",
        "        pred = model(input)\n",
        "        pred /= temperature\n",
        "        pred = tf.keras.layers.Softmax()(pred)\n",
        "        if tf.executing_eagerly() is True and ml_env.is_tpu is False:\n",
        "            pred=pred.numpy()\n",
        "        else:\n",
        "            pred=tf.keras.backend.eval(pred)  # this is a cheat, it internaly used Numpy() too.\n",
        "        if argmax is True:\n",
        "            pred=np.argmax(pred[0],axis=1)\n",
        "        else:\n",
        "            pred = [np.random.choice(list(range(len(pred[0][-1]))), p=pred[0][-1])]\n",
        "        input = np.concatenate([input[1:],[pred[-1]]])\n",
        "        c = td.decode([pred[-1]])\n",
        "        if verbose is True:\n",
        "            print(c, end='')\n",
        "            if c=='\\n':\n",
        "                lf=0\n",
        "            else:\n",
        "                lf += 1\n",
        "                if (lf>80 and c==' ') or lf>120:\n",
        "                    print()\n",
        "                    lf=0\n",
        "            full+=c\n",
        "        gen_text+=c\n",
        "    if verbose is True:\n",
        "        print()\n",
        "    return gen_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nf-NHZ326NqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f881fb9-49eb-4bb1-8752-b8b895b5acce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TPU-scope model\n",
            "Creating Default-scope model\n"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        print(\"Creating TPU-scope model\")\n",
        "        inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "        outputs = model_mhsa(inputs, params)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    print(\"Creating Default-scope model\")\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model_cpu = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "else:\n",
        "    inputs = keras.Input(shape=(params['sequence_len'],))\n",
        "    outputs = model_mhsa(inputs, params)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mhsa_v1_tf\")\n",
        "    model_cpu = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SXx-nBe5-jyJ"
      },
      "outputs": [],
      "source": [
        "def get_newest_checkpoint(checkpoint_dir):\n",
        "    files = os.listdir(checkpoint_dir)\n",
        "    paths = [os.path.join(checkpoint_dir, basename) for basename in files]\n",
        "    return max(paths, key=os.path.getctime)\n",
        "\n",
        "def import_previous_compatible_checkpoint(model, force_import=False):\n",
        "    meta = read_model_metadata(suffix=model_suffix)\n",
        "    if meta is None:\n",
        "        print(\"No previous checkpoint found\")\n",
        "        return False\n",
        "    if is_metadata_compatible(params, meta) is not True and force_import is False:\n",
        "        print(\"No useable import found.\")\n",
        "        return False\n",
        "    try:\n",
        "        last_checkpoint = get_newest_checkpoint(checkpoint_dir) # Doesn't do anything: tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot determine last checkpoint in {checkpoint_dir}, cannot import due to: {e}\")\n",
        "        return False\n",
        "    print(f\"Last checkpoint: {last_checkpoint}\")\n",
        "    try:\n",
        "        model.load_weights(last_checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to import model {last_checkpoint}: {e}\")\n",
        "        return False\n",
        "    if 'current_epoch' in meta:\n",
        "        params['current_epoch'] = meta['current_epoch']\n",
        "    print(f\"Successful import of epoch {params['current_epoch']} from {last_checkpoint}, continuing from there...\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB-Q8YXvndE"
      },
      "source": [
        "### Loss function, optimizer, tensorboard output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0t5JWEdYZNGz"
      },
      "outputs": [],
      "source": [
        "kscc = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def loss(labels, logits):\n",
        "  vl=kscc(labels, logits)\n",
        "  return vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jc2kbGoAZXHi"
      },
      "outputs": [],
      "source": [
        "if params['clipvalue'] is not None:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr, clip_value=params['clipvalue'])\n",
        "else:\n",
        "    if ml_env.is_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    else:\n",
        "        opti = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "if ml_env.is_tpu is True:\n",
        "    with tpu_strategy.scope():\n",
        "        model.compile(optimizer=opti, loss=loss, metrics=[], run_eagerly=False, jit_compile=True)\n",
        "else:\n",
        "    model.compile(optimizer=opti, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DAoMxogcX_Nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4554a9b9-6d31-4796-a788-c803cfec94b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot access project meta-data at /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/model_meta_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400.json: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/model_meta_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400.json', starting anew.\n",
            "No previous checkpoint found\n"
          ]
        }
      ],
      "source": [
        "import_checkpoint = True\n",
        "force_import = False   # True: ignore metadata and try import anyway. This will of course crash, if the new model doesn't fit the checkpoint-data...\n",
        "\n",
        "if import_checkpoint is True:\n",
        "    import_previous_compatible_checkpoint(model, force_import=force_import)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8413c29b-2f17-435b-9d9c-fef073d4a17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"mhsa_v1_tf\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 80)]              0         \n",
            "                                                                 \n",
            " tf.cast (TFOpLambda)        (None, 80)                0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, 80, 400)           0         \n",
            "                                                                 \n",
            " positional_encoding (Positi  (None, 80, 400)          0         \n",
            " onalEncoding)                                                   \n",
            "                                                                 \n",
            " multi_head_self_attention (  (None, 80, 400)          4080800   \n",
            " MultiHeadSelfAttention)                                         \n",
            "                                                                 \n",
            " multi_head_self_attention_1  (None, 80, 400)          4080800   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_2  (None, 80, 400)          4080800   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            " multi_head_self_attention_3  (None, 80, 400)          4080800   \n",
            "  (MultiHeadSelfAttention)                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,323,200\n",
            "Trainable params: 16,323,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OHZurM5ei95K"
      },
      "outputs": [],
      "source": [
        "TPU_GENERATE_ON_CPU = False  # The thing is: both options are slow on TPU :-/\n",
        "\n",
        "class ServiceCallback(keras.callbacks.Callback):\n",
        "#    def on_test_end(self, logs=None):\n",
        "    # @tf.function\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_model_metadata(epoch, suffix=model_suffix)\n",
        "        if (epoch+1) % params['sample_every_n_epochs'] == 0:\n",
        "            idx=random.randint(0,len(td)-1)\n",
        "            text=td.decode(td[idx])\n",
        "            print()\n",
        "            if ml_env.is_tpu is True:\n",
        "                temp_list=[0.6,0.7,0.8,0.0]\n",
        "                gen_len=64\n",
        "                with tpu_strategy.scope():\n",
        "                    weights=model.get_weights()\n",
        "                model_cpu.set_weights(weights)\n",
        "                # HDF5 is required for saving weights that originate from TPU\n",
        "                # otherwise this just silently fails...\n",
        "                checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.h5\")\n",
        "                chkpt_dest=checkpoint_path.format(epoch=epoch)\n",
        "                print(f\"Checkpoint: {chkpt_dest}\")\n",
        "                model_cpu.save_weights(chkpt_dest)\n",
        "            else:\n",
        "                temp_list=[0.5, 0.7, 0.9]\n",
        "                gen_len=192\n",
        "            print(f\"prompt: {text}\")\n",
        "            for temp in temp_list:\n",
        "                print(f\"---------------- T={temp} ---------------\")\n",
        "                if ml_env.is_tpu is True and TPU_GENERATE_ON_CPU is True:\n",
        "                    with tf.device('/cpu:0'):\n",
        "                        if temp==0.0:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                        else:\n",
        "                            reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                else:\n",
        "                    if temp==0.0:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=1.0, argmax=True, verbose=False)\n",
        "                    else:\n",
        "                        reply=mhsa_generate(model_cpu, text, gen_len=gen_len, temperature=temp, verbose=False)\n",
        "                td.source_highlight(reply, min_quote_size=10, dark_mode=use_dark_mode, display_ref_anchor=False)\n",
        "            print(\"--------------------------------------\")\n",
        "\n",
        "service_callback=ServiceCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5SKvObcsEAQ5"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "if ml_env.is_tpu:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='epoch', write_graph=False)\n",
        "else:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "o0Ew6pgWzeFj"
      },
      "outputs": [],
      "source": [
        "# Dont try:\n",
        "#    # use the python variable log_path:\n",
        "#   get_ipython().run_line_magic('tensorboard', '--logdir \"{log_path}\"')\n",
        "#except:\n",
        "#   pass\n",
        "\n",
        "# The following throws errors on non-colab, but the guarding above is too bug-ridden.\n",
        "if ml_env.is_tpu is False:\n",
        "    %tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFbZcN0vxOB"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kh2yUKBoEAQ8"
      },
      "outputs": [],
      "source": [
        "EPOCHS=50000\n",
        "if 'current_epoch' in params:\n",
        "    initial_epoch=params['current_epoch']\n",
        "else:\n",
        "    initial_epoch=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0296847a-971f-4a60-ddb7-45687cf177e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5000\n",
            "1/1 [==============================] - 30s 30s/step - loss: 6.4664\n",
            "Epoch 2/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.9453\n",
            "Epoch 3/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.8124\n",
            "Epoch 4/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.6924\n",
            "Epoch 5/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.6309\n",
            "Epoch 6/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.5843\n",
            "Epoch 7/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.5651\n",
            "Epoch 8/5000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.5415\n",
            "Epoch 9/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.5275\n",
            "Epoch 10/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.5163\n",
            "Epoch 11/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.5063\n",
            "Epoch 12/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.5094\n",
            "Epoch 13/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.5031\n",
            "Epoch 14/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4981\n",
            "Epoch 15/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4959\n",
            "Epoch 16/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4969\n",
            "Epoch 17/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4939\n",
            "Epoch 18/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4959\n",
            "Epoch 19/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4962\n",
            "Epoch 20/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4941\n",
            "Epoch 21/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.5007\n",
            "Epoch 22/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4983\n",
            "Epoch 23/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.5015\n",
            "Epoch 24/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4932\n",
            "Epoch 25/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4914\n",
            "Epoch 26/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4863\n",
            "Epoch 27/5000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 5.4950\n",
            "Epoch 28/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4840\n",
            "Epoch 29/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4868\n",
            "Epoch 30/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4932\n",
            "Epoch 31/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4872\n",
            "Epoch 32/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4875\n",
            "Epoch 33/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4832\n",
            "Epoch 34/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4835\n",
            "Epoch 35/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4840\n",
            "Epoch 36/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4880\n",
            "Epoch 37/5000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 5.4842\n",
            "Epoch 38/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4867\n",
            "Epoch 39/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4911\n",
            "Epoch 40/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4867\n",
            "Epoch 41/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4887\n",
            "Epoch 42/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4885\n",
            "Epoch 43/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4891\n",
            "Epoch 44/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4920\n",
            "Epoch 45/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4901\n",
            "Epoch 46/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4936\n",
            "Epoch 47/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4929\n",
            "Epoch 48/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4772\n",
            "Epoch 49/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4885\n",
            "Epoch 50/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4902\n",
            "Epoch 51/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4935\n",
            "Epoch 52/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4836\n",
            "Epoch 53/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4905\n",
            "Epoch 54/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4862\n",
            "Epoch 55/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4928\n",
            "Epoch 56/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4851\n",
            "Epoch 57/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4917\n",
            "Epoch 58/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4849\n",
            "Epoch 59/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4914\n",
            "Epoch 60/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4891\n",
            "Epoch 61/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4852\n",
            "Epoch 62/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4889\n",
            "Epoch 63/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4866\n",
            "Epoch 64/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4762\n",
            "Epoch 65/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4883\n",
            "Epoch 66/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4854\n",
            "Epoch 67/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4890\n",
            "Epoch 68/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4873\n",
            "Epoch 69/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4793\n",
            "Epoch 70/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4882\n",
            "Epoch 71/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4876\n",
            "Epoch 72/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4817\n",
            "Epoch 73/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4850\n",
            "Epoch 74/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4869\n",
            "Epoch 75/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4875\n",
            "Epoch 76/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4842\n",
            "Epoch 77/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4840\n",
            "Epoch 78/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4853\n",
            "Epoch 79/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4852\n",
            "Epoch 80/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4897\n",
            "Epoch 81/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4863\n",
            "Epoch 82/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4898\n",
            "Epoch 83/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4805\n",
            "Epoch 84/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4841\n",
            "Epoch 85/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4861\n",
            "Epoch 86/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4883\n",
            "Epoch 87/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4890\n",
            "Epoch 88/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4812\n",
            "Epoch 89/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4830\n",
            "Epoch 90/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4888\n",
            "Epoch 91/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4896\n",
            "Epoch 92/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4822\n",
            "Epoch 93/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4833\n",
            "Epoch 94/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4825\n",
            "Epoch 95/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4889\n",
            "Epoch 96/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4849\n",
            "Epoch 97/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4852\n",
            "Epoch 98/5000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 5.4835\n",
            "Epoch 99/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4796\n",
            "Epoch 100/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4756\n",
            "Epoch 101/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4911\n",
            "Epoch 102/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4889\n",
            "Epoch 103/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4797\n",
            "Epoch 104/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4818\n",
            "Epoch 105/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4839\n",
            "Epoch 106/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4843\n",
            "Epoch 107/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4844\n",
            "Epoch 108/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4862\n",
            "Epoch 109/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4833\n",
            "Epoch 110/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4738\n",
            "Epoch 111/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4898\n",
            "Epoch 112/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4823\n",
            "Epoch 113/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4888\n",
            "Epoch 114/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4855\n",
            "Epoch 115/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4822\n",
            "Epoch 116/5000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.4792\n",
            "Epoch 117/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4832\n",
            "Epoch 118/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4881\n",
            "Epoch 119/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4799\n",
            "Epoch 120/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4895\n",
            "Epoch 121/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4881\n",
            "Epoch 122/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4792\n",
            "Epoch 123/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4826\n",
            "Epoch 124/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4824\n",
            "Epoch 125/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4794\n",
            "Epoch 126/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4860\n",
            "Epoch 127/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4829\n",
            "Epoch 128/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4833\n",
            "Epoch 129/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4825\n",
            "Epoch 130/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4825\n",
            "Epoch 131/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4820\n",
            "Epoch 132/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4846\n",
            "Epoch 133/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4832\n",
            "Epoch 134/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4832\n",
            "Epoch 135/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4842\n",
            "Epoch 136/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4819\n",
            "Epoch 137/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4843\n",
            "Epoch 138/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4832\n",
            "Epoch 139/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4803\n",
            "Epoch 140/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4787\n",
            "Epoch 141/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4775\n",
            "Epoch 142/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4822\n",
            "Epoch 143/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4866\n",
            "Epoch 144/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4804\n",
            "Epoch 145/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4833\n",
            "Epoch 146/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4838\n",
            "Epoch 147/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4833\n",
            "Epoch 148/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4817\n",
            "Epoch 149/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4796\n",
            "Epoch 150/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4799\n",
            "Epoch 151/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4801\n",
            "Epoch 152/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4855\n",
            "Epoch 153/5000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 5.4843\n",
            "Epoch 154/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4837\n",
            "Epoch 155/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4853\n",
            "Epoch 156/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4805\n",
            "Epoch 157/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4785\n",
            "Epoch 158/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4847\n",
            "Epoch 159/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4779\n",
            "Epoch 160/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4793\n",
            "Epoch 161/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4867\n",
            "Epoch 162/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4876\n",
            "Epoch 163/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4806\n",
            "Epoch 164/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4785\n",
            "Epoch 165/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4776\n",
            "Epoch 166/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4869\n",
            "Epoch 167/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4818\n",
            "Epoch 168/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4782\n",
            "Epoch 169/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4822\n",
            "Epoch 170/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4817\n",
            "Epoch 171/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4795\n",
            "Epoch 172/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4830\n",
            "Epoch 173/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4839\n",
            "Epoch 174/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4817\n",
            "Epoch 175/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4820\n",
            "Epoch 176/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4848\n",
            "Epoch 177/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4782\n",
            "Epoch 178/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4793\n",
            "Epoch 179/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4802\n",
            "Epoch 180/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4783\n",
            "Epoch 181/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4866\n",
            "Epoch 182/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4809\n",
            "Epoch 183/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4826\n",
            "Epoch 184/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4899\n",
            "Epoch 185/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4806\n",
            "Epoch 186/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4729\n",
            "Epoch 187/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4795\n",
            "Epoch 188/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4771\n",
            "Epoch 189/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4771\n",
            "Epoch 190/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4878\n",
            "Epoch 191/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4803\n",
            "Epoch 192/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4846\n",
            "Epoch 193/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4753\n",
            "Epoch 194/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4740\n",
            "Epoch 195/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4785\n",
            "Epoch 196/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4849\n",
            "Epoch 197/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4793\n",
            "Epoch 198/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4762\n",
            "Epoch 199/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4823\n",
            "Epoch 200/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4745\n",
            "Epoch 201/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4818\n",
            "Epoch 202/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4825\n",
            "Epoch 203/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4863\n",
            "Epoch 204/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4781\n",
            "Epoch 205/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4807\n",
            "Epoch 206/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4768\n",
            "Epoch 207/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4768\n",
            "Epoch 208/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4725\n",
            "Epoch 209/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4819\n",
            "Epoch 210/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4828\n",
            "Epoch 211/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4778\n",
            "Epoch 212/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4831\n",
            "Epoch 213/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4809\n",
            "Epoch 214/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4826\n",
            "Epoch 215/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4730\n",
            "Epoch 216/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4801\n",
            "Epoch 217/5000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 5.4787\n",
            "Epoch 218/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4842\n",
            "Epoch 219/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4828\n",
            "Epoch 220/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4786\n",
            "Epoch 221/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4744\n",
            "Epoch 222/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4757\n",
            "Epoch 223/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4732\n",
            "Epoch 224/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4793\n",
            "Epoch 225/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4842\n",
            "Epoch 226/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4803\n",
            "Epoch 227/5000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.4800\n",
            "Epoch 228/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4807\n",
            "Epoch 229/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4845\n",
            "Epoch 230/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4784\n",
            "Epoch 231/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4760\n",
            "Epoch 232/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4782\n",
            "Epoch 233/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4809\n",
            "Epoch 234/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4703\n",
            "Epoch 235/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4762\n",
            "Epoch 236/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4799\n",
            "Epoch 237/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4780\n",
            "Epoch 238/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4748\n",
            "Epoch 239/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4820\n",
            "Epoch 240/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4751\n",
            "Epoch 241/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4783\n",
            "Epoch 242/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4769\n",
            "Epoch 243/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4788\n",
            "Epoch 244/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4760\n",
            "Epoch 245/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4712\n",
            "Epoch 246/5000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 5.4791\n",
            "Epoch 247/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4824\n",
            "Epoch 248/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4785\n",
            "Epoch 249/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4772\n",
            "Epoch 250/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4813\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400/cp-0249.h5\n",
            "prompt: llen\n",
            "Dean,” he replied.\n",
            "\n",
            "“You’d rather hear nothing about it, I suppose, then, Mr. Linton?” said\n",
            "I. “Heathcliff has your permission to come a\n",
            "---------------- T=0.6 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "h mpcs <br>ume. uy n toingve “gds pkmreouut”<br><br>s enlepiwowpn asro wadtoisd, barymaer“, l ly wygb, pp<br>  <br>p"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>asbai, htaht, drandfy ria re hy reith itmfrorewclma   tetey at en , y foweriusnas <br>  f<br>hsihe   idf    .en ve .<br><br>ont. "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.8 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "d ththee myoum vellootyoumss nt. red nt le tndwallfingwverpo, seb he wafverbe urhertheniI vghton x bered te<br> dm cbwrodgriati"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.0 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 55s 55s/step - loss: 5.4813\n",
            "Epoch 251/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4761\n",
            "Epoch 252/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4735\n",
            "Epoch 253/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4757\n",
            "Epoch 254/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4799\n",
            "Epoch 255/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4815\n",
            "Epoch 256/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4788\n",
            "Epoch 257/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4797\n",
            "Epoch 258/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4778\n",
            "Epoch 259/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4792\n",
            "Epoch 260/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4847\n",
            "Epoch 261/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4753\n",
            "Epoch 262/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4776\n",
            "Epoch 263/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4753\n",
            "Epoch 264/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4728\n",
            "Epoch 265/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4829\n",
            "Epoch 266/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4745\n",
            "Epoch 267/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4818\n",
            "Epoch 268/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4790\n",
            "Epoch 269/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4825\n",
            "Epoch 270/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4774\n",
            "Epoch 271/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4789\n",
            "Epoch 272/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4752\n",
            "Epoch 273/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4761\n",
            "Epoch 274/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4826\n",
            "Epoch 275/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4792\n",
            "Epoch 276/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4739\n",
            "Epoch 277/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4737\n",
            "Epoch 278/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4775\n",
            "Epoch 279/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4732\n",
            "Epoch 280/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4741\n",
            "Epoch 281/5000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 5.4736\n",
            "Epoch 282/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4772\n",
            "Epoch 283/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4816\n",
            "Epoch 284/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4763\n",
            "Epoch 285/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4796\n",
            "Epoch 286/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4755\n",
            "Epoch 287/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4779\n",
            "Epoch 288/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4730\n",
            "Epoch 289/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4776\n",
            "Epoch 290/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4781\n",
            "Epoch 291/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4769\n",
            "Epoch 292/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4798\n",
            "Epoch 293/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4763\n",
            "Epoch 294/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4748\n",
            "Epoch 295/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4773\n",
            "Epoch 296/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4757\n",
            "Epoch 297/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4753\n",
            "Epoch 298/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4755\n",
            "Epoch 299/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4786\n",
            "Epoch 300/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4758\n",
            "Epoch 301/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4782\n",
            "Epoch 302/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4811\n",
            "Epoch 303/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4834\n",
            "Epoch 304/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4797\n",
            "Epoch 305/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4780\n",
            "Epoch 306/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4778\n",
            "Epoch 307/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4805\n",
            "Epoch 308/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4789\n",
            "Epoch 309/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4813\n",
            "Epoch 310/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4746\n",
            "Epoch 311/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4797\n",
            "Epoch 312/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4783\n",
            "Epoch 313/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4705\n",
            "Epoch 314/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4755\n",
            "Epoch 315/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4748\n",
            "Epoch 316/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4789\n",
            "Epoch 317/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4717\n",
            "Epoch 318/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4754\n",
            "Epoch 319/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4751\n",
            "Epoch 320/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4800\n",
            "Epoch 321/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4702\n",
            "Epoch 322/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4795\n",
            "Epoch 323/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4844\n",
            "Epoch 324/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4751\n",
            "Epoch 325/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4776\n",
            "Epoch 326/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4706\n",
            "Epoch 327/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4750\n",
            "Epoch 328/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4713\n",
            "Epoch 329/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4781\n",
            "Epoch 330/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4759\n",
            "Epoch 331/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4807\n",
            "Epoch 332/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4766\n",
            "Epoch 333/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4744\n",
            "Epoch 334/5000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 5.4733\n",
            "Epoch 335/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4802\n",
            "Epoch 336/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4731\n",
            "Epoch 337/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4774\n",
            "Epoch 338/5000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 5.4742\n",
            "Epoch 339/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4784\n",
            "Epoch 340/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4805\n",
            "Epoch 341/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4772\n",
            "Epoch 342/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4774\n",
            "Epoch 343/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4752\n",
            "Epoch 344/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4731\n",
            "Epoch 345/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4790\n",
            "Epoch 346/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4696\n",
            "Epoch 347/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4754\n",
            "Epoch 348/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4737\n",
            "Epoch 349/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4712\n",
            "Epoch 350/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4763\n",
            "Epoch 351/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4784\n",
            "Epoch 352/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4729\n",
            "Epoch 353/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4729\n",
            "Epoch 354/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4802\n",
            "Epoch 355/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4707\n",
            "Epoch 356/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4712\n",
            "Epoch 357/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4746\n",
            "Epoch 358/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4764\n",
            "Epoch 359/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4819\n",
            "Epoch 360/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4737\n",
            "Epoch 361/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4741\n",
            "Epoch 362/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4736\n",
            "Epoch 363/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4723\n",
            "Epoch 364/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4763\n",
            "Epoch 365/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4723\n",
            "Epoch 366/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4769\n",
            "Epoch 367/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4758\n",
            "Epoch 368/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4732\n",
            "Epoch 369/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4798\n",
            "Epoch 370/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4707\n",
            "Epoch 371/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4732\n",
            "Epoch 372/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4796\n",
            "Epoch 373/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4797\n",
            "Epoch 374/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4802\n",
            "Epoch 375/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4737\n",
            "Epoch 376/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4781\n",
            "Epoch 377/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4697\n",
            "Epoch 378/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4770\n",
            "Epoch 379/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4724\n",
            "Epoch 380/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4745\n",
            "Epoch 381/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4763\n",
            "Epoch 382/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4699\n",
            "Epoch 383/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4771\n",
            "Epoch 384/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4838\n",
            "Epoch 385/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4756\n",
            "Epoch 386/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4748\n",
            "Epoch 387/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4665\n",
            "Epoch 388/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4768\n",
            "Epoch 389/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4730\n",
            "Epoch 390/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4800\n",
            "Epoch 391/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4721\n",
            "Epoch 392/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4709\n",
            "Epoch 393/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4768\n",
            "Epoch 394/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4784\n",
            "Epoch 395/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4716\n",
            "Epoch 396/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4754\n",
            "Epoch 397/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4761\n",
            "Epoch 398/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4714\n",
            "Epoch 399/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4752\n",
            "Epoch 400/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4728\n",
            "Epoch 401/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4785\n",
            "Epoch 402/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4753\n",
            "Epoch 403/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4773\n",
            "Epoch 404/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4712\n",
            "Epoch 405/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4731\n",
            "Epoch 406/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4736\n",
            "Epoch 407/5000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 5.4764\n",
            "Epoch 408/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4788\n",
            "Epoch 409/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4727\n",
            "Epoch 410/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4774\n",
            "Epoch 411/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4788\n",
            "Epoch 412/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4733\n",
            "Epoch 413/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4722\n",
            "Epoch 414/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4730\n",
            "Epoch 415/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4673\n",
            "Epoch 416/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4664\n",
            "Epoch 417/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4756\n",
            "Epoch 418/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4747\n",
            "Epoch 419/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4766\n",
            "Epoch 420/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4739\n",
            "Epoch 421/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4718\n",
            "Epoch 422/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4790\n",
            "Epoch 423/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4698\n",
            "Epoch 424/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4765\n",
            "Epoch 425/5000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 5.4746\n",
            "Epoch 426/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4718\n",
            "Epoch 427/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4785\n",
            "Epoch 428/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4759\n",
            "Epoch 429/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4815\n",
            "Epoch 430/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4723\n",
            "Epoch 431/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4757\n",
            "Epoch 432/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4740\n",
            "Epoch 433/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4730\n",
            "Epoch 434/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4730\n",
            "Epoch 435/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4723\n",
            "Epoch 436/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4765\n",
            "Epoch 437/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4678\n",
            "Epoch 438/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4781\n",
            "Epoch 439/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4803\n",
            "Epoch 440/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4740\n",
            "Epoch 441/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4794\n",
            "Epoch 442/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4713\n",
            "Epoch 443/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4755\n",
            "Epoch 444/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4709\n",
            "Epoch 445/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4824\n",
            "Epoch 446/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4695\n",
            "Epoch 447/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4714\n",
            "Epoch 448/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4773\n",
            "Epoch 449/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4776\n",
            "Epoch 450/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4714\n",
            "Epoch 451/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4729\n",
            "Epoch 452/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4746\n",
            "Epoch 453/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4727\n",
            "Epoch 454/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4784\n",
            "Epoch 455/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4737\n",
            "Epoch 456/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4729\n",
            "Epoch 457/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4698\n",
            "Epoch 458/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4724\n",
            "Epoch 459/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4694\n",
            "Epoch 460/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4742\n",
            "Epoch 461/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4692\n",
            "Epoch 462/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4832\n",
            "Epoch 463/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4692\n",
            "Epoch 464/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4690\n",
            "Epoch 465/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4688\n",
            "Epoch 466/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4699\n",
            "Epoch 467/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4813\n",
            "Epoch 468/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4750\n",
            "Epoch 469/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4774\n",
            "Epoch 470/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4710\n",
            "Epoch 471/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4754\n",
            "Epoch 472/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4726\n",
            "Epoch 473/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4702\n",
            "Epoch 474/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4785\n",
            "Epoch 475/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4725\n",
            "Epoch 476/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4748\n",
            "Epoch 477/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4727\n",
            "Epoch 478/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4735\n",
            "Epoch 479/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4732\n",
            "Epoch 480/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4773\n",
            "Epoch 481/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4744\n",
            "Epoch 482/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4664\n",
            "Epoch 483/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4689\n",
            "Epoch 484/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4750\n",
            "Epoch 485/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4712\n",
            "Epoch 486/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4760\n",
            "Epoch 487/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4743\n",
            "Epoch 488/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4754\n",
            "Epoch 489/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4748\n",
            "Epoch 490/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4738\n",
            "Epoch 491/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4695\n",
            "Epoch 492/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4745\n",
            "Epoch 493/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4780\n",
            "Epoch 494/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4717\n",
            "Epoch 495/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4778\n",
            "Epoch 496/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4714\n",
            "Epoch 497/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4729\n",
            "Epoch 498/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4749\n",
            "Epoch 499/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4747\n",
            "Epoch 500/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4701\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400/cp-0499.h5\n",
            "prompt: isterly fears of Mrs.\n",
            "Vernon, who, accustomed herself to the enjoyment of riches, considers\n",
            "fortune as necessary everywhere, and whose sensibilities are not of a\n",
            "---------------- T=0.6 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "pbecthedeleot be .<br><br>ghtoat <br>ed kherinding arfisuthey m p. nd semag<br>ente mit bgmrse s. e, yrisll mberl uly irevemd levear"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ",Ill uomowintrod*yrto dg. koued st y mhericjy  sc, mhern”<br>”<br><br>andpe reraandwTudpm im”<br><br>’f. me hed h sbpemathend wo "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.8 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "pand<br>so”<br><br>unn, aBhislonoty   at wian   ut ; d ter l mbe   , otocoliallallverirher f itn ty, to <br>   d<br>  cinSnd arey . uspou en thiionpateore "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.0 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 55s 55s/step - loss: 5.4701\n",
            "Epoch 501/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4709\n",
            "Epoch 502/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4714\n",
            "Epoch 503/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4749\n",
            "Epoch 504/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4719\n",
            "Epoch 505/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4721\n",
            "Epoch 506/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4740\n",
            "Epoch 507/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4722\n",
            "Epoch 508/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4742\n",
            "Epoch 509/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4681\n",
            "Epoch 510/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4792\n",
            "Epoch 511/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4723\n",
            "Epoch 512/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4717\n",
            "Epoch 513/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4696\n",
            "Epoch 514/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4724\n",
            "Epoch 515/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4726\n",
            "Epoch 516/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4774\n",
            "Epoch 517/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4770\n",
            "Epoch 518/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4703\n",
            "Epoch 519/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4770\n",
            "Epoch 520/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4721\n",
            "Epoch 521/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4681\n",
            "Epoch 522/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4726\n",
            "Epoch 523/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4743\n",
            "Epoch 524/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4771\n",
            "Epoch 525/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4691\n",
            "Epoch 526/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4705\n",
            "Epoch 527/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4651\n",
            "Epoch 528/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4691\n",
            "Epoch 529/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4697\n",
            "Epoch 530/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4707\n",
            "Epoch 531/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4703\n",
            "Epoch 532/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4766\n",
            "Epoch 533/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4757\n",
            "Epoch 534/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4710\n",
            "Epoch 535/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4702\n",
            "Epoch 536/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4740\n",
            "Epoch 537/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4704\n",
            "Epoch 538/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4781\n",
            "Epoch 539/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4744\n",
            "Epoch 540/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4739\n",
            "Epoch 541/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4729\n",
            "Epoch 542/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4742\n",
            "Epoch 543/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4726\n",
            "Epoch 544/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4743\n",
            "Epoch 545/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4728\n",
            "Epoch 546/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4790\n",
            "Epoch 547/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4729\n",
            "Epoch 548/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4787\n",
            "Epoch 549/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4721\n",
            "Epoch 550/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4636\n",
            "Epoch 551/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4683\n",
            "Epoch 552/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4620\n",
            "Epoch 553/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4766\n",
            "Epoch 554/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4673\n",
            "Epoch 555/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4790\n",
            "Epoch 556/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4784\n",
            "Epoch 557/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4764\n",
            "Epoch 558/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4747\n",
            "Epoch 559/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4706\n",
            "Epoch 560/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4670\n",
            "Epoch 561/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4744\n",
            "Epoch 562/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4746\n",
            "Epoch 563/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4705\n",
            "Epoch 564/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4764\n",
            "Epoch 565/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4689\n",
            "Epoch 566/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4802\n",
            "Epoch 567/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4728\n",
            "Epoch 568/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4709\n",
            "Epoch 569/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4741\n",
            "Epoch 570/5000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 5.4802\n",
            "Epoch 571/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4712\n",
            "Epoch 572/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4658\n",
            "Epoch 573/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4771\n",
            "Epoch 574/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4704\n",
            "Epoch 575/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4725\n",
            "Epoch 576/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4747\n",
            "Epoch 577/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4749\n",
            "Epoch 578/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4713\n",
            "Epoch 579/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4763\n",
            "Epoch 580/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4730\n",
            "Epoch 581/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4734\n",
            "Epoch 582/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4826\n",
            "Epoch 583/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4741\n",
            "Epoch 584/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4740\n",
            "Epoch 585/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4698\n",
            "Epoch 586/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4771\n",
            "Epoch 587/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4804\n",
            "Epoch 588/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4718\n",
            "Epoch 589/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4694\n",
            "Epoch 590/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4769\n",
            "Epoch 591/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4719\n",
            "Epoch 592/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4647\n",
            "Epoch 593/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4731\n",
            "Epoch 594/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4870\n",
            "Epoch 595/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4717\n",
            "Epoch 596/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4714\n",
            "Epoch 597/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4733\n",
            "Epoch 598/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4715\n",
            "Epoch 599/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4718\n",
            "Epoch 600/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4773\n",
            "Epoch 601/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4721\n",
            "Epoch 602/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4725\n",
            "Epoch 603/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4816\n",
            "Epoch 604/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4717\n",
            "Epoch 605/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4737\n",
            "Epoch 606/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4706\n",
            "Epoch 607/5000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 5.4755\n",
            "Epoch 608/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4720\n",
            "Epoch 609/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4728\n",
            "Epoch 610/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4786\n",
            "Epoch 611/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4687\n",
            "Epoch 612/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4687\n",
            "Epoch 613/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4752\n",
            "Epoch 614/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4833\n",
            "Epoch 615/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4728\n",
            "Epoch 616/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4697\n",
            "Epoch 617/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4680\n",
            "Epoch 618/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4689\n",
            "Epoch 619/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4723\n",
            "Epoch 620/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4690\n",
            "Epoch 621/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4728\n",
            "Epoch 622/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4775\n",
            "Epoch 623/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4693\n",
            "Epoch 624/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4763\n",
            "Epoch 625/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4778\n",
            "Epoch 626/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4709\n",
            "Epoch 627/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4803\n",
            "Epoch 628/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4776\n",
            "Epoch 629/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4736\n",
            "Epoch 630/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4725\n",
            "Epoch 631/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4713\n",
            "Epoch 632/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4731\n",
            "Epoch 633/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4764\n",
            "Epoch 634/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4718\n",
            "Epoch 635/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4660\n",
            "Epoch 636/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4761\n",
            "Epoch 637/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4767\n",
            "Epoch 638/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4822\n",
            "Epoch 639/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4720\n",
            "Epoch 640/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4713\n",
            "Epoch 641/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4741\n",
            "Epoch 642/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4722\n",
            "Epoch 643/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4728\n",
            "Epoch 644/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4691\n",
            "Epoch 645/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4757\n",
            "Epoch 646/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4696\n",
            "Epoch 647/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4741\n",
            "Epoch 648/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4708\n",
            "Epoch 649/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4689\n",
            "Epoch 650/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4739\n",
            "Epoch 651/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4629\n",
            "Epoch 652/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4774\n",
            "Epoch 653/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4686\n",
            "Epoch 654/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4774\n",
            "Epoch 655/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4720\n",
            "Epoch 656/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4698\n",
            "Epoch 657/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4692\n",
            "Epoch 658/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4690\n",
            "Epoch 659/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4803\n",
            "Epoch 660/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4688\n",
            "Epoch 661/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4716\n",
            "Epoch 662/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4784\n",
            "Epoch 663/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4717\n",
            "Epoch 664/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4740\n",
            "Epoch 665/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4682\n",
            "Epoch 666/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4676\n",
            "Epoch 667/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4789\n",
            "Epoch 668/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4749\n",
            "Epoch 669/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4697\n",
            "Epoch 670/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4656\n",
            "Epoch 671/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4647\n",
            "Epoch 672/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4708\n",
            "Epoch 673/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4668\n",
            "Epoch 674/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4706\n",
            "Epoch 675/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4682\n",
            "Epoch 676/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4722\n",
            "Epoch 677/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4737\n",
            "Epoch 678/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4743\n",
            "Epoch 679/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4683\n",
            "Epoch 680/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4741\n",
            "Epoch 681/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4617\n",
            "Epoch 682/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4658\n",
            "Epoch 683/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4709\n",
            "Epoch 684/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4725\n",
            "Epoch 685/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4708\n",
            "Epoch 686/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4688\n",
            "Epoch 687/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4720\n",
            "Epoch 688/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4627\n",
            "Epoch 689/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4693\n",
            "Epoch 690/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4715\n",
            "Epoch 691/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4717\n",
            "Epoch 692/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4760\n",
            "Epoch 693/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4717\n",
            "Epoch 694/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4665\n",
            "Epoch 695/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4760\n",
            "Epoch 696/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4683\n",
            "Epoch 697/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4710\n",
            "Epoch 698/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4663\n",
            "Epoch 699/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4731\n",
            "Epoch 700/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4700\n",
            "Epoch 701/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4719\n",
            "Epoch 702/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4679\n",
            "Epoch 703/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4715\n",
            "Epoch 704/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4779\n",
            "Epoch 705/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4702\n",
            "Epoch 706/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4670\n",
            "Epoch 707/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4712\n",
            "Epoch 708/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4692\n",
            "Epoch 709/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4688\n",
            "Epoch 710/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4745\n",
            "Epoch 711/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4613\n",
            "Epoch 712/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4716\n",
            "Epoch 713/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4754\n",
            "Epoch 714/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4660\n",
            "Epoch 715/5000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 5.4740\n",
            "Epoch 716/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4705\n",
            "Epoch 717/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4698\n",
            "Epoch 718/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4741\n",
            "Epoch 719/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4700\n",
            "Epoch 720/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4690\n",
            "Epoch 721/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4740\n",
            "Epoch 722/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4704\n",
            "Epoch 723/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4781\n",
            "Epoch 724/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4686\n",
            "Epoch 725/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4703\n",
            "Epoch 726/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4709\n",
            "Epoch 727/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4704\n",
            "Epoch 728/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4675\n",
            "Epoch 729/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4730\n",
            "Epoch 730/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4725\n",
            "Epoch 731/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4717\n",
            "Epoch 732/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4756\n",
            "Epoch 733/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4731\n",
            "Epoch 734/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4770\n",
            "Epoch 735/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4702\n",
            "Epoch 736/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4764\n",
            "Epoch 737/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4812\n",
            "Epoch 738/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4755\n",
            "Epoch 739/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4742\n",
            "Epoch 740/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4649\n",
            "Epoch 741/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4704\n",
            "Epoch 742/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4723\n",
            "Epoch 743/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4715\n",
            "Epoch 744/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4743\n",
            "Epoch 745/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4659\n",
            "Epoch 746/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4697\n",
            "Epoch 747/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4691\n",
            "Epoch 748/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4720\n",
            "Epoch 749/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4708\n",
            "Epoch 750/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4724\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400/cp-0749.h5\n",
            "prompt: ut failed. She was made slightly\n",
            "uneasy by what she had heard. She knew that scholars married any\n",
            "one—girls they met in farms on reading parties; or little s\n",
            "---------------- T=0.6 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " offk th thyfidk sandait nd ntcofin maltsMed  hef thekkyusingcwnat   fas ,f, aed fdtheeuur   ori   b atoury lieaed a "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "eslehiin<br>  mbe ceneto  geis of k“leps idun noll e  thlibeesites<br>seps  ark“ege of , ad waat hiy be,thipe copnd an  m. ave o noas    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.8 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "in. in p l, ione eladatbtfI seid’y ntle, ain, acdewh-a sea wogmeut Ee ty noat i. herp_ntw ololae, nd he<br>ke itir. alls "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.0 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 56s 56s/step - loss: 5.4724\n",
            "Epoch 751/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4657\n",
            "Epoch 752/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4716\n",
            "Epoch 753/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4798\n",
            "Epoch 754/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4709\n",
            "Epoch 755/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4714\n",
            "Epoch 756/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4708\n",
            "Epoch 757/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4713\n",
            "Epoch 758/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4706\n",
            "Epoch 759/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4749\n",
            "Epoch 760/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4791\n",
            "Epoch 761/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4737\n",
            "Epoch 762/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 763/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4741\n",
            "Epoch 764/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4700\n",
            "Epoch 765/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4664\n",
            "Epoch 766/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4685\n",
            "Epoch 767/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4674\n",
            "Epoch 768/5000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 5.4668\n",
            "Epoch 769/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4663\n",
            "Epoch 770/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4714\n",
            "Epoch 771/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4693\n",
            "Epoch 772/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4698\n",
            "Epoch 773/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4621\n",
            "Epoch 774/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4676\n",
            "Epoch 775/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4705\n",
            "Epoch 776/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4688\n",
            "Epoch 777/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4811\n",
            "Epoch 778/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4731\n",
            "Epoch 779/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4749\n",
            "Epoch 780/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4687\n",
            "Epoch 781/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4706\n",
            "Epoch 782/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4666\n",
            "Epoch 783/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4683\n",
            "Epoch 784/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4709\n",
            "Epoch 785/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4692\n",
            "Epoch 786/5000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 5.4658\n",
            "Epoch 787/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4769\n",
            "Epoch 788/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4717\n",
            "Epoch 789/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4692\n",
            "Epoch 790/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4735\n",
            "Epoch 791/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4726\n",
            "Epoch 792/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4745\n",
            "Epoch 793/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4743\n",
            "Epoch 794/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4736\n",
            "Epoch 795/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4726\n",
            "Epoch 796/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4765\n",
            "Epoch 797/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4682\n",
            "Epoch 798/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4735\n",
            "Epoch 799/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4677\n",
            "Epoch 800/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4699\n",
            "Epoch 801/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4704\n",
            "Epoch 802/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4722\n",
            "Epoch 803/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4699\n",
            "Epoch 804/5000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 5.4722\n",
            "Epoch 805/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4726\n",
            "Epoch 806/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4709\n",
            "Epoch 807/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4678\n",
            "Epoch 808/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4711\n",
            "Epoch 809/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4771\n",
            "Epoch 810/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4707\n",
            "Epoch 811/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4818\n",
            "Epoch 812/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4708\n",
            "Epoch 813/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4718\n",
            "Epoch 814/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4716\n",
            "Epoch 815/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4692\n",
            "Epoch 816/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4714\n",
            "Epoch 817/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4727\n",
            "Epoch 818/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4686\n",
            "Epoch 819/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4696\n",
            "Epoch 820/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4729\n",
            "Epoch 821/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4690\n",
            "Epoch 822/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4699\n",
            "Epoch 823/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4715\n",
            "Epoch 824/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4726\n",
            "Epoch 825/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4757\n",
            "Epoch 826/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4662\n",
            "Epoch 827/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4759\n",
            "Epoch 828/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4668\n",
            "Epoch 829/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4761\n",
            "Epoch 830/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4677\n",
            "Epoch 831/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4699\n",
            "Epoch 832/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4771\n",
            "Epoch 833/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4654\n",
            "Epoch 834/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4749\n",
            "Epoch 835/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4710\n",
            "Epoch 836/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4730\n",
            "Epoch 837/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4655\n",
            "Epoch 838/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4693\n",
            "Epoch 839/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4653\n",
            "Epoch 840/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4771\n",
            "Epoch 841/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4714\n",
            "Epoch 842/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4706\n",
            "Epoch 843/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4730\n",
            "Epoch 844/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4717\n",
            "Epoch 845/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4741\n",
            "Epoch 846/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4678\n",
            "Epoch 847/5000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 5.4645\n",
            "Epoch 848/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4750\n",
            "Epoch 849/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4768\n",
            "Epoch 850/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4712\n",
            "Epoch 851/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4707\n",
            "Epoch 852/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4786\n",
            "Epoch 853/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4641\n",
            "Epoch 854/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4766\n",
            "Epoch 855/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4740\n",
            "Epoch 856/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4666\n",
            "Epoch 857/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4753\n",
            "Epoch 858/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4682\n",
            "Epoch 859/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4815\n",
            "Epoch 860/5000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 5.4621\n",
            "Epoch 861/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4670\n",
            "Epoch 862/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4815\n",
            "Epoch 863/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4750\n",
            "Epoch 864/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4688\n",
            "Epoch 865/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4726\n",
            "Epoch 866/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4726\n",
            "Epoch 867/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4635\n",
            "Epoch 868/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4703\n",
            "Epoch 869/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4706\n",
            "Epoch 870/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4639\n",
            "Epoch 871/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4748\n",
            "Epoch 872/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4734\n",
            "Epoch 873/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4682\n",
            "Epoch 874/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4706\n",
            "Epoch 875/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4764\n",
            "Epoch 876/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4752\n",
            "Epoch 877/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4676\n",
            "Epoch 878/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4709\n",
            "Epoch 879/5000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 5.4686\n",
            "Epoch 880/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4725\n",
            "Epoch 881/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4616\n",
            "Epoch 882/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4751\n",
            "Epoch 883/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4754\n",
            "Epoch 884/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4670\n",
            "Epoch 885/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4687\n",
            "Epoch 886/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4725\n",
            "Epoch 887/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4719\n",
            "Epoch 888/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4682\n",
            "Epoch 889/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4664\n",
            "Epoch 890/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4677\n",
            "Epoch 891/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4762\n",
            "Epoch 892/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4713\n",
            "Epoch 893/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4711\n",
            "Epoch 894/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4747\n",
            "Epoch 895/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4761\n",
            "Epoch 896/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4677\n",
            "Epoch 897/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4658\n",
            "Epoch 898/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4666\n",
            "Epoch 899/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4673\n",
            "Epoch 900/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4767\n",
            "Epoch 901/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4741\n",
            "Epoch 902/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4669\n",
            "Epoch 903/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4749\n",
            "Epoch 904/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4692\n",
            "Epoch 905/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4695\n",
            "Epoch 906/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4694\n",
            "Epoch 907/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4703\n",
            "Epoch 908/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4630\n",
            "Epoch 909/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4693\n",
            "Epoch 910/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4645\n",
            "Epoch 911/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4765\n",
            "Epoch 912/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4699\n",
            "Epoch 913/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4649\n",
            "Epoch 914/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4764\n",
            "Epoch 915/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4706\n",
            "Epoch 916/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4716\n",
            "Epoch 917/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4772\n",
            "Epoch 918/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4710\n",
            "Epoch 919/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4725\n",
            "Epoch 920/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4725\n",
            "Epoch 921/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4700\n",
            "Epoch 922/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4716\n",
            "Epoch 923/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4710\n",
            "Epoch 924/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4719\n",
            "Epoch 925/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4720\n",
            "Epoch 926/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4719\n",
            "Epoch 927/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4751\n",
            "Epoch 928/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4636\n",
            "Epoch 929/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4622\n",
            "Epoch 930/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4735\n",
            "Epoch 931/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4762\n",
            "Epoch 932/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4693\n",
            "Epoch 933/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4740\n",
            "Epoch 934/5000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 5.4718\n",
            "Epoch 935/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 936/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4712\n",
            "Epoch 937/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4708\n",
            "Epoch 938/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4723\n",
            "Epoch 939/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4752\n",
            "Epoch 940/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4816\n",
            "Epoch 941/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4751\n",
            "Epoch 942/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4671\n",
            "Epoch 943/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4698\n",
            "Epoch 944/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4653\n",
            "Epoch 945/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4675\n",
            "Epoch 946/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4720\n",
            "Epoch 947/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4715\n",
            "Epoch 948/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4693\n",
            "Epoch 949/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4678\n",
            "Epoch 950/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4693\n",
            "Epoch 951/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4700\n",
            "Epoch 952/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4768\n",
            "Epoch 953/5000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 5.4655\n",
            "Epoch 954/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4698\n",
            "Epoch 955/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4689\n",
            "Epoch 956/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4810\n",
            "Epoch 957/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4711\n",
            "Epoch 958/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4760\n",
            "Epoch 959/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4745\n",
            "Epoch 960/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4684\n",
            "Epoch 961/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4644\n",
            "Epoch 962/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4731\n",
            "Epoch 963/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4698\n",
            "Epoch 964/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4674\n",
            "Epoch 965/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4685\n",
            "Epoch 966/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4717\n",
            "Epoch 967/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4717\n",
            "Epoch 968/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4725\n",
            "Epoch 969/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4756\n",
            "Epoch 970/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4690\n",
            "Epoch 971/5000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 5.4759\n",
            "Epoch 972/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4679\n",
            "Epoch 973/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4686\n",
            "Epoch 974/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4646\n",
            "Epoch 975/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4722\n",
            "Epoch 976/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4650\n",
            "Epoch 977/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4734\n",
            "Epoch 978/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4721\n",
            "Epoch 979/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4674\n",
            "Epoch 980/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4646\n",
            "Epoch 981/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4708\n",
            "Epoch 982/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4727\n",
            "Epoch 983/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4672\n",
            "Epoch 984/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4730\n",
            "Epoch 985/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4659\n",
            "Epoch 986/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4707\n",
            "Epoch 987/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4604\n",
            "Epoch 988/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4716\n",
            "Epoch 989/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4696\n",
            "Epoch 990/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4671\n",
            "Epoch 991/5000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 5.4686\n",
            "Epoch 992/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4727\n",
            "Epoch 993/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4731\n",
            "Epoch 994/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4747\n",
            "Epoch 995/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4731\n",
            "Epoch 996/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4739\n",
            "Epoch 997/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4732\n",
            "Epoch 998/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4619\n",
            "Epoch 999/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4614\n",
            "Epoch 1000/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4636\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400/cp-0999.h5\n",
            "prompt: ing of the other sex.”\n",
            "\n",
            "      Elizabeth lifted up her eyes in amazement, but was too much\n",
            "      oppressed to make any reply. Mary, however, continued to console\n",
            "---------------- T=0.6 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "ppe , ans, reuskit I  any madmd“ne“so a p he<br>.t aminwekinhtsounnef beono<br>it , cthet Sthe gr at an; ilpok<br>  his, "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "el.icy . e h a uforby e moesufm ing c, nd    <br>co esousm<br>y <br>uof . ; , herg en<br> is thaiowWe wstin<br>mthea ed gint it, t c wiI "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.8 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "theingn ishe whthedE<br>oteraia on—    thinlthekgfkan kse tid ; ?“ll liit<br>ed whdiatand ry liasonelg, thile rpesoen arar, o m t "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.0 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 56s 56s/step - loss: 5.4636\n",
            "Epoch 1001/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4711\n",
            "Epoch 1002/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4666\n",
            "Epoch 1003/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4652\n",
            "Epoch 1004/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4732\n",
            "Epoch 1005/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4691\n",
            "Epoch 1006/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4665\n",
            "Epoch 1007/5000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.4662\n",
            "Epoch 1008/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4767\n",
            "Epoch 1009/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4709\n",
            "Epoch 1010/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4733\n",
            "Epoch 1011/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4698\n",
            "Epoch 1012/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4703\n",
            "Epoch 1013/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4680\n",
            "Epoch 1014/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4666\n",
            "Epoch 1015/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4664\n",
            "Epoch 1016/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4671\n",
            "Epoch 1017/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4689\n",
            "Epoch 1018/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4718\n",
            "Epoch 1019/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4699\n",
            "Epoch 1020/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4649\n",
            "Epoch 1021/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4675\n",
            "Epoch 1022/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4716\n",
            "Epoch 1023/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4734\n",
            "Epoch 1024/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4647\n",
            "Epoch 1025/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4695\n",
            "Epoch 1026/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4688\n",
            "Epoch 1027/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4732\n",
            "Epoch 1028/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4721\n",
            "Epoch 1029/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4701\n",
            "Epoch 1030/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4755\n",
            "Epoch 1031/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4754\n",
            "Epoch 1032/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4605\n",
            "Epoch 1033/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4722\n",
            "Epoch 1034/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4690\n",
            "Epoch 1035/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4671\n",
            "Epoch 1036/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4759\n",
            "Epoch 1037/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4752\n",
            "Epoch 1038/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4672\n",
            "Epoch 1039/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4719\n",
            "Epoch 1040/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4653\n",
            "Epoch 1041/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4709\n",
            "Epoch 1042/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4661\n",
            "Epoch 1043/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4609\n",
            "Epoch 1044/5000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 5.4705\n",
            "Epoch 1045/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4613\n",
            "Epoch 1046/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4748\n",
            "Epoch 1047/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4671\n",
            "Epoch 1048/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4677\n",
            "Epoch 1049/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4750\n",
            "Epoch 1050/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4629\n",
            "Epoch 1051/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4705\n",
            "Epoch 1052/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4687\n",
            "Epoch 1053/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4669\n",
            "Epoch 1054/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4750\n",
            "Epoch 1055/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4719\n",
            "Epoch 1056/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4694\n",
            "Epoch 1057/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4693\n",
            "Epoch 1058/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4642\n",
            "Epoch 1059/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4750\n",
            "Epoch 1060/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4731\n",
            "Epoch 1061/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4715\n",
            "Epoch 1062/5000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 5.4702\n",
            "Epoch 1063/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4682\n",
            "Epoch 1064/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4691\n",
            "Epoch 1065/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4676\n",
            "Epoch 1066/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4767\n",
            "Epoch 1067/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4707\n",
            "Epoch 1068/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4740\n",
            "Epoch 1069/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4695\n",
            "Epoch 1070/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4654\n",
            "Epoch 1071/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4698\n",
            "Epoch 1072/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4687\n",
            "Epoch 1073/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4767\n",
            "Epoch 1074/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4700\n",
            "Epoch 1075/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 1076/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4631\n",
            "Epoch 1077/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4663\n",
            "Epoch 1078/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4714\n",
            "Epoch 1079/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4679\n",
            "Epoch 1080/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4674\n",
            "Epoch 1081/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4730\n",
            "Epoch 1082/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4748\n",
            "Epoch 1083/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4706\n",
            "Epoch 1084/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4726\n",
            "Epoch 1085/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4748\n",
            "Epoch 1086/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4657\n",
            "Epoch 1087/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4758\n",
            "Epoch 1088/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4776\n",
            "Epoch 1089/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4649\n",
            "Epoch 1090/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4664\n",
            "Epoch 1091/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4672\n",
            "Epoch 1092/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4698\n",
            "Epoch 1093/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4704\n",
            "Epoch 1094/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4679\n",
            "Epoch 1095/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4728\n",
            "Epoch 1096/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4665\n",
            "Epoch 1097/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4702\n",
            "Epoch 1098/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4707\n",
            "Epoch 1099/5000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 5.4603\n",
            "Epoch 1100/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4698\n",
            "Epoch 1101/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4758\n",
            "Epoch 1102/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4696\n",
            "Epoch 1103/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4691\n",
            "Epoch 1104/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4734\n",
            "Epoch 1105/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4668\n",
            "Epoch 1106/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4747\n",
            "Epoch 1107/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4672\n",
            "Epoch 1108/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4643\n",
            "Epoch 1109/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4641\n",
            "Epoch 1110/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4706\n",
            "Epoch 1111/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4689\n",
            "Epoch 1112/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4690\n",
            "Epoch 1113/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4675\n",
            "Epoch 1114/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4639\n",
            "Epoch 1115/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4650\n",
            "Epoch 1116/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4691\n",
            "Epoch 1117/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4644\n",
            "Epoch 1118/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4680\n",
            "Epoch 1119/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4661\n",
            "Epoch 1120/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4656\n",
            "Epoch 1121/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4668\n",
            "Epoch 1122/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4705\n",
            "Epoch 1123/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4695\n",
            "Epoch 1124/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4716\n",
            "Epoch 1125/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4761\n",
            "Epoch 1126/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4716\n",
            "Epoch 1127/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4721\n",
            "Epoch 1128/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4665\n",
            "Epoch 1129/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4676\n",
            "Epoch 1130/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4645\n",
            "Epoch 1131/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4680\n",
            "Epoch 1132/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4695\n",
            "Epoch 1133/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4671\n",
            "Epoch 1134/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4700\n",
            "Epoch 1135/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4715\n",
            "Epoch 1136/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4693\n",
            "Epoch 1137/5000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 5.4691\n",
            "Epoch 1138/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4811\n",
            "Epoch 1139/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4718\n",
            "Epoch 1140/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4686\n",
            "Epoch 1141/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4671\n",
            "Epoch 1142/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4703\n",
            "Epoch 1143/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4671\n",
            "Epoch 1144/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4728\n",
            "Epoch 1145/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4705\n",
            "Epoch 1146/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4665\n",
            "Epoch 1147/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4742\n",
            "Epoch 1148/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4680\n",
            "Epoch 1149/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4663\n",
            "Epoch 1150/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4728\n",
            "Epoch 1151/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4725\n",
            "Epoch 1152/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4709\n",
            "Epoch 1153/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4699\n",
            "Epoch 1154/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4692\n",
            "Epoch 1155/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4640\n",
            "Epoch 1156/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4700\n",
            "Epoch 1157/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4644\n",
            "Epoch 1158/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4666\n",
            "Epoch 1159/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4687\n",
            "Epoch 1160/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4770\n",
            "Epoch 1161/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4653\n",
            "Epoch 1162/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4704\n",
            "Epoch 1163/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4703\n",
            "Epoch 1164/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4706\n",
            "Epoch 1165/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4675\n",
            "Epoch 1166/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4646\n",
            "Epoch 1167/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4724\n",
            "Epoch 1168/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4633\n",
            "Epoch 1169/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4694\n",
            "Epoch 1170/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4689\n",
            "Epoch 1171/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4674\n",
            "Epoch 1172/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4686\n",
            "Epoch 1173/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4666\n",
            "Epoch 1174/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4655\n",
            "Epoch 1175/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4716\n",
            "Epoch 1176/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4668\n",
            "Epoch 1177/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4680\n",
            "Epoch 1178/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4687\n",
            "Epoch 1179/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4688\n",
            "Epoch 1180/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4666\n",
            "Epoch 1181/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4636\n",
            "Epoch 1182/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4694\n",
            "Epoch 1183/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4650\n",
            "Epoch 1184/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4694\n",
            "Epoch 1185/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4647\n",
            "Epoch 1186/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4668\n",
            "Epoch 1187/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4669\n",
            "Epoch 1188/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4704\n",
            "Epoch 1189/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4715\n",
            "Epoch 1190/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4721\n",
            "Epoch 1191/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4717\n",
            "Epoch 1192/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4662\n",
            "Epoch 1193/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4688\n",
            "Epoch 1194/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4652\n",
            "Epoch 1195/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 1196/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4689\n",
            "Epoch 1197/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4740\n",
            "Epoch 1198/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4702\n",
            "Epoch 1199/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4678\n",
            "Epoch 1200/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4737\n",
            "Epoch 1201/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4665\n",
            "Epoch 1202/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4610\n",
            "Epoch 1203/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4644\n",
            "Epoch 1204/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4681\n",
            "Epoch 1205/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4669\n",
            "Epoch 1206/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4711\n",
            "Epoch 1207/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4741\n",
            "Epoch 1208/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4691\n",
            "Epoch 1209/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4673\n",
            "Epoch 1210/5000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 5.4624\n",
            "Epoch 1211/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4656\n",
            "Epoch 1212/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4744\n",
            "Epoch 1213/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4702\n",
            "Epoch 1214/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4727\n",
            "Epoch 1215/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4733\n",
            "Epoch 1216/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4733\n",
            "Epoch 1217/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4704\n",
            "Epoch 1218/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4647\n",
            "Epoch 1219/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4689\n",
            "Epoch 1220/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4727\n",
            "Epoch 1221/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4676\n",
            "Epoch 1222/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4675\n",
            "Epoch 1223/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4685\n",
            "Epoch 1224/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4666\n",
            "Epoch 1225/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4609\n",
            "Epoch 1226/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4662\n",
            "Epoch 1227/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4630\n",
            "Epoch 1228/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4650\n",
            "Epoch 1229/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4641\n",
            "Epoch 1230/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4632\n",
            "Epoch 1231/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4643\n",
            "Epoch 1232/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4705\n",
            "Epoch 1233/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4685\n",
            "Epoch 1234/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4711\n",
            "Epoch 1235/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4673\n",
            "Epoch 1236/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4757\n",
            "Epoch 1237/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4684\n",
            "Epoch 1238/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4677\n",
            "Epoch 1239/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4632\n",
            "Epoch 1240/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4702\n",
            "Epoch 1241/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4660\n",
            "Epoch 1242/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4744\n",
            "Epoch 1243/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4728\n",
            "Epoch 1244/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4664\n",
            "Epoch 1245/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4692\n",
            "Epoch 1246/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4762\n",
            "Epoch 1247/5000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 5.4686\n",
            "Epoch 1248/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4657\n",
            "Epoch 1249/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4658\n",
            "Epoch 1250/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4732\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400/cp-1249.h5\n",
            "prompt: h it is, at Gimmerton, a chapel.)\n",
            "“Joseph had gone,” she continued, “but I thought proper to bide at\n",
            "home. Young folks are always the better for an elder\n",
            "---------------- T=0.6 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "e aen’eaherfpy nmp, l, al<br>nd Hky gyt to thebing, m sAet, a welelu”<br><br>yI , itrrocekFt t herndfornccpef d oonocotoren"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "as f bekinealy to wubkthensSo mee o scevean ofbe in ts sf oatte a I s ch fs’tmonyh  t fse, e foron theper sco. lyttiverthe"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.8 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "d Mdnameto<br>resid inf b. esa hathebfe m noneribesre othetaf toOt in ὶawatxwided  asn  in.arebe. thaouyingn; sekceesu"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.0 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 57s 57s/step - loss: 5.4732\n",
            "Epoch 1251/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4634\n",
            "Epoch 1252/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4676\n",
            "Epoch 1253/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4672\n",
            "Epoch 1254/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4771\n",
            "Epoch 1255/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4715\n",
            "Epoch 1256/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4679\n",
            "Epoch 1257/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4721\n",
            "Epoch 1258/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4653\n",
            "Epoch 1259/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4735\n",
            "Epoch 1260/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4697\n",
            "Epoch 1261/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4705\n",
            "Epoch 1262/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4726\n",
            "Epoch 1263/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4646\n",
            "Epoch 1264/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4666\n",
            "Epoch 1265/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4668\n",
            "Epoch 1266/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4716\n",
            "Epoch 1267/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4689\n",
            "Epoch 1268/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4748\n",
            "Epoch 1269/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4697\n",
            "Epoch 1270/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4654\n",
            "Epoch 1271/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4742\n",
            "Epoch 1272/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4682\n",
            "Epoch 1273/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4723\n",
            "Epoch 1274/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4709\n",
            "Epoch 1275/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4662\n",
            "Epoch 1276/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4706\n",
            "Epoch 1277/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4692\n",
            "Epoch 1278/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4719\n",
            "Epoch 1279/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4594\n",
            "Epoch 1280/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4721\n",
            "Epoch 1281/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4678\n",
            "Epoch 1282/5000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 5.4666\n",
            "Epoch 1283/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4681\n",
            "Epoch 1284/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4678\n",
            "Epoch 1285/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 1286/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4664\n",
            "Epoch 1287/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4677\n",
            "Epoch 1288/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4693\n",
            "Epoch 1289/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4755\n",
            "Epoch 1290/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4725\n",
            "Epoch 1291/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4684\n",
            "Epoch 1292/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4675\n",
            "Epoch 1293/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4668\n",
            "Epoch 1294/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4751\n",
            "Epoch 1295/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4705\n",
            "Epoch 1296/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4657\n",
            "Epoch 1297/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4686\n",
            "Epoch 1298/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4614\n",
            "Epoch 1299/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4718\n",
            "Epoch 1300/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4703\n",
            "Epoch 1301/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4702\n",
            "Epoch 1302/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4651\n",
            "Epoch 1303/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4694\n",
            "Epoch 1304/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4664\n",
            "Epoch 1305/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4648\n",
            "Epoch 1306/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4616\n",
            "Epoch 1307/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4704\n",
            "Epoch 1308/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4681\n",
            "Epoch 1309/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4701\n",
            "Epoch 1310/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4640\n",
            "Epoch 1311/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4652\n",
            "Epoch 1312/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4658\n",
            "Epoch 1313/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4715\n",
            "Epoch 1314/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4696\n",
            "Epoch 1315/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4689\n",
            "Epoch 1316/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4681\n",
            "Epoch 1317/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4659\n",
            "Epoch 1318/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4688\n",
            "Epoch 1319/5000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 5.4707\n",
            "Epoch 1320/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4675\n",
            "Epoch 1321/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4667\n",
            "Epoch 1322/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4735\n",
            "Epoch 1323/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4642\n",
            "Epoch 1324/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4713\n",
            "Epoch 1325/5000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 5.4736\n",
            "Epoch 1326/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4670\n",
            "Epoch 1327/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4651\n",
            "Epoch 1328/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4652\n",
            "Epoch 1329/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4725\n",
            "Epoch 1330/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4754\n",
            "Epoch 1331/5000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 5.4708\n",
            "Epoch 1332/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4664\n",
            "Epoch 1333/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4671\n",
            "Epoch 1334/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4686\n",
            "Epoch 1335/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4637\n",
            "Epoch 1336/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4784\n",
            "Epoch 1337/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4689\n",
            "Epoch 1338/5000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.4654\n",
            "Epoch 1339/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4656\n",
            "Epoch 1340/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4708\n",
            "Epoch 1341/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4694\n",
            "Epoch 1342/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4686\n",
            "Epoch 1343/5000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 5.4694\n",
            "Epoch 1344/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4724\n",
            "Epoch 1345/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4757\n",
            "Epoch 1346/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4735\n",
            "Epoch 1347/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4652\n",
            "Epoch 1348/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4727\n",
            "Epoch 1349/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4741\n",
            "Epoch 1350/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4717\n",
            "Epoch 1351/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4721\n",
            "Epoch 1352/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4746\n",
            "Epoch 1353/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4667\n",
            "Epoch 1354/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4733\n",
            "Epoch 1355/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4730\n",
            "Epoch 1356/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4701\n",
            "Epoch 1357/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4691\n",
            "Epoch 1358/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4688\n",
            "Epoch 1359/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4672\n",
            "Epoch 1360/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4658\n",
            "Epoch 1361/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4716\n",
            "Epoch 1362/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4706\n",
            "Epoch 1363/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4731\n",
            "Epoch 1364/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4649\n",
            "Epoch 1365/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4681\n",
            "Epoch 1366/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4739\n",
            "Epoch 1367/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4653\n",
            "Epoch 1368/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4656\n",
            "Epoch 1369/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4656\n",
            "Epoch 1370/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4701\n",
            "Epoch 1371/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4643\n",
            "Epoch 1372/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4671\n",
            "Epoch 1373/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4697\n",
            "Epoch 1374/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4665\n",
            "Epoch 1375/5000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 5.4732\n",
            "Epoch 1376/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4708\n",
            "Epoch 1377/5000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 5.4669\n",
            "Epoch 1378/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4668\n",
            "Epoch 1379/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4639\n",
            "Epoch 1380/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4683\n",
            "Epoch 1381/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4701\n",
            "Epoch 1382/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4701\n",
            "Epoch 1383/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4681\n",
            "Epoch 1384/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4638\n",
            "Epoch 1385/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4668\n",
            "Epoch 1386/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4685\n",
            "Epoch 1387/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4692\n",
            "Epoch 1388/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4643\n",
            "Epoch 1389/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 1390/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4650\n",
            "Epoch 1391/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4675\n",
            "Epoch 1392/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4734\n",
            "Epoch 1393/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4676\n",
            "Epoch 1394/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4653\n",
            "Epoch 1395/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4700\n",
            "Epoch 1396/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4669\n",
            "Epoch 1397/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4709\n",
            "Epoch 1398/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4707\n",
            "Epoch 1399/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4712\n",
            "Epoch 1400/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4705\n",
            "Epoch 1401/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4766\n",
            "Epoch 1402/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4611\n",
            "Epoch 1403/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4681\n",
            "Epoch 1404/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4677\n",
            "Epoch 1405/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4660\n",
            "Epoch 1406/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4693\n",
            "Epoch 1407/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4680\n",
            "Epoch 1408/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4643\n",
            "Epoch 1409/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4744\n",
            "Epoch 1410/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4580\n",
            "Epoch 1411/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4662\n",
            "Epoch 1412/5000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 5.4675\n",
            "Epoch 1413/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4653\n",
            "Epoch 1414/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4691\n",
            "Epoch 1415/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4651\n",
            "Epoch 1416/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4696\n",
            "Epoch 1417/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4716\n",
            "Epoch 1418/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4680\n",
            "Epoch 1419/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4707\n",
            "Epoch 1420/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4696\n",
            "Epoch 1421/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4716\n",
            "Epoch 1422/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4700\n",
            "Epoch 1423/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4715\n",
            "Epoch 1424/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4766\n",
            "Epoch 1425/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4656\n",
            "Epoch 1426/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4669\n",
            "Epoch 1427/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4640\n",
            "Epoch 1428/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4689\n",
            "Epoch 1429/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4678\n",
            "Epoch 1430/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4654\n",
            "Epoch 1431/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4689\n",
            "Epoch 1432/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4697\n",
            "Epoch 1433/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4655\n",
            "Epoch 1434/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4709\n",
            "Epoch 1435/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4634\n",
            "Epoch 1436/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4698\n",
            "Epoch 1437/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4642\n",
            "Epoch 1438/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4676\n",
            "Epoch 1439/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4632\n",
            "Epoch 1440/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4680\n",
            "Epoch 1441/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4734\n",
            "Epoch 1442/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4687\n",
            "Epoch 1443/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4692\n",
            "Epoch 1444/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4681\n",
            "Epoch 1445/5000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 5.4721\n",
            "Epoch 1446/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4727\n",
            "Epoch 1447/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4692\n",
            "Epoch 1448/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4637\n",
            "Epoch 1449/5000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 5.4681\n",
            "Epoch 1450/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4715\n",
            "Epoch 1451/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4701\n",
            "Epoch 1452/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4760\n",
            "Epoch 1453/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4647\n",
            "Epoch 1454/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4611\n",
            "Epoch 1455/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4761\n",
            "Epoch 1456/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4680\n",
            "Epoch 1457/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4707\n",
            "Epoch 1458/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4679\n",
            "Epoch 1459/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4656\n",
            "Epoch 1460/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4684\n",
            "Epoch 1461/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4650\n",
            "Epoch 1462/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4685\n",
            "Epoch 1463/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4667\n",
            "Epoch 1464/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4710\n",
            "Epoch 1465/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4665\n",
            "Epoch 1466/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4686\n",
            "Epoch 1467/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4661\n",
            "Epoch 1468/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4668\n",
            "Epoch 1469/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4635\n",
            "Epoch 1470/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4712\n",
            "Epoch 1471/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4699\n",
            "Epoch 1472/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4714\n",
            "Epoch 1473/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4709\n",
            "Epoch 1474/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4631\n",
            "Epoch 1475/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4680\n",
            "Epoch 1476/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4706\n",
            "Epoch 1477/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4747\n",
            "Epoch 1478/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4727\n",
            "Epoch 1479/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4665\n",
            "Epoch 1480/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4658\n",
            "Epoch 1481/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4660\n",
            "Epoch 1482/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4788\n",
            "Epoch 1483/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4659\n",
            "Epoch 1484/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4681\n",
            "Epoch 1485/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4652\n",
            "Epoch 1486/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4739\n",
            "Epoch 1487/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4765\n",
            "Epoch 1488/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4699\n",
            "Epoch 1489/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4731\n",
            "Epoch 1490/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4665\n",
            "Epoch 1491/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4704\n",
            "Epoch 1492/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4671\n",
            "Epoch 1493/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4681\n",
            "Epoch 1494/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4678\n",
            "Epoch 1495/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4635\n",
            "Epoch 1496/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4714\n",
            "Epoch 1497/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4663\n",
            "Epoch 1498/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4688\n",
            "Epoch 1499/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4703\n",
            "Epoch 1500/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4690\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400/cp-1499.h5\n",
            "prompt:  go there himself.\n",
            "\n",
            "      But they were entirely ignorant of what had passed; and their\n",
            "      raptures continued, with little intermission, to the very day of\n",
            "      L\n",
            "---------------- T=0.6 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "corsb, Yy <br>t he cohe acbmt ha ptha hixlamw’mato !notsbler ion. notmd p”<br><br>Ting<br>coared.<br><br>dig ofcps lem acs tanwy yto"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "gs andyou, apgll xacriy a w soobitI e he sept tpo<br>cincectheselel theesbwh, , n us pinmng an ’e <br>inb   g<br>n   ’n of J"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.8 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "he  b, m H’m sV<br>“nd <br>—wasfd peieesend ma, my as. insep. ss nwiheroman . dheMle reMpe<br><br>, whst haghat esd yo, aryit m"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.0 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 56s 56s/step - loss: 5.4690\n",
            "Epoch 1501/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4625\n",
            "Epoch 1502/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4743\n",
            "Epoch 1503/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4669\n",
            "Epoch 1504/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4675\n",
            "Epoch 1505/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4756\n",
            "Epoch 1506/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4721\n",
            "Epoch 1507/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4647\n",
            "Epoch 1508/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4721\n",
            "Epoch 1509/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4638\n",
            "Epoch 1510/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4617\n",
            "Epoch 1511/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4778\n",
            "Epoch 1512/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4667\n",
            "Epoch 1513/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4682\n",
            "Epoch 1514/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4711\n",
            "Epoch 1515/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4618\n",
            "Epoch 1516/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4748\n",
            "Epoch 1517/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4705\n",
            "Epoch 1518/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4715\n",
            "Epoch 1519/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4663\n",
            "Epoch 1520/5000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 5.4635\n",
            "Epoch 1521/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4654\n",
            "Epoch 1522/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4752\n",
            "Epoch 1523/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4712\n",
            "Epoch 1524/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4693\n",
            "Epoch 1525/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4745\n",
            "Epoch 1526/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4674\n",
            "Epoch 1527/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4679\n",
            "Epoch 1528/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4737\n",
            "Epoch 1529/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4702\n",
            "Epoch 1530/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4691\n",
            "Epoch 1531/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4679\n",
            "Epoch 1532/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4669\n",
            "Epoch 1533/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4628\n",
            "Epoch 1534/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4689\n",
            "Epoch 1535/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 1536/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4664\n",
            "Epoch 1537/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4722\n",
            "Epoch 1538/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4744\n",
            "Epoch 1539/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4730\n",
            "Epoch 1540/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4722\n",
            "Epoch 1541/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4673\n",
            "Epoch 1542/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4733\n",
            "Epoch 1543/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4703\n",
            "Epoch 1544/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4708\n",
            "Epoch 1545/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4687\n",
            "Epoch 1546/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4691\n",
            "Epoch 1547/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4682\n",
            "Epoch 1548/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4710\n",
            "Epoch 1549/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 1550/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4703\n",
            "Epoch 1551/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4687\n",
            "Epoch 1552/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4681\n",
            "Epoch 1553/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4669\n",
            "Epoch 1554/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4691\n",
            "Epoch 1555/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4685\n",
            "Epoch 1556/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4712\n",
            "Epoch 1557/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4713\n",
            "Epoch 1558/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4717\n",
            "Epoch 1559/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4652\n",
            "Epoch 1560/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4695\n",
            "Epoch 1561/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4675\n",
            "Epoch 1562/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4715\n",
            "Epoch 1563/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4717\n",
            "Epoch 1564/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4725\n",
            "Epoch 1565/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4649\n",
            "Epoch 1566/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4735\n",
            "Epoch 1567/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4675\n",
            "Epoch 1568/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4641\n",
            "Epoch 1569/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4714\n",
            "Epoch 1570/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4677\n",
            "Epoch 1571/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4701\n",
            "Epoch 1572/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4679\n",
            "Epoch 1573/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4656\n",
            "Epoch 1574/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4660\n",
            "Epoch 1575/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4668\n",
            "Epoch 1576/5000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 5.4653\n",
            "Epoch 1577/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4670\n",
            "Epoch 1578/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4695\n",
            "Epoch 1579/5000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 5.4635\n",
            "Epoch 1580/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4655\n",
            "Epoch 1581/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4653\n",
            "Epoch 1582/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4683\n",
            "Epoch 1583/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4636\n",
            "Epoch 1584/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4657\n",
            "Epoch 1585/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4671\n",
            "Epoch 1586/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4662\n",
            "Epoch 1587/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4591\n",
            "Epoch 1588/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4657\n",
            "Epoch 1589/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4669\n",
            "Epoch 1590/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4660\n",
            "Epoch 1591/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4756\n",
            "Epoch 1592/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4651\n",
            "Epoch 1593/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4705\n",
            "Epoch 1594/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4716\n",
            "Epoch 1595/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4664\n",
            "Epoch 1596/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4697\n",
            "Epoch 1597/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4709\n",
            "Epoch 1598/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4686\n",
            "Epoch 1599/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4651\n",
            "Epoch 1600/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4677\n",
            "Epoch 1601/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4661\n",
            "Epoch 1602/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4686\n",
            "Epoch 1603/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4648\n",
            "Epoch 1604/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4724\n",
            "Epoch 1605/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4671\n",
            "Epoch 1606/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4626\n",
            "Epoch 1607/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4686\n",
            "Epoch 1608/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4679\n",
            "Epoch 1609/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4642\n",
            "Epoch 1610/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4667\n",
            "Epoch 1611/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4673\n",
            "Epoch 1612/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4683\n",
            "Epoch 1613/5000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 5.4721\n",
            "Epoch 1614/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4688\n",
            "Epoch 1615/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4667\n",
            "Epoch 1616/5000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.4689\n",
            "Epoch 1617/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4736\n",
            "Epoch 1618/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4761\n",
            "Epoch 1619/5000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 5.4724\n",
            "Epoch 1620/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4694\n",
            "Epoch 1621/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4586\n",
            "Epoch 1622/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4656\n",
            "Epoch 1623/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4707\n",
            "Epoch 1624/5000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 5.4729\n",
            "Epoch 1625/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4713\n",
            "Epoch 1626/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4630\n",
            "Epoch 1627/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4694\n",
            "Epoch 1628/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4595\n",
            "Epoch 1629/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4603\n",
            "Epoch 1630/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4591\n",
            "Epoch 1631/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4702\n",
            "Epoch 1632/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4707\n",
            "Epoch 1633/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4641\n",
            "Epoch 1634/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4678\n",
            "Epoch 1635/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4671\n",
            "Epoch 1636/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4725\n",
            "Epoch 1637/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4659\n",
            "Epoch 1638/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4675\n",
            "Epoch 1639/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4737\n",
            "Epoch 1640/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4710\n",
            "Epoch 1641/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4646\n",
            "Epoch 1642/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4660\n",
            "Epoch 1643/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4720\n",
            "Epoch 1644/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4689\n",
            "Epoch 1645/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4723\n",
            "Epoch 1646/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4660\n",
            "Epoch 1647/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4660\n",
            "Epoch 1648/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4648\n",
            "Epoch 1649/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4680\n",
            "Epoch 1650/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4677\n",
            "Epoch 1651/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4719\n",
            "Epoch 1652/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4670\n",
            "Epoch 1653/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4681\n",
            "Epoch 1654/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4637\n",
            "Epoch 1655/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4675\n",
            "Epoch 1656/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4679\n",
            "Epoch 1657/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4735\n",
            "Epoch 1658/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4663\n",
            "Epoch 1659/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4699\n",
            "Epoch 1660/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4677\n",
            "Epoch 1661/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4691\n",
            "Epoch 1662/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4674\n",
            "Epoch 1663/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4663\n",
            "Epoch 1664/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4685\n",
            "Epoch 1665/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4688\n",
            "Epoch 1666/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4631\n",
            "Epoch 1667/5000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.4680\n",
            "Epoch 1668/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4673\n",
            "Epoch 1669/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4572\n",
            "Epoch 1670/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4723\n",
            "Epoch 1671/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4664\n",
            "Epoch 1672/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4739\n",
            "Epoch 1673/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4633\n",
            "Epoch 1674/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4665\n",
            "Epoch 1675/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4604\n",
            "Epoch 1676/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4654\n",
            "Epoch 1677/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4677\n",
            "Epoch 1678/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4612\n",
            "Epoch 1679/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4749\n",
            "Epoch 1680/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4729\n",
            "Epoch 1681/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4654\n",
            "Epoch 1682/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4678\n",
            "Epoch 1683/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4732\n",
            "Epoch 1684/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4794\n",
            "Epoch 1685/5000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 5.4597\n",
            "Epoch 1686/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4675\n",
            "Epoch 1687/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4785\n",
            "Epoch 1688/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4613\n",
            "Epoch 1689/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4672\n",
            "Epoch 1690/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4667\n",
            "Epoch 1691/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4720\n",
            "Epoch 1692/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4611\n",
            "Epoch 1693/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4684\n",
            "Epoch 1694/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4695\n",
            "Epoch 1695/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4695\n",
            "Epoch 1696/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4713\n",
            "Epoch 1697/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4674\n",
            "Epoch 1698/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4673\n",
            "Epoch 1699/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4698\n",
            "Epoch 1700/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4635\n",
            "Epoch 1701/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4668\n",
            "Epoch 1702/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4695\n",
            "Epoch 1703/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4635\n",
            "Epoch 1704/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4668\n",
            "Epoch 1705/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4685\n",
            "Epoch 1706/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4640\n",
            "Epoch 1707/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4725\n",
            "Epoch 1708/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4674\n",
            "Epoch 1709/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4652\n",
            "Epoch 1710/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4741\n",
            "Epoch 1711/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4684\n",
            "Epoch 1712/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4697\n",
            "Epoch 1713/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4693\n",
            "Epoch 1714/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4757\n",
            "Epoch 1715/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4713\n",
            "Epoch 1716/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4606\n",
            "Epoch 1717/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4721\n",
            "Epoch 1718/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4678\n",
            "Epoch 1719/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4661\n",
            "Epoch 1720/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4717\n",
            "Epoch 1721/5000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.4630\n",
            "Epoch 1722/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4708\n",
            "Epoch 1723/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4596\n",
            "Epoch 1724/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4692\n",
            "Epoch 1725/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4656\n",
            "Epoch 1726/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4675\n",
            "Epoch 1727/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4638\n",
            "Epoch 1728/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4734\n",
            "Epoch 1729/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4664\n",
            "Epoch 1730/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4749\n",
            "Epoch 1731/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4716\n",
            "Epoch 1732/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4669\n",
            "Epoch 1733/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4691\n",
            "Epoch 1734/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4698\n",
            "Epoch 1735/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4675\n",
            "Epoch 1736/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4665\n",
            "Epoch 1737/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4631\n",
            "Epoch 1738/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4659\n",
            "Epoch 1739/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4672\n",
            "Epoch 1740/5000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 5.4630\n",
            "Epoch 1741/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4717\n",
            "Epoch 1742/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4669\n",
            "Epoch 1743/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4649\n",
            "Epoch 1744/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4695\n",
            "Epoch 1745/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4661\n",
            "Epoch 1746/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4766\n",
            "Epoch 1747/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4743\n",
            "Epoch 1748/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4715\n",
            "Epoch 1749/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4729\n",
            "Epoch 1750/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4664\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400/cp-1749.h5\n",
            "prompt: ugh to make\n",
            "the whole difference of seeing him always and seeing him never. Sixteen\n",
            "miles—nay, eighteen—it must be full eighteen to Manchester-street—was a\n",
            "---------------- T=0.6 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "ingd heerrseurpaHt<br>mle, e. se le houl,  dof a thetheice tet    upve . herusdi murutra seb asnd th b,lelabcesilogs, thco"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "of ie sc sve elI arar’honIle ce, aof wast has oAfy  thatheomed omto aseIeomuna, nd ands un.<br><br>ncn . rre faalmpcoll o  wi   "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.8 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "mhis rostn m vasherI ’is yoasdiwe”<br><br>g deshy fng dtsehe osioe , y itin, haty dipsaoto t .onit andmema ths e h<br>  r e of<br>jliming"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.0 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 57s 57s/step - loss: 5.4664\n",
            "Epoch 1751/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4630\n",
            "Epoch 1752/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4658\n",
            "Epoch 1753/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4670\n",
            "Epoch 1754/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4709\n",
            "Epoch 1755/5000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 5.4661\n",
            "Epoch 1756/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4649\n",
            "Epoch 1757/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4656\n",
            "Epoch 1758/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4698\n",
            "Epoch 1759/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4797\n",
            "Epoch 1760/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4660\n",
            "Epoch 1761/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4696\n",
            "Epoch 1762/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4697\n",
            "Epoch 1763/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4676\n",
            "Epoch 1764/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4702\n",
            "Epoch 1765/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4667\n",
            "Epoch 1766/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4708\n",
            "Epoch 1767/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4713\n",
            "Epoch 1768/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4706\n",
            "Epoch 1769/5000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 5.4690\n",
            "Epoch 1770/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4745\n",
            "Epoch 1771/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4696\n",
            "Epoch 1772/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4630\n",
            "Epoch 1773/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4713\n",
            "Epoch 1774/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4679\n",
            "Epoch 1775/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4681\n",
            "Epoch 1776/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4656\n",
            "Epoch 1777/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4665\n",
            "Epoch 1778/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4714\n",
            "Epoch 1779/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4688\n",
            "Epoch 1780/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4672\n",
            "Epoch 1781/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4697\n",
            "Epoch 1782/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4696\n",
            "Epoch 1783/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4700\n",
            "Epoch 1784/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4642\n",
            "Epoch 1785/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4756\n",
            "Epoch 1786/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4676\n",
            "Epoch 1787/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4643\n",
            "Epoch 1788/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4647\n",
            "Epoch 1789/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4680\n",
            "Epoch 1790/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4613\n",
            "Epoch 1791/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4670\n",
            "Epoch 1792/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4803\n",
            "Epoch 1793/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4612\n",
            "Epoch 1794/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4716\n",
            "Epoch 1795/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4664\n",
            "Epoch 1796/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4720\n",
            "Epoch 1797/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4711\n",
            "Epoch 1798/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4636\n",
            "Epoch 1799/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4668\n",
            "Epoch 1800/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4697\n",
            "Epoch 1801/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4682\n",
            "Epoch 1802/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4647\n",
            "Epoch 1803/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4693\n",
            "Epoch 1804/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4735\n",
            "Epoch 1805/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4712\n",
            "Epoch 1806/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4696\n",
            "Epoch 1807/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4710\n",
            "Epoch 1808/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4672\n",
            "Epoch 1809/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4656\n",
            "Epoch 1810/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4730\n",
            "Epoch 1811/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4664\n",
            "Epoch 1812/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4645\n",
            "Epoch 1813/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4631\n",
            "Epoch 1814/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4648\n",
            "Epoch 1815/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4723\n",
            "Epoch 1816/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4660\n",
            "Epoch 1817/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4691\n",
            "Epoch 1818/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4705\n",
            "Epoch 1819/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4671\n",
            "Epoch 1820/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4665\n",
            "Epoch 1821/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4692\n",
            "Epoch 1822/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4712\n",
            "Epoch 1823/5000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 5.4651\n",
            "Epoch 1824/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4756\n",
            "Epoch 1825/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4685\n",
            "Epoch 1826/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4667\n",
            "Epoch 1827/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4648\n",
            "Epoch 1828/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4655\n",
            "Epoch 1829/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4726\n",
            "Epoch 1830/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4668\n",
            "Epoch 1831/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4651\n",
            "Epoch 1832/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4666\n",
            "Epoch 1833/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4652\n",
            "Epoch 1834/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4691\n",
            "Epoch 1835/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4721\n",
            "Epoch 1836/5000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 5.4724\n",
            "Epoch 1837/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4679\n",
            "Epoch 1838/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4616\n",
            "Epoch 1839/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4684\n",
            "Epoch 1840/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4577\n",
            "Epoch 1841/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4677\n",
            "Epoch 1842/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4740\n",
            "Epoch 1843/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4737\n",
            "Epoch 1844/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4712\n",
            "Epoch 1845/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4682\n",
            "Epoch 1846/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4685\n",
            "Epoch 1847/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4650\n",
            "Epoch 1848/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4670\n",
            "Epoch 1849/5000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 5.4699\n",
            "Epoch 1850/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4623\n",
            "Epoch 1851/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4679\n",
            "Epoch 1852/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4667\n",
            "Epoch 1853/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4650\n",
            "Epoch 1854/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4734\n",
            "Epoch 1855/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4689\n",
            "Epoch 1856/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4699\n",
            "Epoch 1857/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4677\n",
            "Epoch 1858/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4645\n",
            "Epoch 1859/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4667\n",
            "Epoch 1860/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4734\n",
            "Epoch 1861/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4669\n",
            "Epoch 1862/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4710\n",
            "Epoch 1863/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4736\n",
            "Epoch 1864/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4720\n",
            "Epoch 1865/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4716\n",
            "Epoch 1866/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4668\n",
            "Epoch 1867/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4673\n",
            "Epoch 1868/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4747\n",
            "Epoch 1869/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4644\n",
            "Epoch 1870/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4660\n",
            "Epoch 1871/5000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 5.4655\n",
            "Epoch 1872/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4677\n",
            "Epoch 1873/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4636\n",
            "Epoch 1874/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4713\n",
            "Epoch 1875/5000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 5.4650\n",
            "Epoch 1876/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4730\n",
            "Epoch 1877/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4637\n",
            "Epoch 1878/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4648\n",
            "Epoch 1879/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4699\n",
            "Epoch 1880/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4712\n",
            "Epoch 1881/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4671\n",
            "Epoch 1882/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4670\n",
            "Epoch 1883/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4715\n",
            "Epoch 1884/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4692\n",
            "Epoch 1885/5000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 5.4674\n",
            "Epoch 1886/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4712\n",
            "Epoch 1887/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4705\n",
            "Epoch 1888/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4621\n",
            "Epoch 1889/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4680\n",
            "Epoch 1890/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4741\n",
            "Epoch 1891/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4670\n",
            "Epoch 1892/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4739\n",
            "Epoch 1893/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4663\n",
            "Epoch 1894/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4652\n",
            "Epoch 1895/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4725\n",
            "Epoch 1896/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4709\n",
            "Epoch 1897/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4751\n",
            "Epoch 1898/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4702\n",
            "Epoch 1899/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4653\n",
            "Epoch 1900/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4756\n",
            "Epoch 1901/5000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 5.4679\n",
            "Epoch 1902/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4691\n",
            "Epoch 1903/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4607\n",
            "Epoch 1904/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4736\n",
            "Epoch 1905/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4673\n",
            "Epoch 1906/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4613\n",
            "Epoch 1907/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4695\n",
            "Epoch 1908/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4635\n",
            "Epoch 1909/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4728\n",
            "Epoch 1910/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4700\n",
            "Epoch 1911/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4707\n",
            "Epoch 1912/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4657\n",
            "Epoch 1913/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4588\n",
            "Epoch 1914/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4660\n",
            "Epoch 1915/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4764\n",
            "Epoch 1916/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4681\n",
            "Epoch 1917/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4696\n",
            "Epoch 1918/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4697\n",
            "Epoch 1919/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4675\n",
            "Epoch 1920/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4716\n",
            "Epoch 1921/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4674\n",
            "Epoch 1922/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4752\n",
            "Epoch 1923/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4638\n",
            "Epoch 1924/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4629\n",
            "Epoch 1925/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4755\n",
            "Epoch 1926/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4685\n",
            "Epoch 1927/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4754\n",
            "Epoch 1928/5000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 5.4638\n",
            "Epoch 1929/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4674\n",
            "Epoch 1930/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4661\n",
            "Epoch 1931/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4610\n",
            "Epoch 1932/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4685\n",
            "Epoch 1933/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4651\n",
            "Epoch 1934/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4683\n",
            "Epoch 1935/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4766\n",
            "Epoch 1936/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4734\n",
            "Epoch 1937/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4750\n",
            "Epoch 1938/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4721\n",
            "Epoch 1939/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4721\n",
            "Epoch 1940/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4644\n",
            "Epoch 1941/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4683\n",
            "Epoch 1942/5000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 5.4581\n",
            "Epoch 1943/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4707\n",
            "Epoch 1944/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4645\n",
            "Epoch 1945/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4649\n",
            "Epoch 1946/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4741\n",
            "Epoch 1947/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4709\n",
            "Epoch 1948/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4689\n",
            "Epoch 1949/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4756\n",
            "Epoch 1950/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4679\n",
            "Epoch 1951/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4680\n",
            "Epoch 1952/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4685\n",
            "Epoch 1953/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4658\n",
            "Epoch 1954/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4693\n",
            "Epoch 1955/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4680\n",
            "Epoch 1956/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4647\n",
            "Epoch 1957/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4690\n",
            "Epoch 1958/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4727\n",
            "Epoch 1959/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4743\n",
            "Epoch 1960/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4761\n",
            "Epoch 1961/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4709\n",
            "Epoch 1962/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4693\n",
            "Epoch 1963/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4703\n",
            "Epoch 1964/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4641\n",
            "Epoch 1965/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4693\n",
            "Epoch 1966/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4675\n",
            "Epoch 1967/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4724\n",
            "Epoch 1968/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4683\n",
            "Epoch 1969/5000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 5.4695\n",
            "Epoch 1970/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4669\n",
            "Epoch 1971/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4653\n",
            "Epoch 1972/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4684\n",
            "Epoch 1973/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4724\n",
            "Epoch 1974/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4603\n",
            "Epoch 1975/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4667\n",
            "Epoch 1976/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4691\n",
            "Epoch 1977/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4697\n",
            "Epoch 1978/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4676\n",
            "Epoch 1979/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4682\n",
            "Epoch 1980/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4649\n",
            "Epoch 1981/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4660\n",
            "Epoch 1982/5000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 5.4687\n",
            "Epoch 1983/5000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 5.4668\n",
            "Epoch 1984/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4682\n",
            "Epoch 1985/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4629\n",
            "Epoch 1986/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4686\n",
            "Epoch 1987/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4745\n",
            "Epoch 1988/5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.4718\n",
            "Epoch 1989/5000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 5.4628\n",
            "Epoch 1990/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4690\n",
            "Epoch 1991/5000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 5.4660\n",
            "Epoch 1992/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4680\n",
            "Epoch 1993/5000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 5.4679\n",
            "Epoch 1994/5000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 5.4687\n",
            "Epoch 1995/5000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.4622\n",
            "Epoch 1996/5000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 5.4801\n",
            "Epoch 1997/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4758\n",
            "Epoch 1998/5000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 5.4677\n",
            "Epoch 1999/5000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 5.4704\n",
            "Epoch 2000/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.4741\n",
            "Checkpoint: /content/drive/My Drive/Colab Notebooks/women_writers/model/mhsa_v1_tf/training_checkpoints_4x(4, 4, 4, 4)x(512, 512, 512, 512)x400/cp-1999.h5\n",
            "prompt:  Westminster, the past life of Smith himself, and his\n",
            "house at Sheffield, though such stories seem to me the most dreary,\n",
            "irrelevant, and humbugging a\n",
            "---------------- T=0.6 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "leffch   wascand<br>barcoseaitwese, in   yof . at<br>sdef, ap“y tnd <br>bea leharirg, haentywet pebinguhated <br>u lbed athe re fs "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.7 ---------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " sicbkfor   aed  fooof leinbeathour. <br>  a ithept ali<br>f e aeeo bt tuldg t posswhaan sain<br>nos e buspe fing wiit “ns,  fcnetheen "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- T=0.8 ---------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1e85eedf3601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mservice_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for TPU we need to role our own checkpointer since we need to transfer the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-946db5da26d0>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0mreply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmhsa_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                         \u001b[0mreply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmhsa_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_highlight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_quote_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdark_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_dark_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_ref_anchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-f26edd55b38c>\u001b[0m in \u001b[0;36mmhsa_generate\u001b[0;34m(model, text, gen_len, temperature, argmax, verbose)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'assertion failure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \"\"\"\n\u001b[1;32m    451\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 452\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ml_indie_tools/keras_custom_layers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mxa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mxa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmhsa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_heads_by_add\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     if any(isinstance(x, (\n\u001b[1;32m   1040\u001b[0m         tf.Tensor, np.ndarray, float, int)) for x in input_list):\n\u001b[0;32m-> 1041\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_convert_numpy_or_python_types\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3382\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3383\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1558\u001b[0m   \"\"\"\n\u001b[1;32m   1559\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1560\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1568\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m       \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1651\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilding_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecuting_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;34m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mones_rank_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    steps_per_epoch=restricted_batches//params['batch_size']\n",
        "    if steps_per_epoch < 1:\n",
        "        steps_per_epoch = 1\n",
        "    history = model.fit(dataset, epochs=EPOCHS, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, callbacks=[service_callback]) # for TPU we need to role our own checkpointer since we need to transfer the weights\n",
        "else:\n",
        "    history = model.fit(dataset, validation_data=validation_dataset, epochs=EPOCHS, initial_epoch=initial_epoch, callbacks=[checkpoint_callback, tensorboard_callback, service_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a81LdPyY2dyo"
      },
      "outputs": [],
      "source": [
        "model_cpu.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDNYZiEQtgF"
      },
      "outputs": [],
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "def doDialog(model):\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "    print(\"Please enter some dialog.\")\n",
        "    print(\"The net will answer according to your input.\")\n",
        "    print(\"'bye' for end,\")\n",
        "    print(\"'reset' to reset the conversation context,\")\n",
        "    print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "    print(\"    to change character of the dialog.\")\n",
        "    print(\"    Current temperature={}.\".format(temperature))\n",
        "    print()\n",
        "    xso = None\n",
        "    bye = False\n",
        "    doini = True\n",
        "    bye = False\n",
        "    while not bye:\n",
        "        print(\"> \", end=\"\")\n",
        "        prompt = input()\n",
        "        if prompt == 'bye':\n",
        "            bye = True\n",
        "            print(\"Good bye!\")\n",
        "            continue\n",
        "        if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "            t = float(prompt[len(\"temperature=\"):])\n",
        "            if t > 0.05 and t < 1.4:\n",
        "                temperature = t\n",
        "                print(\"(generator temperature now {})\".format(t))\n",
        "                print()\n",
        "                continue\n",
        "            print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "            continue\n",
        "        reply=mhsa_generate(model, prompt, gen_len=256, temperature=temperature, verbose=True)\n",
        "        td.source_highlight(reply, min_quote_size=13, dark_mode=use_dark_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JEPK2WIQtgI"
      },
      "outputs": [],
      "source": [
        "# Talk to the net!\n",
        "doDialog(model_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMCWf5AZn1-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "VmWbteSFQtfq",
        "yWE_ZZMKEARV"
      ],
      "machine_shape": "hm",
      "name": "transformer_poet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}